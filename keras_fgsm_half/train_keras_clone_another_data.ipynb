{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from fgsm.deep_convnet import DeepConvNet\n",
    "from common.functions import softmax\n",
    "from common.trainer import Trainer\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "epochs = 20\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (60000, 28, 28)\n",
      "y_train.shape (60000,)\n",
      "X_test.shape (10000, 28, 28)\n",
      "y_test.shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape (30000, 28, 28)\n",
      "y_train1.shape (30000,)\n",
      "X_train2.shape (30000, 28, 28)\n",
      "y_train2.shape (30000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryuto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_train2, y_train1, y_train2 = train_test_split(X_train, y_train,train_size=0.5)\n",
    "print(\"X_train1.shape\", X_train1.shape)\n",
    "print(\"y_train1.shape\", y_train1.shape)\n",
    "print(\"X_train2.shape\", X_train2.shape)\n",
    "print(\"y_train2.shape\", y_train2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train1.reshape(X_train1.shape[0], 784).astype('float32') / 255\n",
    "X_train2 = X_train2.reshape(X_train2.shape[0], 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "y_train1 = keras.utils.to_categorical(y_train1, num_classes)\n",
    "\n",
    "X_test1 = X_test.reshape(X_test.shape[0], 784).astype('float32') / 255\n",
    "X_test2 = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32') / 255\n",
    "\n",
    "y_test1 = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_test2 = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1127 14:14:50.186344 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1127 14:14:50.208239 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1127 14:14:50.211606 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1127 14:14:50.230448 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1127 14:14:50.241995 140736894542784 deprecation.py:506] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1127 14:14:50.325966 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1127 14:14:50.334398 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 14:14:50.542925 140736894542784 deprecation.py:323] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "30000/30000 [==============================] - 5s 157us/step - loss: 0.3246 - acc: 0.8991 - val_loss: 0.1526 - val_acc: 0.9520\n",
      "Epoch 2/20\n",
      "30000/30000 [==============================] - 4s 131us/step - loss: 0.1378 - acc: 0.9581 - val_loss: 0.1025 - val_acc: 0.9694\n",
      "Epoch 3/20\n",
      "30000/30000 [==============================] - 4s 126us/step - loss: 0.0934 - acc: 0.9715 - val_loss: 0.0974 - val_acc: 0.9706\n",
      "Epoch 4/20\n",
      "30000/30000 [==============================] - 4s 130us/step - loss: 0.0730 - acc: 0.9782 - val_loss: 0.0996 - val_acc: 0.9719\n",
      "Epoch 5/20\n",
      "30000/30000 [==============================] - 4s 124us/step - loss: 0.0566 - acc: 0.9825 - val_loss: 0.0961 - val_acc: 0.9761\n",
      "Epoch 6/20\n",
      "30000/30000 [==============================] - 4s 144us/step - loss: 0.0500 - acc: 0.9847 - val_loss: 0.1053 - val_acc: 0.9741\n",
      "Epoch 7/20\n",
      "30000/30000 [==============================] - 4s 147us/step - loss: 0.0407 - acc: 0.9870 - val_loss: 0.0932 - val_acc: 0.9769\n",
      "Epoch 8/20\n",
      "30000/30000 [==============================] - 4s 137us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.1080 - val_acc: 0.9771\n",
      "Epoch 9/20\n",
      "30000/30000 [==============================] - 4s 131us/step - loss: 0.0315 - acc: 0.9908 - val_loss: 0.1126 - val_acc: 0.9756\n",
      "Epoch 10/20\n",
      "30000/30000 [==============================] - 5s 150us/step - loss: 0.0276 - acc: 0.9913 - val_loss: 0.1018 - val_acc: 0.9784\n",
      "Epoch 11/20\n",
      "30000/30000 [==============================] - 4s 140us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.1118 - val_acc: 0.9772\n",
      "Epoch 12/20\n",
      "30000/30000 [==============================] - 4s 137us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.1097 - val_acc: 0.9792\n",
      "Epoch 13/20\n",
      "30000/30000 [==============================] - 4s 142us/step - loss: 0.0219 - acc: 0.9934 - val_loss: 0.1137 - val_acc: 0.9787\n",
      "Epoch 14/20\n",
      "30000/30000 [==============================] - 4s 145us/step - loss: 0.0206 - acc: 0.9937 - val_loss: 0.1345 - val_acc: 0.9773\n",
      "Epoch 15/20\n",
      "30000/30000 [==============================] - 4s 144us/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.1196 - val_acc: 0.9789\n",
      "Epoch 16/20\n",
      "30000/30000 [==============================] - 4s 141us/step - loss: 0.0172 - acc: 0.9950 - val_loss: 0.1351 - val_acc: 0.9776\n",
      "Epoch 17/20\n",
      "30000/30000 [==============================] - 4s 143us/step - loss: 0.0141 - acc: 0.9957 - val_loss: 0.1464 - val_acc: 0.9772\n",
      "Epoch 18/20\n",
      "30000/30000 [==============================] - 4s 141us/step - loss: 0.0161 - acc: 0.9957 - val_loss: 0.1437 - val_acc: 0.9791\n",
      "Epoch 19/20\n",
      "30000/30000 [==============================] - 4s 130us/step - loss: 0.0142 - acc: 0.9958 - val_loss: 0.1378 - val_acc: 0.9788\n",
      "Epoch 20/20\n",
      "30000/30000 [==============================] - 5s 166us/step - loss: 0.0135 - acc: 0.9959 - val_loss: 0.1529 - val_acc: 0.9768\n",
      "Test loss: 0.15291149665892168\n",
      "Test accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train1, y_train1,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test1, y_test1))\n",
    "score = model.evaluate(X_test1, y_test1, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved.\n"
     ]
    }
   ],
   "source": [
    "model_json_str = model.to_json()\n",
    "open('mnist_mlp_model_half.json', 'w').write(model_json_str)\n",
    "model.save_weights('mnist_mlp_weights_half.h5');\n",
    "print('model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred = model.predict(X_train2.reshape(X_train2.shape[0], 784))\n",
    "model_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.281419304548943\n",
      "=== epoch:1, train acc:0.134, test acc:0.131 ===\n",
      "train loss:2.2915047600473657\n",
      "train loss:2.2971671039749224\n",
      "train loss:2.2694230408813776\n",
      "train loss:2.292703828982368\n",
      "train loss:2.2613371570795535\n",
      "train loss:2.2693653931703035\n",
      "train loss:2.2317517190310245\n",
      "train loss:2.2473347302263975\n",
      "train loss:2.2341446023520333\n",
      "train loss:2.2179519058647896\n",
      "train loss:2.225723680844179\n",
      "train loss:2.1959997349531637\n",
      "train loss:2.1674147628065614\n",
      "train loss:2.2271876773842685\n",
      "train loss:2.2123606984222746\n",
      "train loss:2.1971360505780937\n",
      "train loss:2.1635264893415176\n",
      "train loss:2.0859901755063652\n",
      "train loss:2.150887245238712\n",
      "train loss:2.1998143729869035\n",
      "train loss:2.042400872931367\n",
      "train loss:2.0421013997412847\n",
      "train loss:2.0899201621029047\n",
      "train loss:2.1756238385334727\n",
      "train loss:1.9298596275160305\n",
      "train loss:2.0658923326945073\n",
      "train loss:2.0198481705878417\n",
      "train loss:1.9552130698045795\n",
      "train loss:2.095578530138086\n",
      "train loss:2.02869104618809\n",
      "train loss:2.016365145311923\n",
      "train loss:2.0123553041569204\n",
      "train loss:1.9434658685131891\n",
      "train loss:2.0119682279729374\n",
      "train loss:2.1033305354140244\n",
      "train loss:1.9812247732660508\n",
      "train loss:1.8540415960918832\n",
      "train loss:1.9660673616822657\n",
      "train loss:1.9550914240028052\n",
      "train loss:2.06950143912775\n",
      "train loss:1.731335727890763\n",
      "train loss:1.8624795094459916\n",
      "train loss:1.8440782147686627\n",
      "train loss:1.9420004339918364\n",
      "train loss:1.8615828725975616\n",
      "train loss:1.695751734595055\n",
      "train loss:1.9170907819603826\n",
      "train loss:1.8236905696626038\n",
      "train loss:1.9597272546182394\n",
      "train loss:1.8688510394503914\n",
      "train loss:1.9691880612781163\n",
      "train loss:1.8737009043134947\n",
      "train loss:1.807221350851839\n",
      "train loss:1.8168706468221487\n",
      "train loss:1.858486287476838\n",
      "train loss:1.689864134616063\n",
      "train loss:1.8649465075322171\n",
      "train loss:1.771653271628771\n",
      "train loss:1.6265454579663154\n",
      "train loss:1.85409038407742\n",
      "train loss:1.7948506577932788\n",
      "train loss:1.8960733051547811\n",
      "train loss:1.7217050748668359\n",
      "train loss:1.7785025109864678\n",
      "train loss:1.750243430487165\n",
      "train loss:1.8143847680397918\n",
      "train loss:1.7576915142521816\n",
      "train loss:1.7586596216201764\n",
      "train loss:1.8246779225476633\n",
      "train loss:1.6190821911548474\n",
      "train loss:1.6385978533300454\n",
      "train loss:1.648675191502303\n",
      "train loss:1.634485918332392\n",
      "train loss:1.6141056773740243\n",
      "train loss:1.6579599273362071\n",
      "train loss:1.8727946896821428\n",
      "train loss:1.5948685615079576\n",
      "train loss:1.947522236308502\n",
      "train loss:1.7305980459235601\n",
      "train loss:2.023812734653299\n",
      "train loss:1.5635966515851731\n",
      "train loss:1.6952296707322088\n",
      "train loss:1.5828319201356826\n",
      "train loss:1.5086615056689079\n",
      "train loss:1.5126334665419625\n",
      "train loss:1.4965786906695646\n",
      "train loss:1.7650901102652277\n",
      "train loss:1.557849145266462\n",
      "train loss:1.7307273432214803\n",
      "train loss:1.6622631886199917\n",
      "train loss:1.6016886811963025\n",
      "train loss:1.6369169794714395\n",
      "train loss:1.806035174806159\n",
      "train loss:1.5978833523419627\n",
      "train loss:1.574135772507936\n",
      "train loss:1.7622650941573448\n",
      "train loss:1.5465602166674726\n",
      "train loss:1.4865002464680754\n",
      "train loss:1.6133553111320653\n",
      "train loss:1.6730711421732636\n",
      "train loss:1.6948024590030832\n",
      "train loss:1.5239301865324406\n",
      "train loss:1.629489724610324\n",
      "train loss:1.5763057471754311\n",
      "train loss:1.5390709141500785\n",
      "train loss:1.5577724605128502\n",
      "train loss:1.4869466213814064\n",
      "train loss:1.557577499911211\n",
      "train loss:1.3737756988034062\n",
      "train loss:1.561199356372431\n",
      "train loss:1.584023762200478\n",
      "train loss:1.5135816858555398\n",
      "train loss:1.5780327558268759\n",
      "train loss:1.586898180944169\n",
      "train loss:1.677663297199345\n",
      "train loss:1.5126517360809066\n",
      "train loss:1.4440270325504574\n",
      "train loss:1.5120381692715978\n",
      "train loss:1.4332407759806574\n",
      "train loss:1.6737172704018448\n",
      "train loss:1.5712506447162264\n",
      "train loss:1.4928155611551475\n",
      "train loss:1.5675166380788754\n",
      "train loss:1.688699128281556\n",
      "train loss:1.4429616238077276\n",
      "train loss:1.5852635558674195\n",
      "train loss:1.3681773805689526\n",
      "train loss:1.4639458490669972\n",
      "train loss:1.5153383828550941\n",
      "train loss:1.4937765293704677\n",
      "train loss:1.5256760731080257\n",
      "train loss:1.4770187746741437\n",
      "train loss:1.507685102971938\n",
      "train loss:1.5330906166146416\n",
      "train loss:1.490219294804146\n",
      "train loss:1.7013453317728375\n",
      "train loss:1.4431828496356705\n",
      "train loss:1.5294271765800798\n",
      "train loss:1.3677040451305589\n",
      "train loss:1.4244582265615997\n",
      "train loss:1.7132744640982538\n",
      "train loss:1.4526842399335056\n",
      "train loss:1.424540430364629\n",
      "train loss:1.3644407552776194\n",
      "train loss:1.3850862992659396\n",
      "train loss:1.3748993809045968\n",
      "train loss:1.5104824418818819\n",
      "train loss:1.477329413355143\n",
      "train loss:1.3812880199385267\n",
      "train loss:1.4750950058818335\n",
      "train loss:1.4242847722559557\n",
      "train loss:1.4727714347853713\n",
      "train loss:1.354515686123912\n",
      "train loss:1.4809139297619651\n",
      "train loss:1.4656392803689133\n",
      "train loss:1.4302919596901467\n",
      "train loss:1.5201687823273504\n",
      "train loss:1.431398900654228\n",
      "train loss:1.405429736231252\n",
      "train loss:1.3600790301073464\n",
      "train loss:1.3159207224825549\n",
      "train loss:1.6073794802159456\n",
      "train loss:1.4384465428858815\n",
      "train loss:1.408272731997905\n",
      "train loss:1.3693109259842686\n",
      "train loss:1.4906803499920358\n",
      "train loss:1.461229957418356\n",
      "train loss:1.362492253197909\n",
      "train loss:1.425031349469016\n",
      "train loss:1.4877159805554279\n",
      "train loss:1.351206640497506\n",
      "train loss:1.2690115897760061\n",
      "train loss:1.4372763793468042\n",
      "train loss:1.2851757716704304\n",
      "train loss:1.4675905573491521\n",
      "train loss:1.3979428818953514\n",
      "train loss:1.5778514154337169\n",
      "train loss:1.2415910466693543\n",
      "train loss:1.4367565498513684\n",
      "train loss:1.2743477167981647\n",
      "train loss:1.6087661297688534\n",
      "train loss:1.4681629885961687\n",
      "train loss:1.3767461004100319\n",
      "train loss:1.3538765447543537\n",
      "train loss:1.427728117679365\n",
      "train loss:1.4305522253689669\n",
      "train loss:1.4615320080671967\n",
      "train loss:1.4844627725351183\n",
      "train loss:1.3642010567933647\n",
      "train loss:1.2964613907849414\n",
      "train loss:1.3549706505238388\n",
      "train loss:1.299147052125485\n",
      "train loss:1.531963191152601\n",
      "train loss:1.2186169832685856\n",
      "train loss:1.259250880292131\n",
      "train loss:1.4776837684396056\n",
      "train loss:1.4467628245251014\n",
      "train loss:1.371584688286525\n",
      "train loss:1.2734299629137147\n",
      "train loss:1.3794080576175762\n",
      "train loss:1.3866133433602041\n",
      "train loss:1.5721911822427557\n",
      "train loss:1.2054254451151596\n",
      "train loss:1.337568763043787\n",
      "train loss:1.2884900829021253\n",
      "train loss:1.4115614894145403\n",
      "train loss:1.2436590880849012\n",
      "train loss:1.5061121563296802\n",
      "train loss:1.3598649583826985\n",
      "train loss:1.397577609713824\n",
      "train loss:1.608238637415281\n",
      "train loss:1.4097360648095032\n",
      "train loss:1.2570015381250423\n",
      "train loss:1.4065671458866098\n",
      "train loss:1.3819972129788283\n",
      "train loss:1.3033007907248728\n",
      "train loss:1.193749996328099\n",
      "train loss:1.1310270319143623\n",
      "train loss:1.3247444068509295\n",
      "train loss:1.5009076711648441\n",
      "train loss:1.2027406454566842\n",
      "train loss:1.4379019425790702\n",
      "train loss:1.2824537895582302\n",
      "train loss:1.2942965184584478\n",
      "train loss:1.2816726927891913\n",
      "train loss:1.288897645642087\n",
      "train loss:1.2365089554536262\n",
      "train loss:1.1949331350209966\n",
      "train loss:1.4131127242437114\n",
      "train loss:1.2959113066408645\n",
      "train loss:1.3972010789354021\n",
      "train loss:1.5565591175126667\n",
      "train loss:1.368470768673478\n",
      "train loss:1.217958140712966\n",
      "train loss:1.4520811777582714\n",
      "train loss:1.497775305797273\n",
      "train loss:1.4901613101101596\n",
      "train loss:1.324448097545349\n",
      "train loss:1.4925551964995203\n",
      "train loss:1.2289984856899399\n",
      "train loss:1.4028295712985368\n",
      "train loss:1.4089522580206948\n",
      "train loss:1.302321677488028\n",
      "train loss:1.2161783458087256\n",
      "train loss:1.384728397247715\n",
      "train loss:1.2892895466532974\n",
      "train loss:1.2791597453300683\n",
      "train loss:1.4464913058796427\n",
      "train loss:1.2162024563296319\n",
      "train loss:1.3992596002798765\n",
      "train loss:1.5404713091432956\n",
      "train loss:1.4311639431904022\n",
      "train loss:1.2866241590682623\n",
      "train loss:1.3426157288730485\n",
      "train loss:1.3637065043683783\n",
      "train loss:1.3488379778620219\n",
      "train loss:1.3712535034872795\n",
      "train loss:1.2973890820864278\n",
      "train loss:1.2666679488121035\n",
      "train loss:1.2375555409483316\n",
      "train loss:1.2963760536847306\n",
      "train loss:1.195565857755755\n",
      "train loss:1.3270563888955713\n",
      "train loss:1.2764651524798467\n",
      "train loss:1.3965294462382989\n",
      "train loss:1.31296803743261\n",
      "train loss:1.3060239680010879\n",
      "train loss:1.333226828321674\n",
      "train loss:1.4062148740276006\n",
      "train loss:1.3718772738425755\n",
      "train loss:1.219724527359264\n",
      "train loss:1.1927364083059708\n",
      "train loss:1.3582454533348565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.3346492474783542\n",
      "train loss:1.2102236665144834\n",
      "train loss:1.3079246104320479\n",
      "train loss:1.349595989776029\n",
      "train loss:1.2965730884812277\n",
      "train loss:1.169070336579835\n",
      "train loss:1.2097322921064817\n",
      "train loss:1.4612606582077177\n",
      "train loss:1.2820166209742985\n",
      "train loss:1.229714914924386\n",
      "train loss:1.2374225095931681\n",
      "train loss:1.3143959960386835\n",
      "train loss:1.1863406807035426\n",
      "train loss:1.3110354616154993\n",
      "train loss:1.3385915031766698\n",
      "train loss:1.299512287814476\n",
      "train loss:1.357877865597132\n",
      "train loss:1.2172027068162425\n",
      "train loss:1.2753946539537362\n",
      "train loss:1.2718452760890682\n",
      "train loss:1.2445003042488179\n",
      "train loss:1.4122641709214767\n",
      "train loss:1.3481262914382106\n",
      "train loss:1.2520551427717919\n",
      "train loss:1.303876515358169\n",
      "train loss:1.1597641557182\n",
      "train loss:1.1740563785314118\n",
      "=== epoch:2, train acc:0.96, test acc:0.96 ===\n",
      "train loss:1.3477095338840437\n",
      "train loss:1.325339842500861\n",
      "train loss:1.3320422349823897\n",
      "train loss:1.2664863423578299\n",
      "train loss:1.3885972288141857\n",
      "train loss:1.211843551438068\n",
      "train loss:1.481613937494948\n",
      "train loss:1.2479934764630098\n",
      "train loss:1.2723893910837711\n",
      "train loss:1.4022479713688174\n",
      "train loss:1.2813313900781247\n",
      "train loss:1.325946891661652\n",
      "train loss:1.2304803654590037\n",
      "train loss:1.2614724158019426\n",
      "train loss:1.5291222330434975\n",
      "train loss:1.408967708349857\n",
      "train loss:1.1328056090299263\n",
      "train loss:1.2424555882215866\n",
      "train loss:1.049758601864927\n",
      "train loss:1.2670197916925945\n",
      "train loss:1.1964731315843609\n",
      "train loss:1.2663986884035459\n",
      "train loss:1.143772316779451\n",
      "train loss:1.2815425385968866\n",
      "train loss:1.2677782553282064\n",
      "train loss:1.1427138883002887\n",
      "train loss:1.2351613907299974\n",
      "train loss:1.3230619691197725\n",
      "train loss:1.284889042377141\n",
      "train loss:1.2995286407237137\n",
      "train loss:1.324450188378222\n",
      "train loss:1.17810167606081\n",
      "train loss:1.2259249942334662\n",
      "train loss:1.6261984544538657\n",
      "train loss:1.331019492317968\n",
      "train loss:1.2604502906586672\n",
      "train loss:1.4815627791598198\n",
      "train loss:1.279181582483607\n",
      "train loss:1.322346193509238\n",
      "train loss:1.2158338539256666\n",
      "train loss:1.4031439362397489\n",
      "train loss:1.274461068634795\n",
      "train loss:1.0868339609019235\n",
      "train loss:1.239993288624084\n",
      "train loss:1.3235412756292222\n",
      "train loss:1.2958309866652435\n",
      "train loss:1.1186621173150992\n",
      "train loss:1.1714606662065115\n",
      "train loss:1.4024098469732689\n",
      "train loss:1.3846543256045671\n",
      "train loss:1.3476713297866019\n",
      "train loss:1.2308067710069113\n",
      "train loss:1.277335444124254\n",
      "train loss:1.2056642140646863\n",
      "train loss:1.210568081110372\n",
      "train loss:1.2482559615669258\n",
      "train loss:1.2269447110988227\n",
      "train loss:1.2625512970035648\n",
      "train loss:0.9494511274985112\n",
      "train loss:1.1666803688630205\n",
      "train loss:1.2562891390104556\n",
      "train loss:1.1985924245146413\n",
      "train loss:1.1622507069777246\n",
      "train loss:1.1717723401454827\n",
      "train loss:1.098483304150833\n",
      "train loss:1.2076879707185972\n",
      "train loss:1.1833793445367737\n",
      "train loss:1.3036577518524766\n",
      "train loss:1.1860405635709308\n",
      "train loss:1.2853870730963042\n",
      "train loss:1.3184588808896947\n",
      "train loss:1.4803323041203296\n",
      "train loss:1.3111177929743016\n",
      "train loss:1.324613749476784\n",
      "train loss:1.174599373580742\n",
      "train loss:1.2247332327992053\n",
      "train loss:1.1376544628360106\n",
      "train loss:1.143949185772193\n",
      "train loss:1.0016263792856368\n",
      "train loss:1.3453327916818705\n",
      "train loss:1.1756752131143284\n",
      "train loss:1.126185859238459\n",
      "train loss:1.3418185018775475\n",
      "train loss:1.331584449823094\n",
      "train loss:1.2347389740892583\n",
      "train loss:1.2814097837921339\n",
      "train loss:1.2893458197270022\n",
      "train loss:1.3200341080152826\n",
      "train loss:1.1482172149122787\n",
      "train loss:1.1181653838916357\n",
      "train loss:1.07541474349156\n",
      "train loss:1.2270090469035202\n",
      "train loss:1.3329948708555035\n",
      "train loss:1.0902576504247252\n",
      "train loss:1.3248325129183\n",
      "train loss:1.164193485722768\n",
      "train loss:1.2355753868282187\n",
      "train loss:1.2642716443292734\n",
      "train loss:1.3367872732923414\n",
      "train loss:1.2764518929522828\n",
      "train loss:1.2618786682291376\n",
      "train loss:1.117821277223014\n",
      "train loss:1.1586881175954955\n",
      "train loss:1.3780932053170858\n",
      "train loss:1.183556484682315\n",
      "train loss:1.0826320063157047\n",
      "train loss:1.1949056043211819\n",
      "train loss:1.1497420871437811\n",
      "train loss:1.2432822505508183\n",
      "train loss:1.291869277899725\n",
      "train loss:1.2453753039098456\n",
      "train loss:1.0921386921024234\n",
      "train loss:1.0789905071114563\n",
      "train loss:1.2060266815018288\n",
      "train loss:1.2644025627493372\n",
      "train loss:1.385110568399419\n",
      "train loss:1.2804447162679196\n",
      "train loss:1.1018119840550928\n",
      "train loss:1.0793680246036326\n",
      "train loss:1.1567867681187503\n",
      "train loss:1.1720300229448555\n",
      "train loss:1.2179386373902144\n",
      "train loss:1.2626959996491522\n",
      "train loss:1.0702960790118814\n",
      "train loss:1.169714503711376\n",
      "train loss:1.179617992419544\n",
      "train loss:1.0011999874318278\n",
      "train loss:1.0779439748970987\n",
      "train loss:1.1499386363671054\n",
      "train loss:1.2764874653898486\n",
      "train loss:1.262626631435068\n",
      "train loss:1.0985958840064647\n",
      "train loss:1.1839859641253572\n",
      "train loss:1.07383846747129\n",
      "train loss:1.1825918254239787\n",
      "train loss:1.4304883557551014\n",
      "train loss:1.1837552275172456\n",
      "train loss:1.3396477756258747\n",
      "train loss:1.252747372857124\n",
      "train loss:1.217764033901334\n",
      "train loss:1.0869694545161048\n",
      "train loss:1.1442810983280092\n",
      "train loss:1.1511527642536186\n",
      "train loss:1.286474776635656\n",
      "train loss:1.3027864886880791\n",
      "train loss:1.1260508777505784\n",
      "train loss:1.1681260294669118\n",
      "train loss:1.3538436805391385\n",
      "train loss:0.9994922103944949\n",
      "train loss:1.196263206190092\n",
      "train loss:1.113966178473995\n",
      "train loss:1.0719575602708935\n",
      "train loss:1.20050843203045\n",
      "train loss:1.124825948590875\n",
      "train loss:1.1603174877840208\n",
      "train loss:1.1248031155124005\n",
      "train loss:1.314801486756811\n",
      "train loss:1.0824006564117197\n",
      "train loss:1.1405145506450718\n",
      "train loss:1.155078726441836\n",
      "train loss:1.3923714223845118\n",
      "train loss:1.3089356485560517\n",
      "train loss:1.1341936227715812\n",
      "train loss:1.0820081621705113\n",
      "train loss:1.2573539983759268\n",
      "train loss:1.0175103775666192\n",
      "train loss:1.1667097515075193\n",
      "train loss:1.0195363880685109\n",
      "train loss:1.30060894613733\n",
      "train loss:1.2871245134570708\n",
      "train loss:0.9921465655470236\n",
      "train loss:0.9973006437098033\n",
      "train loss:1.1195039775317293\n",
      "train loss:1.136668446514087\n",
      "train loss:1.1680161869979886\n",
      "train loss:1.113942280845989\n",
      "train loss:1.0961747753425537\n",
      "train loss:1.214652671893891\n",
      "train loss:1.1576065700356803\n",
      "train loss:1.049157429271642\n",
      "train loss:1.2487900923983388\n",
      "train loss:1.1382387231009405\n",
      "train loss:0.997470206604256\n",
      "train loss:1.15845614066445\n",
      "train loss:1.2053820939267628\n",
      "train loss:0.9396991608010776\n",
      "train loss:1.3673445195246112\n",
      "train loss:1.07316954775849\n",
      "train loss:1.2366733862345294\n",
      "train loss:1.0126569905179457\n",
      "train loss:1.1157960267678853\n",
      "train loss:1.293213212075283\n",
      "train loss:1.3098447963648794\n",
      "train loss:1.0401104199712108\n",
      "train loss:1.3324399554786042\n",
      "train loss:1.1148477873369849\n",
      "train loss:1.2641454075946492\n",
      "train loss:1.326980106551389\n",
      "train loss:1.0854826449101962\n",
      "train loss:1.3019186142647525\n",
      "train loss:1.0444713412184319\n",
      "train loss:1.179241477867785\n",
      "train loss:1.1433483119246308\n",
      "train loss:1.1714737812035054\n",
      "train loss:1.140009628500933\n",
      "train loss:1.0704428528541563\n",
      "train loss:1.2267355826542838\n",
      "train loss:1.1948291432406637\n",
      "train loss:1.1182006475797381\n",
      "train loss:1.3337380609432268\n",
      "train loss:1.2628704631721774\n",
      "train loss:1.2907996457232327\n",
      "train loss:1.1980744279334912\n",
      "train loss:1.4049767846647603\n",
      "train loss:1.341213210313094\n",
      "train loss:1.3575792764397647\n",
      "train loss:1.2697613899183053\n",
      "train loss:1.2378621585545055\n",
      "train loss:1.008878263609978\n",
      "train loss:1.4277517540463507\n",
      "train loss:1.2420700867507135\n",
      "train loss:1.1075877006718648\n",
      "train loss:1.2621386222810398\n",
      "train loss:1.2713214627200715\n",
      "train loss:1.2540132699725997\n",
      "train loss:1.2151910904183705\n",
      "train loss:1.0812999137651926\n",
      "train loss:1.0980471889729086\n",
      "train loss:1.2848921472525308\n",
      "train loss:1.2356871469249755\n",
      "train loss:1.0292489222453205\n",
      "train loss:1.1999311304566225\n",
      "train loss:1.1008116929087595\n",
      "train loss:1.0936832769945384\n",
      "train loss:1.2332470051148234\n",
      "train loss:1.145605220219324\n",
      "train loss:0.9839670938348977\n",
      "train loss:1.1348558163992568\n",
      "train loss:1.169628918516223\n",
      "train loss:1.0799918725697193\n",
      "train loss:1.026237635066541\n",
      "train loss:1.0724295918673095\n",
      "train loss:1.2091096152587368\n",
      "train loss:1.0154817074865088\n",
      "train loss:1.1810548422559586\n",
      "train loss:1.1460411540149684\n",
      "train loss:1.1857129697852864\n",
      "train loss:1.2364901399570578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1852783446814286\n",
      "train loss:0.8744677180930922\n",
      "train loss:1.212647641677476\n",
      "train loss:1.193047389796529\n",
      "train loss:1.1367792555213376\n",
      "train loss:1.0080753169455827\n",
      "train loss:1.215446334841222\n",
      "train loss:1.3323398598192784\n",
      "train loss:1.1470639912380667\n",
      "train loss:1.3182669406650147\n",
      "train loss:1.1115307654843292\n",
      "train loss:1.2057129440615768\n",
      "train loss:1.234049582080768\n",
      "train loss:1.196809332850657\n",
      "train loss:1.016611644564796\n",
      "train loss:1.151852993210931\n",
      "train loss:1.0500392233909002\n",
      "train loss:1.14004591216459\n",
      "train loss:1.047483266963883\n",
      "train loss:1.174925439510842\n",
      "train loss:1.1522376529105187\n",
      "train loss:1.0871624304720093\n",
      "train loss:1.1337105451260485\n",
      "train loss:1.17450119095082\n",
      "train loss:1.1005046618612817\n",
      "train loss:1.4032747635906828\n",
      "train loss:1.0742077957261669\n",
      "train loss:1.1939498611800807\n",
      "train loss:1.3511856304211818\n",
      "train loss:1.2967251595426965\n",
      "train loss:1.1563164910993888\n",
      "train loss:1.2336616465282624\n",
      "train loss:1.0571988873426894\n",
      "train loss:1.140033771063945\n",
      "train loss:1.1475874726786643\n",
      "train loss:1.1107051869328217\n",
      "train loss:1.1195568344913194\n",
      "train loss:1.1921961372425876\n",
      "train loss:1.063008029607247\n",
      "train loss:1.1448513511197969\n",
      "train loss:1.0192057979895603\n",
      "train loss:1.0200773059695782\n",
      "train loss:1.3086996427280564\n",
      "train loss:0.9710086638878355\n",
      "train loss:1.0819839521169388\n",
      "train loss:1.2554625031450928\n",
      "train loss:1.1044490924352188\n",
      "train loss:1.128960079655738\n",
      "train loss:1.1718944518220031\n",
      "train loss:1.077354885085652\n",
      "train loss:1.0745362677906318\n",
      "train loss:1.2252870891808039\n",
      "=== epoch:3, train acc:0.969, test acc:0.963 ===\n",
      "train loss:1.3867988297925864\n",
      "train loss:1.0878312056629202\n",
      "train loss:1.136842175842451\n",
      "train loss:1.322462807240823\n",
      "train loss:1.1070807176539967\n",
      "train loss:1.068060607380412\n",
      "train loss:1.1809872904029282\n",
      "train loss:1.0178845756304165\n",
      "train loss:1.0442921538753578\n",
      "train loss:1.166130540335446\n",
      "train loss:1.0130061051805568\n",
      "train loss:0.981607571523777\n",
      "train loss:1.1139344346869078\n",
      "train loss:1.402329456771636\n",
      "train loss:1.1181163144346529\n",
      "train loss:1.2554843745250723\n",
      "train loss:1.0967865983950698\n",
      "train loss:1.2269236429004442\n",
      "train loss:1.0382634591705242\n",
      "train loss:0.9463347843112545\n",
      "train loss:1.195468773707092\n",
      "train loss:0.9048366886129264\n",
      "train loss:1.0543578120087296\n",
      "train loss:1.01314100023208\n",
      "train loss:1.2140063835418196\n",
      "train loss:1.1137502896975175\n",
      "train loss:1.3476517661960816\n",
      "train loss:0.9920729134340869\n",
      "train loss:1.180010181866375\n",
      "train loss:1.144586555857466\n",
      "train loss:1.026965237161307\n",
      "train loss:1.1660961955465459\n",
      "train loss:1.2874182623507906\n",
      "train loss:1.1154824044996412\n",
      "train loss:1.1271404621265264\n",
      "train loss:0.7879704262995467\n",
      "train loss:0.9668176919208592\n",
      "train loss:1.2429370967111613\n",
      "train loss:1.108475695973456\n",
      "train loss:1.0199431975071904\n",
      "train loss:1.316637705753025\n",
      "train loss:1.1844197933599316\n",
      "train loss:1.119581997212611\n",
      "train loss:1.2070312386316984\n",
      "train loss:1.217263919727286\n",
      "train loss:1.0500388662868998\n",
      "train loss:1.1383326204603252\n",
      "train loss:1.066043181929806\n",
      "train loss:1.0400055619939501\n",
      "train loss:1.0305173706341\n",
      "train loss:1.2004217358739973\n",
      "train loss:1.1988295074512019\n",
      "train loss:0.9843060683515797\n",
      "train loss:1.267412918017791\n",
      "train loss:1.216907693153948\n",
      "train loss:1.2409763440787218\n",
      "train loss:1.2944474046018097\n",
      "train loss:1.1665921113254558\n",
      "train loss:1.225511503407226\n",
      "train loss:0.9342214819576924\n",
      "train loss:1.4396657295973005\n",
      "train loss:1.1506678256732108\n",
      "train loss:1.1728218416567422\n",
      "train loss:1.002721050180953\n",
      "train loss:1.0886574697880522\n",
      "train loss:1.0677105456885043\n",
      "train loss:1.2053540624097088\n",
      "train loss:1.098346587186015\n",
      "train loss:0.9594347408067049\n",
      "train loss:1.0292074277392627\n",
      "train loss:1.1045175568971712\n",
      "train loss:1.0133531258973572\n",
      "train loss:1.1238877477140365\n",
      "train loss:1.0138959103584029\n",
      "train loss:1.2340549353109846\n",
      "train loss:0.9304979005177194\n",
      "train loss:1.059341653347412\n",
      "train loss:1.0529217945479596\n",
      "train loss:1.1869807586479972\n",
      "train loss:1.0559171639877731\n",
      "train loss:1.2535675592654483\n",
      "train loss:1.0863607560692354\n",
      "train loss:1.1891910772748795\n",
      "train loss:1.115066950431536\n",
      "train loss:1.0160370352758863\n",
      "train loss:1.163619562686868\n",
      "train loss:0.9915634698881632\n",
      "train loss:1.2011630536410778\n",
      "train loss:1.0677035055228832\n",
      "train loss:1.3355703049750831\n",
      "train loss:1.0071029790335504\n",
      "train loss:1.118833478642261\n",
      "train loss:1.1239318923855632\n",
      "train loss:1.1360203955683845\n",
      "train loss:1.095121219761751\n",
      "train loss:1.128125744222456\n",
      "train loss:1.177523839851312\n",
      "train loss:0.9695605502124106\n",
      "train loss:1.1127486676687446\n",
      "train loss:0.9681108991550671\n",
      "train loss:1.1051166605711307\n",
      "train loss:1.1546014348478526\n",
      "train loss:1.2037055566630526\n",
      "train loss:1.1985903225734158\n",
      "train loss:1.1342265351411296\n",
      "train loss:1.2077868432550551\n",
      "train loss:1.0939814422834997\n",
      "train loss:0.9656442559509609\n",
      "train loss:1.1153717224890891\n",
      "train loss:1.0184660289592948\n",
      "train loss:1.1244513358299015\n",
      "train loss:1.0865371608691823\n",
      "train loss:1.0698239415686936\n",
      "train loss:1.2493260702578752\n",
      "train loss:1.0244634260496552\n",
      "train loss:1.167486102272442\n",
      "train loss:1.083233653730995\n",
      "train loss:1.1043328013199518\n",
      "train loss:1.1111891184163336\n",
      "train loss:0.9864891060301006\n",
      "train loss:1.2501556811986498\n",
      "train loss:1.052029277839356\n",
      "train loss:1.2012806576313628\n",
      "train loss:1.0636390510312517\n",
      "train loss:1.0897849549951741\n",
      "train loss:0.9705608365383759\n",
      "train loss:0.9934120351056331\n",
      "train loss:1.0116640191654123\n",
      "train loss:1.2559471012617371\n",
      "train loss:0.9544762634607609\n",
      "train loss:1.3106232473053985\n",
      "train loss:1.3705219718848647\n",
      "train loss:1.2255546317791766\n",
      "train loss:1.1431167491362912\n",
      "train loss:1.1899493821501\n",
      "train loss:1.0328795713223058\n",
      "train loss:1.101402591675971\n",
      "train loss:1.179778845880448\n",
      "train loss:1.2149219352828278\n",
      "train loss:0.9437320021168096\n",
      "train loss:1.3091234901043283\n",
      "train loss:1.2627754476108337\n",
      "train loss:1.3122090955704155\n",
      "train loss:1.114933464217133\n",
      "train loss:1.2493842495223964\n",
      "train loss:1.2043190770744294\n",
      "train loss:1.19974642642169\n",
      "train loss:1.122187203362477\n",
      "train loss:1.0261518760653068\n",
      "train loss:1.1382809807211094\n",
      "train loss:1.094007264496189\n",
      "train loss:1.2141350030471054\n",
      "train loss:1.1985826133175355\n",
      "train loss:1.2222298500099038\n",
      "train loss:1.0348185084656034\n",
      "train loss:1.109130991812573\n",
      "train loss:1.253859530323035\n",
      "train loss:0.9680083286381616\n",
      "train loss:1.0752391525804483\n",
      "train loss:1.1519978292435678\n",
      "train loss:0.9760666051727012\n",
      "train loss:1.030744985705405\n",
      "train loss:0.8845591743900701\n",
      "train loss:1.0964412068518774\n",
      "train loss:1.1520428410305157\n",
      "train loss:1.0921139097095744\n",
      "train loss:1.0772944687935526\n",
      "train loss:1.1284891959768486\n",
      "train loss:1.0402640192373813\n",
      "train loss:1.1900251181937511\n",
      "train loss:0.9511828670068652\n",
      "train loss:1.16439558792874\n",
      "train loss:1.1939322361916849\n",
      "train loss:1.276347128980358\n",
      "train loss:1.0682801367887336\n",
      "train loss:1.1117802874060123\n",
      "train loss:1.1748473617329451\n",
      "train loss:1.147463561111157\n",
      "train loss:0.9349318930456735\n",
      "train loss:1.1183867508502805\n",
      "train loss:1.1185440099694655\n",
      "train loss:1.3274033114405066\n",
      "train loss:1.2244624738549215\n",
      "train loss:0.9867614490217512\n",
      "train loss:1.001042907127179\n",
      "train loss:1.1141831338742216\n",
      "train loss:0.993506471271885\n",
      "train loss:1.1043283033600835\n",
      "train loss:1.1460234559688445\n",
      "train loss:1.0340216447279544\n",
      "train loss:1.1835080778761704\n",
      "train loss:1.2127453737114957\n",
      "train loss:1.0729252547183883\n",
      "train loss:0.9899038340892728\n",
      "train loss:0.9961339867713229\n",
      "train loss:0.9485934611920749\n",
      "train loss:1.1564249974867409\n",
      "train loss:1.1919458018648947\n",
      "train loss:1.0123002531835426\n",
      "train loss:1.0850646705912914\n",
      "train loss:1.1763306895651913\n",
      "train loss:1.1338635964627763\n",
      "train loss:1.134853599776011\n",
      "train loss:0.9584682934263047\n",
      "train loss:1.0714128281365176\n",
      "train loss:1.0706799474650028\n",
      "train loss:1.2918668016693626\n",
      "train loss:1.2284715844606255\n",
      "train loss:1.0031142641403508\n",
      "train loss:1.3016018030146221\n",
      "train loss:1.2460419929945814\n",
      "train loss:0.9446450388986171\n",
      "train loss:1.0995139355027739\n",
      "train loss:0.9899905273739027\n",
      "train loss:1.0764747681079239\n",
      "train loss:1.203465569964878\n",
      "train loss:1.0881430385292126\n",
      "train loss:1.0610048803580776\n",
      "train loss:1.2017344180638991\n",
      "train loss:1.3010803626643792\n",
      "train loss:1.0929458449908183\n",
      "train loss:1.247834952313807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0903939243069531\n",
      "train loss:1.1479594281660694\n",
      "train loss:1.3713401780125885\n",
      "train loss:1.2493928592346009\n",
      "train loss:1.0654892951896477\n",
      "train loss:1.1950328559294865\n",
      "train loss:1.0348397886776288\n",
      "train loss:1.1028666806377467\n",
      "train loss:1.2542869244026413\n",
      "train loss:1.0417703260063806\n",
      "train loss:1.2017913771495266\n",
      "train loss:1.027349062457825\n",
      "train loss:1.25189395551706\n",
      "train loss:1.0083160371530913\n",
      "train loss:1.3151940519789733\n",
      "train loss:1.0266081099086168\n",
      "train loss:1.1392256884460483\n",
      "train loss:1.1107105068018617\n",
      "train loss:1.23465499377898\n",
      "train loss:1.121868731399056\n",
      "train loss:1.0901043719368644\n",
      "train loss:1.0937555195440825\n",
      "train loss:1.10452441504933\n",
      "train loss:1.179697165782134\n",
      "train loss:1.0120418242051006\n",
      "train loss:1.1261301053898192\n",
      "train loss:1.1619287063768644\n",
      "train loss:1.1202563170943605\n",
      "train loss:1.0865176920324324\n",
      "train loss:1.0251807186900648\n",
      "train loss:1.1019604814393973\n",
      "train loss:1.162371751554672\n",
      "train loss:1.0138816720077923\n",
      "train loss:0.992883513159943\n",
      "train loss:1.1335257789858155\n",
      "train loss:1.2203512653274107\n",
      "train loss:1.1810810146778044\n",
      "train loss:1.044973376633734\n",
      "train loss:1.0456312091221933\n",
      "train loss:1.0779886248889128\n",
      "train loss:0.9379994882024459\n",
      "train loss:0.9893221491107583\n",
      "train loss:1.0668540266714437\n",
      "train loss:1.0902576948107698\n",
      "train loss:1.1438004814343214\n",
      "train loss:0.8195839246243143\n",
      "train loss:1.0111851249345443\n",
      "train loss:1.1233023385554983\n",
      "train loss:0.9945068780657691\n",
      "train loss:1.0759663372110857\n",
      "train loss:1.0688925220500283\n",
      "train loss:1.0818284303835026\n",
      "train loss:1.1953895651268263\n",
      "train loss:0.9587414000898417\n",
      "train loss:1.0233653559908185\n",
      "train loss:0.9995774824670488\n",
      "train loss:1.0423891277709858\n",
      "train loss:0.983249044815336\n",
      "train loss:0.9932828994468162\n",
      "train loss:0.917847434741537\n",
      "train loss:0.9348690242074486\n",
      "train loss:1.0222003146614498\n",
      "train loss:0.9170482342127403\n",
      "train loss:0.8416401359546013\n",
      "train loss:1.161953152220525\n",
      "train loss:1.1053381752724016\n",
      "train loss:0.9487768329026846\n",
      "train loss:1.096408394930759\n",
      "train loss:1.0407486685705023\n",
      "train loss:1.1482613732706743\n",
      "train loss:1.1381501782148404\n",
      "train loss:1.0650905871119003\n",
      "train loss:1.0083930357532727\n",
      "train loss:1.0326764215284734\n",
      "train loss:0.8477344584455239\n",
      "train loss:1.0198607742213706\n",
      "train loss:1.289413441023558\n",
      "train loss:1.1189682768240947\n",
      "=== epoch:4, train acc:0.981, test acc:0.974 ===\n",
      "train loss:1.151455306295529\n",
      "train loss:1.011396859743511\n",
      "train loss:1.141816958852857\n",
      "train loss:0.9597698865509134\n",
      "train loss:1.0031561880670465\n",
      "train loss:1.1193251560588018\n",
      "train loss:1.0292215649278678\n",
      "train loss:1.0002292210640358\n",
      "train loss:1.1354436247760662\n",
      "train loss:0.9590226245599033\n",
      "train loss:0.9599448617758273\n",
      "train loss:1.0453291085170822\n",
      "train loss:1.0196912210740559\n",
      "train loss:0.8828316255416976\n",
      "train loss:1.0040366289173261\n",
      "train loss:1.1276144770046301\n",
      "train loss:0.9162503710316373\n",
      "train loss:1.110432183418438\n",
      "train loss:1.0182277675401545\n",
      "train loss:1.159690897645142\n",
      "train loss:1.0295144594153058\n",
      "train loss:1.2105497594259582\n",
      "train loss:1.1951161235011598\n",
      "train loss:1.1804877161109288\n",
      "train loss:0.8823866945604809\n",
      "train loss:0.9284971120224346\n",
      "train loss:1.0258472706619441\n",
      "train loss:1.0548082683617297\n",
      "train loss:0.8159016410047367\n",
      "train loss:1.1219619912226235\n",
      "train loss:1.211210083007594\n",
      "train loss:1.2288553767328239\n",
      "train loss:0.9902362566444876\n",
      "train loss:1.1717939355825104\n",
      "train loss:0.9222142803388651\n",
      "train loss:0.9571006553466682\n",
      "train loss:1.1612925710632749\n",
      "train loss:1.1174080101478625\n",
      "train loss:0.9315977221429321\n",
      "train loss:1.0344713283045097\n",
      "train loss:1.1762892654879684\n",
      "train loss:0.9306572754479503\n",
      "train loss:1.1405991364485932\n",
      "train loss:1.0249357510501143\n",
      "train loss:1.130037168007808\n",
      "train loss:1.0021024056880599\n",
      "train loss:1.0993757263815325\n",
      "train loss:0.9476446046960609\n",
      "train loss:1.0104186365987031\n",
      "train loss:1.221894502658543\n",
      "train loss:1.0032538383689356\n",
      "train loss:1.1065758106796644\n",
      "train loss:1.1062694062287668\n",
      "train loss:1.0515295614027833\n",
      "train loss:0.9003662809713481\n",
      "train loss:0.8461616755314972\n",
      "train loss:0.9543565441587457\n",
      "train loss:0.9845526338836332\n",
      "train loss:1.031536831441505\n",
      "train loss:0.8652752377926258\n",
      "train loss:0.9888695782485787\n",
      "train loss:1.0296372501924487\n",
      "train loss:1.0889850035783726\n",
      "train loss:1.1515099873548489\n",
      "train loss:1.0934043066654164\n",
      "train loss:1.1920323678975253\n",
      "train loss:1.0011222111689124\n",
      "train loss:0.9908545253761225\n",
      "train loss:1.0518774240357418\n",
      "train loss:0.9201242917525854\n",
      "train loss:0.954062349838233\n",
      "train loss:0.954684658836327\n",
      "train loss:1.1733589702050327\n",
      "train loss:0.9913348294128183\n",
      "train loss:1.0303911077498453\n",
      "train loss:1.0920477938413564\n",
      "train loss:1.1500048261535225\n",
      "train loss:0.9541668561890927\n",
      "train loss:1.0059525801605453\n",
      "train loss:1.07940726808448\n",
      "train loss:0.9994723114273606\n",
      "train loss:1.0647571379714866\n",
      "train loss:1.1253651078164961\n",
      "train loss:1.0151505217645784\n",
      "train loss:1.0670699038150198\n",
      "train loss:1.2026477501034551\n",
      "train loss:1.0256083156140294\n",
      "train loss:1.2035336697441932\n",
      "train loss:1.174037395833708\n",
      "train loss:1.116955280787413\n",
      "train loss:1.0393063510920442\n",
      "train loss:1.0517056164741236\n",
      "train loss:1.1253799653987508\n",
      "train loss:1.0043891972008365\n",
      "train loss:0.9029277537832624\n",
      "train loss:0.9226238538269521\n",
      "train loss:0.9451100045645568\n",
      "train loss:0.973646667381533\n",
      "train loss:1.0122741959780968\n",
      "train loss:1.0140179633597717\n",
      "train loss:1.0593857380407428\n",
      "train loss:0.9572366315472616\n",
      "train loss:1.151524611769682\n",
      "train loss:1.0686224470892414\n",
      "train loss:0.8876208371685662\n",
      "train loss:0.9851268487506464\n",
      "train loss:1.02240287403394\n",
      "train loss:0.929028041414398\n",
      "train loss:0.919798075906959\n",
      "train loss:1.07018970028513\n",
      "train loss:0.9795849208485259\n",
      "train loss:0.9225160704580145\n",
      "train loss:0.8395082358124718\n",
      "train loss:0.9988087452544582\n",
      "train loss:1.071180789566272\n",
      "train loss:1.0881319333209725\n",
      "train loss:0.9076399993858429\n",
      "train loss:1.0044872831895335\n",
      "train loss:1.0740253122880103\n",
      "train loss:1.002795074932494\n",
      "train loss:1.0726571022739726\n",
      "train loss:1.0159181760760831\n",
      "train loss:1.0032421774002984\n",
      "train loss:0.9589255609836753\n",
      "train loss:1.0613717262398163\n",
      "train loss:1.080380778575784\n",
      "train loss:1.2080409196051323\n",
      "train loss:1.0747235831127104\n",
      "train loss:0.9768912484479906\n",
      "train loss:0.9637859660456415\n",
      "train loss:1.053956861200425\n",
      "train loss:0.9367320173984598\n",
      "train loss:1.1296815147984176\n",
      "train loss:0.9659055157059168\n",
      "train loss:0.9930353850801731\n",
      "train loss:1.0997374905091255\n",
      "train loss:1.0762832025378928\n",
      "train loss:1.0346607143159803\n",
      "train loss:1.088353292838546\n",
      "train loss:1.0532525730706905\n",
      "train loss:1.0236154625673541\n",
      "train loss:1.021439721476589\n",
      "train loss:1.1295223972691897\n",
      "train loss:0.937052671666385\n",
      "train loss:0.93650839695473\n",
      "train loss:0.9121844138497213\n",
      "train loss:1.0409681948392293\n",
      "train loss:1.0357006610136223\n",
      "train loss:1.0296694489384997\n",
      "train loss:1.1350005101870924\n",
      "train loss:1.07879363675664\n",
      "train loss:0.9960911760770267\n",
      "train loss:0.9504017642877417\n",
      "train loss:0.9353007549226064\n",
      "train loss:1.049019400005788\n",
      "train loss:1.2925592622148185\n",
      "train loss:1.0412541854467794\n",
      "train loss:1.117460001537833\n",
      "train loss:1.0062299403535575\n",
      "train loss:1.1092834802236924\n",
      "train loss:0.926159702167169\n",
      "train loss:1.153721758332843\n",
      "train loss:1.0581401939695705\n",
      "train loss:0.9730596360939948\n",
      "train loss:0.9778879637353808\n",
      "train loss:1.0443647140399313\n",
      "train loss:1.1458441825162033\n",
      "train loss:0.9434541134817451\n",
      "train loss:1.1018019023076313\n",
      "train loss:1.0181038846041077\n",
      "train loss:1.040464571512296\n",
      "train loss:0.8680268423860718\n",
      "train loss:0.875014834058208\n",
      "train loss:0.9797425159781052\n",
      "train loss:1.1695979301493689\n",
      "train loss:1.1134070052331126\n",
      "train loss:0.9639350922083247\n",
      "train loss:1.0615582671251518\n",
      "train loss:0.9726526374947264\n",
      "train loss:1.0524094187656754\n",
      "train loss:0.9605386017409415\n",
      "train loss:1.0654463353699613\n",
      "train loss:1.036331068609599\n",
      "train loss:1.0776996122073417\n",
      "train loss:1.2923157703122345\n",
      "train loss:0.9059817042657552\n",
      "train loss:1.0842841351265662\n",
      "train loss:0.9962956427422556\n",
      "train loss:1.1898088009365686\n",
      "train loss:0.9605923410015788\n",
      "train loss:0.8336806782681941\n",
      "train loss:1.0836727418301642\n",
      "train loss:1.0772208278121302\n",
      "train loss:1.146389097279724\n",
      "train loss:1.0283715499723924\n",
      "train loss:0.9081378667819409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0777152048023528\n",
      "train loss:0.9890311353519691\n",
      "train loss:1.0840251166132224\n",
      "train loss:1.0757550754538254\n",
      "train loss:1.2319138009660042\n",
      "train loss:1.0758410107376017\n",
      "train loss:0.8516186109216668\n",
      "train loss:0.9405203198117021\n",
      "train loss:0.8706797845701169\n",
      "train loss:1.2582700027640228\n",
      "train loss:0.9726162162427171\n",
      "train loss:0.8877852829306385\n",
      "train loss:1.0824275028620047\n",
      "train loss:1.075468537867948\n",
      "train loss:0.9344306780195393\n",
      "train loss:0.8719998335691905\n",
      "train loss:0.9345009272521411\n",
      "train loss:0.8477835723740961\n",
      "train loss:0.964309798138159\n",
      "train loss:1.0209895293303684\n",
      "train loss:1.1380627487742878\n",
      "train loss:1.0275920797383569\n",
      "train loss:0.9066251477567536\n",
      "train loss:1.0127038391395145\n",
      "train loss:0.9617337557163915\n",
      "train loss:0.9782349642184472\n",
      "train loss:1.037527962160439\n",
      "train loss:1.0034141630104791\n",
      "train loss:1.1134439094649775\n",
      "train loss:0.8635670635682088\n",
      "train loss:0.9768625944954538\n",
      "train loss:1.1339397629051426\n",
      "train loss:0.9980868854956612\n",
      "train loss:1.0349310545634216\n",
      "train loss:0.9286085845718504\n",
      "train loss:1.0260524257565333\n",
      "train loss:1.00401085045604\n",
      "train loss:0.9955099412567421\n",
      "train loss:0.9268116978593152\n",
      "train loss:0.871858143231589\n",
      "train loss:0.9911348484798838\n",
      "train loss:1.18953850045433\n",
      "train loss:0.9886897032631956\n",
      "train loss:1.0287563715526182\n",
      "train loss:1.0509482561800438\n",
      "train loss:1.1501385065356384\n",
      "train loss:1.053232390361522\n",
      "train loss:1.045175045291902\n",
      "train loss:1.0670692256264847\n",
      "train loss:0.9374610987369897\n",
      "train loss:1.0767206836368377\n",
      "train loss:1.0077784312862352\n",
      "train loss:0.9632693112305666\n",
      "train loss:1.0787724228323916\n",
      "train loss:0.9906876596868741\n",
      "train loss:0.893899964319951\n",
      "train loss:1.1010545258864426\n",
      "train loss:1.07107075982244\n",
      "train loss:1.1193879935253173\n",
      "train loss:1.146089852170499\n",
      "train loss:0.9590048789894736\n",
      "train loss:1.121325398746765\n",
      "train loss:1.0557036564382427\n",
      "train loss:1.0113869817981844\n",
      "train loss:1.2162932793750263\n",
      "train loss:1.0326322883754242\n",
      "train loss:1.1085178038006902\n",
      "train loss:1.0418291980652767\n",
      "train loss:0.7243679076540637\n",
      "train loss:0.9369342005215897\n",
      "train loss:0.9119716507768237\n",
      "train loss:0.8871927896343488\n",
      "train loss:0.8949907727365222\n",
      "train loss:1.0028700009286022\n",
      "train loss:0.951616190714384\n",
      "train loss:0.9656649069082889\n",
      "train loss:1.1586776828360774\n",
      "train loss:0.9754695089365046\n",
      "train loss:0.9204787883975167\n",
      "train loss:1.0268169805739027\n",
      "train loss:1.1113112238473966\n",
      "train loss:1.144594469023604\n",
      "train loss:1.1307608673754361\n",
      "train loss:1.053051704730806\n",
      "train loss:1.1463470767982513\n",
      "train loss:1.0958904876682667\n",
      "train loss:1.0145752382239603\n",
      "train loss:1.0049177311308457\n",
      "train loss:1.0588522555200426\n",
      "train loss:1.194860966863135\n",
      "train loss:1.2216734851370192\n",
      "train loss:0.825827570609544\n",
      "train loss:0.9547150045703496\n",
      "train loss:1.1766231497480246\n",
      "train loss:0.942244634449171\n",
      "train loss:0.9890786729821803\n",
      "train loss:1.031503658713343\n",
      "train loss:0.843918988829076\n",
      "train loss:1.0815917786277423\n",
      "train loss:1.048163152163983\n",
      "train loss:1.0866748044279047\n",
      "train loss:1.035905272280808\n",
      "train loss:1.0095399508652467\n",
      "train loss:1.0017996597184269\n",
      "=== epoch:5, train acc:0.974, test acc:0.971 ===\n",
      "train loss:0.9124637388557525\n",
      "train loss:1.061493675643034\n",
      "train loss:1.0039031365620763\n",
      "train loss:1.0724812290195196\n",
      "train loss:1.0178621551835163\n",
      "train loss:1.0789706008577244\n",
      "train loss:1.0446990258105493\n",
      "train loss:0.934209789454116\n",
      "train loss:1.1682624617094812\n",
      "train loss:1.1181933005974671\n",
      "train loss:1.050887916336425\n",
      "train loss:0.9507937822046838\n",
      "train loss:1.0797422603768587\n",
      "train loss:1.052109913769465\n",
      "train loss:0.9265574914825813\n",
      "train loss:1.0519743256405636\n",
      "train loss:0.999863192006311\n",
      "train loss:0.9480795707331094\n",
      "train loss:0.9378929576325011\n",
      "train loss:0.9960976448736545\n",
      "train loss:0.9214992258329445\n",
      "train loss:0.9722538998442073\n",
      "train loss:1.0175820513457792\n",
      "train loss:0.826447521458871\n",
      "train loss:0.9530393370842353\n",
      "train loss:0.9651920774157187\n",
      "train loss:1.0857946243980405\n",
      "train loss:1.1657699263803314\n",
      "train loss:0.9367222011135129\n",
      "train loss:0.9730995570650528\n",
      "train loss:0.9943791875373552\n",
      "train loss:1.122138411707975\n",
      "train loss:0.9687482783998489\n",
      "train loss:1.0774157649112635\n",
      "train loss:1.011702954154263\n",
      "train loss:1.0686800797728913\n",
      "train loss:1.137970362695197\n",
      "train loss:1.10047116296903\n",
      "train loss:1.0597246204491646\n",
      "train loss:0.9357461422409676\n",
      "train loss:1.1970772654559871\n",
      "train loss:1.0009617896118417\n",
      "train loss:1.0595121969948513\n",
      "train loss:0.814107004704942\n",
      "train loss:1.1853564561453611\n",
      "train loss:1.0875807822392252\n",
      "train loss:0.9789319980246642\n",
      "train loss:0.9893375310742201\n",
      "train loss:1.0146821379948414\n",
      "train loss:0.9845487258086739\n",
      "train loss:1.029472269241551\n",
      "train loss:0.9093018795509645\n",
      "train loss:0.9456849876492303\n",
      "train loss:1.1278927464897004\n",
      "train loss:1.0854135267609228\n",
      "train loss:1.091336004117078\n",
      "train loss:0.8843316886481383\n",
      "train loss:0.8602364220272369\n",
      "train loss:1.12911321538884\n",
      "train loss:0.8775617257929562\n",
      "train loss:0.8915363813613175\n",
      "train loss:0.9265673639064724\n",
      "train loss:0.9318584443866986\n",
      "train loss:1.0864122992382017\n",
      "train loss:1.029582225599637\n",
      "train loss:0.8799471908356484\n",
      "train loss:0.9872024137335145\n",
      "train loss:0.9920574977698302\n",
      "train loss:1.0011928514941382\n",
      "train loss:0.9524671950332868\n",
      "train loss:0.8243234607006706\n",
      "train loss:0.9068873633183506\n",
      "train loss:0.918211623087431\n",
      "train loss:1.0195006203286547\n",
      "train loss:0.9417142707242014\n",
      "train loss:0.9350979109832079\n",
      "train loss:0.9673050209349104\n",
      "train loss:1.1384552577745286\n",
      "train loss:0.8733797108145969\n",
      "train loss:0.952371545542344\n",
      "train loss:1.0490991095869986\n",
      "train loss:1.1014274829131532\n",
      "train loss:1.1792754054587575\n",
      "train loss:1.0933439892968337\n",
      "train loss:0.9078864219294183\n",
      "train loss:0.9047964192884355\n",
      "train loss:1.0222890125923108\n",
      "train loss:1.1426005422010765\n",
      "train loss:1.1058357816759639\n",
      "train loss:0.9887733772755801\n",
      "train loss:1.1751568834433967\n",
      "train loss:0.9651768562792367\n",
      "train loss:0.9904398602665347\n",
      "train loss:0.838658338263791\n",
      "train loss:0.9186467037966919\n",
      "train loss:0.9228399209819156\n",
      "train loss:1.020983827677785\n",
      "train loss:0.897443964926352\n",
      "train loss:0.9259172328691263\n",
      "train loss:0.9254240410288312\n",
      "train loss:1.0307997067441501\n",
      "train loss:0.9683083060348291\n",
      "train loss:0.9674234813970491\n",
      "train loss:1.1209655005984471\n",
      "train loss:1.08792613852266\n",
      "train loss:1.0554024801504927\n",
      "train loss:0.7977547607333532\n",
      "train loss:1.0100416814640296\n",
      "train loss:0.9617058468530001\n",
      "train loss:1.1026486197787329\n",
      "train loss:1.107183517968744\n",
      "train loss:0.83463787265872\n",
      "train loss:1.0552594095644536\n",
      "train loss:0.9874663372502083\n",
      "train loss:0.9422013185764933\n",
      "train loss:0.9540661460194759\n",
      "train loss:1.0805843943948843\n",
      "train loss:1.091960143742534\n",
      "train loss:1.1428027080022372\n",
      "train loss:0.9226669824964782\n",
      "train loss:1.0202812448839083\n",
      "train loss:0.9673770490985014\n",
      "train loss:1.139094008305608\n",
      "train loss:0.9706526515538174\n",
      "train loss:0.841808770475316\n",
      "train loss:0.8840121630271597\n",
      "train loss:1.004286226692515\n",
      "train loss:0.9726023678270562\n",
      "train loss:1.052737603097502\n",
      "train loss:1.1048260717019607\n",
      "train loss:0.95425878906836\n",
      "train loss:1.17761048084079\n",
      "train loss:1.0595649004241507\n",
      "train loss:0.9186340473807438\n",
      "train loss:1.0827696213665987\n",
      "train loss:0.9757719261853408\n",
      "train loss:1.0260268338248388\n",
      "train loss:0.9629744361492891\n",
      "train loss:1.0652265798877094\n",
      "train loss:1.1625800736803946\n",
      "train loss:1.1249882544579863\n",
      "train loss:0.9728804919628753\n",
      "train loss:1.0462486648610547\n",
      "train loss:1.071240232376841\n",
      "train loss:0.8083461440182433\n",
      "train loss:0.8761672439948314\n",
      "train loss:0.9019511139924198\n",
      "train loss:1.1747552440176565\n",
      "train loss:0.9668887714601739\n",
      "train loss:0.932938440442225\n",
      "train loss:1.0765174160926083\n",
      "train loss:1.0020918456930392\n",
      "train loss:1.0640656899923224\n",
      "train loss:1.2310830737177318\n",
      "train loss:0.8509834451419164\n",
      "train loss:1.0748078767910647\n",
      "train loss:0.9240999193053447\n",
      "train loss:0.927871489891343\n",
      "train loss:1.0691714014964488\n",
      "train loss:0.8671153098398738\n",
      "train loss:1.1236047677091654\n",
      "train loss:0.8173936521708456\n",
      "train loss:0.9439818892142455\n",
      "train loss:0.9524973897244027\n",
      "train loss:1.1362609287894878\n",
      "train loss:1.0331562290421208\n",
      "train loss:1.0699756827302662\n",
      "train loss:0.8182984357538546\n",
      "train loss:1.204315367333726\n",
      "train loss:0.9319354279603245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0241985696937324\n",
      "train loss:1.1123455919126004\n",
      "train loss:0.9214248465846663\n",
      "train loss:1.0183231486651128\n",
      "train loss:1.019638730527521\n",
      "train loss:1.038120486474974\n",
      "train loss:0.9156729366075744\n",
      "train loss:1.099485691842083\n",
      "train loss:1.034907792427221\n",
      "train loss:0.9643376868298517\n",
      "train loss:0.9531999279465683\n",
      "train loss:1.07451282670266\n",
      "train loss:1.09359845649718\n",
      "train loss:0.8790393884493366\n",
      "train loss:1.041387818589572\n",
      "train loss:1.01764335256484\n",
      "train loss:0.8878465990148173\n",
      "train loss:0.9986530968620179\n",
      "train loss:1.09444212835569\n",
      "train loss:1.039885201661877\n",
      "train loss:1.0485340263167346\n",
      "train loss:1.1233574648405389\n",
      "train loss:0.9774413168000262\n",
      "train loss:1.0583355241001167\n",
      "train loss:1.1270384819901758\n",
      "train loss:1.0759748950067236\n",
      "train loss:0.9840232471299984\n",
      "train loss:1.117442735537715\n",
      "train loss:1.2029822774303354\n",
      "train loss:1.0674889882134893\n",
      "train loss:1.0627628107527292\n",
      "train loss:1.1043162486976386\n",
      "train loss:1.1465143092644965\n",
      "train loss:0.9875419258598321\n",
      "train loss:0.9751495265799102\n",
      "train loss:1.2047226283664083\n",
      "train loss:1.0956153911679227\n",
      "train loss:0.9397520608838561\n",
      "train loss:1.2813259178807346\n",
      "train loss:1.0203107256791397\n",
      "train loss:1.0847474106703132\n",
      "train loss:0.8636668283246246\n",
      "train loss:0.9727571732667404\n",
      "train loss:0.9863040761526801\n",
      "train loss:0.8545845688216188\n",
      "train loss:0.9850611599799508\n",
      "train loss:1.1237060823893816\n",
      "train loss:0.8354768712154802\n",
      "train loss:1.053161182274984\n",
      "train loss:1.034115549709498\n",
      "train loss:1.113089628312797\n",
      "train loss:0.9216200974333765\n",
      "train loss:0.8457054999822159\n",
      "train loss:0.9731497446276776\n",
      "train loss:1.1281096021625223\n",
      "train loss:0.9377171989734729\n",
      "train loss:0.6817205853129503\n",
      "train loss:1.0128653475423832\n",
      "train loss:1.007553661627709\n",
      "train loss:1.1070751347626762\n",
      "train loss:1.104314536310986\n",
      "train loss:1.0577277132836993\n",
      "train loss:1.1250121602565408\n",
      "train loss:1.083818988880097\n",
      "train loss:1.0322658820859008\n",
      "train loss:0.930681045073991\n",
      "train loss:0.9109556479288886\n",
      "train loss:1.0372542139509984\n",
      "train loss:0.7721830720721735\n",
      "train loss:1.0394224838187658\n",
      "train loss:0.9800328156582967\n",
      "train loss:0.9782255468834578\n",
      "train loss:0.8117080998854834\n",
      "train loss:1.023579061641064\n",
      "train loss:1.0320740891649145\n",
      "train loss:1.0751302031145995\n",
      "train loss:1.0726102690424824\n",
      "train loss:0.840460015405115\n",
      "train loss:0.8925488358245781\n",
      "train loss:0.8019480409464431\n",
      "train loss:1.0140097338572533\n",
      "train loss:0.7928148623602651\n",
      "train loss:0.9524974794619568\n",
      "train loss:0.9780976097369727\n",
      "train loss:0.8599071963831002\n",
      "train loss:0.9903781256359042\n",
      "train loss:0.9373189214682626\n",
      "train loss:0.8392259499739448\n",
      "train loss:0.9467854681728276\n",
      "train loss:1.099034511623891\n",
      "train loss:0.9632861982043519\n",
      "train loss:1.004938971874297\n",
      "train loss:0.9590910163610252\n",
      "train loss:1.0010863334136162\n",
      "train loss:1.0134724906608839\n",
      "train loss:1.0351696744633527\n",
      "train loss:0.8361988158081662\n",
      "train loss:0.9687571490496437\n",
      "train loss:0.8523047564779607\n",
      "train loss:0.9723205815550422\n",
      "train loss:1.0181980335399605\n",
      "train loss:1.0632974641832147\n",
      "train loss:0.9876707124175423\n",
      "train loss:0.9413583937670889\n",
      "train loss:1.0289731185835833\n",
      "train loss:1.037324745843902\n",
      "train loss:0.8989623306833944\n",
      "train loss:0.9294199600396448\n",
      "train loss:1.0606962548389813\n",
      "train loss:1.0329190036276221\n",
      "train loss:1.257030910640238\n",
      "train loss:0.8360151063356577\n",
      "train loss:0.9522847568669716\n",
      "train loss:0.9737896766793197\n",
      "train loss:0.9891431340241392\n",
      "train loss:0.9881457189325268\n",
      "train loss:0.8539943329959613\n",
      "train loss:0.9121204555508159\n",
      "train loss:0.9366376087152578\n",
      "train loss:1.1070769227286017\n",
      "train loss:0.9033298600291221\n",
      "train loss:1.0403299097897842\n",
      "train loss:0.9849706663372753\n",
      "train loss:0.9538450689658631\n",
      "train loss:0.9367045245173615\n",
      "train loss:1.0942259512369088\n",
      "train loss:1.019609611935418\n",
      "train loss:1.0643664165786848\n",
      "train loss:0.9471551629049809\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.983\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "network = DeepConvNet()\n",
    "trainer = Trainer(network, X_train2, model_pred, X_test2, y_test2,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.983\n"
     ]
    }
   ],
   "source": [
    "test_acc = network.accuracy(X_test2, y_test2)\n",
    "print(\"\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "network.save_params(\"keras_clone_params_half.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# \n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('graph_keras_clone_half.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
