{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "%matplotlib inline\n",
    "\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# 3次元から4次元へ\n",
    "x_train = x_train.reshape(-1, 1, 28, 28).astype(np.float32) / 255\n",
    "x_test = x_test.reshape(-1, 1, 28, 28).astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_noise_list(x_list, eps):\n",
    "    noise_list = []\n",
    "    for x in x_list:\n",
    "        random_noise1 = np.random.normal(loc = 0, scale = 1, size = (1, 28, 28))\n",
    "        noise = (x + eps * np.sign(random_noise1)).clip(min=0, max=1)\n",
    "        noise_list.append(noise)\n",
    "        \n",
    "    noise_list = np.array(noise_list)\n",
    "    return noise_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.3\n",
    "max_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_train = change_noise_list(x_train, eps)\n",
    "noised_test = change_noise_list(x_test, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "network = DeepConvNet()\n",
    "\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "print(\"loaded Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "無加工に対する正答率： 0.9925\n",
      "ランダムノイズに対する正答率： 0.9016\n"
     ]
    }
   ],
   "source": [
    "print(\"無加工に対する正答率：\", network.accuracy(x_test, t_test))\n",
    "print(\"ランダムノイズに対する正答率：\", network.accuracy(noised_test, t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9929028314983925\n",
      "=== epoch:1, train acc:0.966, test acc:0.989 ===\n",
      "train loss:0.9979099109831453\n",
      "train loss:1.0804025737995908\n",
      "train loss:1.023863455602601\n",
      "train loss:0.908619124684028\n",
      "train loss:1.0024604261538117\n",
      "train loss:0.8023029835340596\n",
      "train loss:1.2311385297178923\n",
      "train loss:1.1766909519729492\n",
      "train loss:0.9496445652022304\n",
      "train loss:0.9510091617591542\n",
      "train loss:1.0462389716901128\n",
      "train loss:0.8099298534572493\n",
      "train loss:0.9583526319027016\n",
      "train loss:0.9409329137179371\n",
      "train loss:1.0258743074365604\n",
      "train loss:0.7709389947208042\n",
      "train loss:0.7791716404262296\n",
      "train loss:0.8954277693486788\n",
      "train loss:0.9604298261048286\n",
      "train loss:0.9407032938792124\n",
      "train loss:0.9747955949549646\n",
      "train loss:0.7835030320513751\n",
      "train loss:1.0768860912887894\n",
      "train loss:0.8891047149912406\n",
      "train loss:0.9042294500807273\n",
      "train loss:0.980345089269925\n",
      "train loss:1.0109748198726256\n",
      "train loss:0.9737820489120618\n",
      "train loss:0.8533919513587643\n",
      "train loss:0.998063512565716\n",
      "train loss:0.6814866331136202\n",
      "train loss:0.8050336545343884\n",
      "train loss:0.8023187251377472\n",
      "train loss:0.9931279460074748\n",
      "train loss:1.0853345166966917\n",
      "train loss:0.8272433230126524\n",
      "train loss:1.0168721619827135\n",
      "train loss:0.8469774188966316\n",
      "train loss:0.8109816768760606\n",
      "train loss:1.0320477791103995\n",
      "train loss:0.8887861297253221\n",
      "train loss:0.9621466162694007\n",
      "train loss:0.9628132406207536\n",
      "train loss:0.8393338062521697\n",
      "train loss:0.8093196827661652\n",
      "train loss:1.0462446600367992\n",
      "train loss:0.9035484647180365\n",
      "train loss:0.9570734038318065\n",
      "train loss:0.889377082123277\n",
      "train loss:0.8338039932802204\n",
      "train loss:0.9378290649567751\n",
      "train loss:0.856789474990897\n",
      "train loss:0.9758810661719441\n",
      "train loss:0.9723098337254616\n",
      "train loss:1.0455749536802819\n",
      "train loss:0.9724078351847399\n",
      "train loss:0.9340516073087073\n",
      "train loss:0.817247510281411\n",
      "train loss:0.8892297995231647\n",
      "train loss:1.1009420419723295\n",
      "train loss:0.8202388648260514\n",
      "train loss:0.9358707410061906\n",
      "train loss:0.8734731914646571\n",
      "train loss:1.0276322685842882\n",
      "train loss:0.9475567790524599\n",
      "train loss:0.7772583910825185\n",
      "train loss:1.0441986938471854\n",
      "train loss:0.9288983215676481\n",
      "train loss:0.9956647497103454\n",
      "train loss:0.9984365898013801\n",
      "train loss:0.7809886471051997\n",
      "train loss:1.018074906427487\n",
      "train loss:0.9838160330637074\n",
      "train loss:0.9517933386425357\n",
      "train loss:0.881797420029212\n",
      "train loss:0.9238264333950016\n",
      "train loss:0.9299173079440359\n",
      "train loss:1.1058456057172743\n",
      "train loss:0.8214286964695618\n",
      "train loss:0.8882155741574321\n",
      "train loss:0.9475166061591254\n",
      "train loss:0.9612695972704423\n",
      "train loss:1.1351528462674867\n",
      "train loss:1.0069205507603995\n",
      "train loss:0.7935966163201352\n",
      "train loss:0.7994522605218534\n",
      "train loss:0.9694195629671941\n",
      "train loss:0.9420906774528319\n",
      "train loss:0.9282670408762398\n",
      "train loss:0.8482398606436158\n",
      "train loss:0.8523245684726904\n",
      "train loss:0.7469925827456279\n",
      "train loss:0.9372506931988748\n",
      "train loss:0.8316590821002801\n",
      "train loss:0.8814686463842116\n",
      "train loss:0.9539109437086125\n",
      "train loss:0.9387176120373012\n",
      "train loss:1.053514490379768\n",
      "train loss:0.891605840296809\n",
      "train loss:0.9845628112594774\n",
      "train loss:0.8734166497592057\n",
      "train loss:0.9431989955383215\n",
      "train loss:0.8022821174577035\n",
      "train loss:0.957541226072644\n",
      "train loss:1.0422142137366164\n",
      "train loss:0.9604869424067717\n",
      "train loss:0.9188919919225083\n",
      "train loss:0.8657226750524813\n",
      "train loss:0.8294712959366852\n",
      "train loss:0.8948732301500635\n",
      "train loss:0.9084654210454066\n",
      "train loss:1.051299267101227\n",
      "train loss:0.812678697544595\n",
      "train loss:0.7495387101470754\n",
      "train loss:0.8966753124631686\n",
      "train loss:0.8112263141297217\n",
      "train loss:1.0820477843478788\n",
      "train loss:0.807190151544987\n",
      "train loss:0.8568772753007516\n",
      "train loss:0.8309313610162373\n",
      "train loss:0.8414890243384968\n",
      "train loss:0.8813886990491081\n",
      "train loss:0.8077600016838852\n",
      "train loss:0.9667201258842116\n",
      "train loss:0.791286061796691\n",
      "train loss:0.9619644188090822\n",
      "train loss:0.8448604970662608\n",
      "train loss:0.8279548418759944\n",
      "train loss:0.9410630916041728\n",
      "train loss:0.9246506179356316\n",
      "train loss:0.8993825807064298\n",
      "train loss:0.9568802160952528\n",
      "train loss:1.0174648932776937\n",
      "train loss:0.9054133205910248\n",
      "train loss:1.05072825841932\n",
      "train loss:0.8217336189531315\n",
      "train loss:1.1567116616725903\n",
      "train loss:0.7507564428597864\n",
      "train loss:0.8115728795114329\n",
      "train loss:0.9243137939634528\n",
      "train loss:1.1365067636795958\n",
      "train loss:0.9778981339201223\n",
      "train loss:1.0058636130186052\n",
      "train loss:0.9256619514452388\n",
      "train loss:0.8551139494877027\n",
      "train loss:1.02430727726668\n",
      "train loss:0.8955951926013053\n",
      "train loss:0.8046658652194254\n",
      "train loss:1.0226950629173355\n",
      "train loss:0.9939472714986977\n",
      "train loss:1.0596900765630672\n",
      "train loss:0.8348387626274875\n",
      "train loss:0.9686388842302485\n",
      "train loss:0.8601208375092778\n",
      "train loss:0.8968087902347058\n",
      "train loss:0.9150495425905463\n",
      "train loss:1.0159726513091223\n",
      "train loss:0.84188304688888\n",
      "train loss:0.9112415340636164\n",
      "train loss:0.9434545051556236\n",
      "train loss:1.0139882535920808\n",
      "train loss:0.9994070162766278\n",
      "train loss:0.8287812686648652\n",
      "train loss:0.8412915887423612\n",
      "train loss:0.9149079071535716\n",
      "train loss:1.137361170278593\n",
      "train loss:0.9191542471621987\n",
      "train loss:0.9171625283134597\n",
      "train loss:0.7784297129690241\n",
      "train loss:0.9106875254195037\n",
      "train loss:0.8755282801629378\n",
      "train loss:0.7795476303711201\n",
      "train loss:0.9651131612091116\n",
      "train loss:1.0909078866753474\n",
      "train loss:0.7900875219938039\n",
      "train loss:0.7515384176918868\n",
      "train loss:0.9264180386116045\n",
      "train loss:0.9690583671596272\n",
      "train loss:0.9850532052136088\n",
      "train loss:0.965855988460997\n",
      "train loss:0.8660335934960063\n",
      "train loss:1.0714253996813934\n",
      "train loss:0.7004617398949741\n",
      "train loss:0.8445302633606262\n",
      "train loss:0.9617269082373369\n",
      "train loss:0.9833055467578398\n",
      "train loss:0.8912812667577295\n",
      "train loss:0.8826587408844594\n",
      "train loss:0.8718141249997933\n",
      "train loss:0.8710102419671693\n",
      "train loss:0.8709585464305566\n",
      "train loss:0.9055465692715846\n",
      "train loss:1.073118630700247\n",
      "train loss:0.9263887470239627\n",
      "train loss:0.8749096758588384\n",
      "train loss:0.9189066504968559\n",
      "train loss:1.0570665033313107\n",
      "train loss:0.8036504411891521\n",
      "train loss:0.83965592960702\n",
      "train loss:0.7916506726016784\n",
      "train loss:0.8912086844199898\n",
      "train loss:0.953132008342703\n",
      "train loss:0.9771840731685525\n",
      "train loss:1.0062802159122965\n",
      "train loss:0.9078341882741663\n",
      "train loss:0.932553552729956\n",
      "train loss:1.0335317773388768\n",
      "train loss:0.8037430493957585\n",
      "train loss:0.8964808133503138\n",
      "train loss:0.8743725923710386\n",
      "train loss:1.0565145419769535\n",
      "train loss:1.1270225646901364\n",
      "train loss:0.8083001260272344\n",
      "train loss:0.9507453174532126\n",
      "train loss:0.910129670500021\n",
      "train loss:0.9029975054746625\n",
      "train loss:0.8718204425159068\n",
      "train loss:0.9749229852494231\n",
      "train loss:0.9385981158158778\n",
      "train loss:0.8262850695988299\n",
      "train loss:0.9232204253705147\n",
      "train loss:0.9060928724139727\n",
      "train loss:0.8026859947848165\n",
      "train loss:0.8514691977281145\n",
      "train loss:0.9204708002592341\n",
      "train loss:0.8842827115807149\n",
      "train loss:0.9406978740931584\n",
      "train loss:0.9147549156984164\n",
      "train loss:0.8366298773880405\n",
      "train loss:0.9409926543442243\n",
      "train loss:0.9296820954786589\n",
      "train loss:0.8879394431647668\n",
      "train loss:0.7359076191280205\n",
      "train loss:0.8973963492794035\n",
      "train loss:0.9826181366773157\n",
      "train loss:0.8388193180674841\n",
      "train loss:0.8342052985918563\n",
      "train loss:1.0314982115696023\n",
      "train loss:0.9905257865923491\n",
      "train loss:0.9043699735825821\n",
      "train loss:0.9320360404778237\n",
      "train loss:0.8582751406541926\n",
      "train loss:0.9318999230856403\n",
      "train loss:0.9863045769183694\n",
      "train loss:0.7674874565300267\n",
      "train loss:0.8439915437728835\n",
      "train loss:0.8322494620692937\n",
      "train loss:0.9509432778510035\n",
      "train loss:0.9616366136251314\n",
      "train loss:0.9485102331886444\n",
      "train loss:0.7529282081610608\n",
      "train loss:0.8882366380519912\n",
      "train loss:0.7669821928436573\n",
      "train loss:0.9029085647117584\n",
      "train loss:0.9183631539986494\n",
      "train loss:0.8957568595361259\n",
      "train loss:0.8744673441318908\n",
      "train loss:0.9402934202038611\n",
      "train loss:0.8122486649711987\n",
      "train loss:0.9396811715552098\n",
      "train loss:0.8565432579916602\n",
      "train loss:0.8019137430856972\n",
      "train loss:0.9635285053874214\n",
      "train loss:0.83988874791232\n",
      "train loss:1.160744093906709\n",
      "train loss:0.8302181926987834\n",
      "train loss:0.8482301744011674\n",
      "train loss:0.9306607835968309\n",
      "train loss:1.0112408610356605\n",
      "train loss:0.9165989858260292\n",
      "train loss:0.9084008752526755\n",
      "train loss:0.8550503221324648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8769102124981355\n",
      "train loss:0.8366505942509689\n",
      "train loss:0.7838032778667423\n",
      "train loss:0.9474111357323232\n",
      "train loss:1.0837107379515947\n",
      "train loss:0.9286917424505029\n",
      "train loss:0.9371510566905027\n",
      "train loss:1.0550853209876\n",
      "train loss:0.7531066500034664\n",
      "train loss:0.9693563508080574\n",
      "train loss:1.1162505199960762\n",
      "train loss:0.891424258423101\n",
      "train loss:0.7723646368683893\n",
      "train loss:1.0054490952802555\n",
      "train loss:0.8512923876697776\n",
      "train loss:0.8021132742412068\n",
      "train loss:0.9496008541113659\n",
      "train loss:0.9128569702536927\n",
      "train loss:1.116372653011012\n",
      "train loss:1.0448173527932054\n",
      "train loss:0.9306691425407719\n",
      "train loss:0.9209227850631418\n",
      "train loss:0.8710324124803626\n",
      "train loss:1.0185840092478908\n",
      "train loss:1.0291930543268504\n",
      "train loss:0.9135571041353484\n",
      "train loss:0.9600641023556465\n",
      "train loss:1.000621364261679\n",
      "train loss:0.8887314764850011\n",
      "train loss:0.9028752436433627\n",
      "train loss:0.8064763177533788\n",
      "train loss:1.0308674649194496\n",
      "train loss:1.08572253132046\n",
      "train loss:0.7874348911409659\n",
      "train loss:0.898307447158718\n",
      "train loss:0.8750113312486192\n",
      "train loss:0.8018997271369763\n",
      "train loss:0.9105858656765542\n",
      "train loss:0.8921753596361219\n",
      "train loss:0.8457432955705143\n",
      "train loss:0.8767304772012042\n",
      "train loss:0.9314708392163605\n",
      "train loss:0.9078093675891032\n",
      "train loss:0.6825096883031552\n",
      "train loss:0.9363999724500545\n",
      "train loss:0.8894985212837924\n",
      "train loss:1.0124849952337336\n",
      "train loss:1.0723993837587182\n",
      "train loss:0.9297877169043807\n",
      "train loss:1.0060231367969994\n",
      "train loss:0.8329581213946499\n",
      "train loss:0.793466189461498\n",
      "train loss:0.8774390762171883\n",
      "train loss:0.8429765760390647\n",
      "train loss:0.690727972901541\n",
      "train loss:0.8858246340472031\n",
      "train loss:1.0469106141602047\n",
      "train loss:0.982315685074539\n",
      "train loss:0.8025971108633653\n",
      "train loss:0.7773124472615242\n",
      "train loss:1.046714944414066\n",
      "train loss:0.8767058731790668\n",
      "train loss:0.980535041709552\n",
      "train loss:0.9797367652807432\n",
      "train loss:0.9800562363885149\n",
      "train loss:0.8145445527152073\n",
      "train loss:0.9738997326288791\n",
      "train loss:1.0578904577943633\n",
      "train loss:0.8894531113103943\n",
      "train loss:0.7690500510514812\n",
      "train loss:1.0479599746379187\n",
      "train loss:0.7596954336820122\n",
      "train loss:0.9003262609086968\n",
      "train loss:0.9653361059816487\n",
      "train loss:0.9139651713093131\n",
      "train loss:1.0060361714671742\n",
      "train loss:0.8858605848956443\n",
      "train loss:1.033253848763031\n",
      "train loss:0.974218837458023\n",
      "train loss:0.8166006664024116\n",
      "train loss:0.8914073630411842\n",
      "train loss:0.8458673204614202\n",
      "train loss:0.7746677354551831\n",
      "train loss:0.8907714012611947\n",
      "train loss:0.9297781407699103\n",
      "train loss:0.8814114900085097\n",
      "train loss:0.8970169585670115\n",
      "train loss:0.9879962774265562\n",
      "train loss:0.8012485242421211\n",
      "train loss:0.8743896058713773\n",
      "train loss:1.0239729774151558\n",
      "train loss:0.9546066816965195\n",
      "train loss:0.9631952918640843\n",
      "train loss:0.8262471859662497\n",
      "train loss:0.9627443711375007\n",
      "train loss:0.8477525795947163\n",
      "train loss:0.9510834194203289\n",
      "train loss:0.9075486389754879\n",
      "train loss:0.9433367738201716\n",
      "train loss:1.0174669821975038\n",
      "train loss:0.9861712489317153\n",
      "train loss:0.8432033224683206\n",
      "train loss:0.9418570990722424\n",
      "train loss:1.0711716681167562\n",
      "train loss:0.7877745220343222\n",
      "train loss:0.8981626524429258\n",
      "train loss:0.763281859974306\n",
      "train loss:0.7030778278979295\n",
      "train loss:0.7782306473096267\n",
      "train loss:0.9193426258587156\n",
      "train loss:0.8627636440741863\n",
      "train loss:1.0093305762712201\n",
      "train loss:0.7740966772504848\n",
      "train loss:0.683887183950955\n",
      "train loss:0.945758963640368\n",
      "train loss:1.0047678272456337\n",
      "train loss:0.8884731140862135\n",
      "train loss:0.8915980773041765\n",
      "train loss:0.8940872475148816\n",
      "train loss:0.9961442801728373\n",
      "train loss:0.8391222967896832\n",
      "train loss:0.8897839662971064\n",
      "train loss:1.0523977008948793\n",
      "train loss:0.8816333327841467\n",
      "train loss:0.8068938126614738\n",
      "train loss:0.935355141072903\n",
      "train loss:0.8628523021309519\n",
      "train loss:0.9165339747857274\n",
      "train loss:0.935159660188397\n",
      "train loss:0.7644710196878775\n",
      "train loss:0.9436793342796982\n",
      "train loss:0.7747780473765836\n",
      "train loss:0.8684149977366447\n",
      "train loss:0.8516226734443493\n",
      "train loss:1.0329151621929034\n",
      "train loss:0.9803530184476671\n",
      "train loss:0.9950568056442971\n",
      "train loss:1.0478563048094431\n",
      "train loss:0.7644007993236616\n",
      "train loss:0.8286277661939703\n",
      "train loss:0.9879905043410221\n",
      "train loss:0.8978551247451011\n",
      "train loss:0.8402425873298078\n",
      "train loss:0.9858536261827505\n",
      "train loss:0.7974755511661895\n",
      "train loss:0.9368536140223278\n",
      "train loss:0.7834121067916611\n",
      "train loss:0.9218328139402894\n",
      "train loss:0.9831275434364697\n",
      "train loss:0.8821422929871308\n",
      "train loss:0.7322764268867847\n",
      "train loss:0.7902116028203224\n",
      "train loss:0.9778412844220573\n",
      "train loss:0.8955643585040408\n",
      "train loss:0.8635150522066237\n",
      "train loss:0.9471247564341949\n",
      "train loss:1.0697427486987303\n",
      "train loss:0.7806755874522346\n",
      "train loss:0.8274552358318481\n",
      "train loss:0.8335959389341268\n",
      "train loss:0.8480336416208949\n",
      "train loss:0.9399953367134566\n",
      "train loss:0.9427360416124928\n",
      "train loss:0.9907685652553458\n",
      "train loss:0.924560016055776\n",
      "train loss:0.8809816650339797\n",
      "train loss:0.8738990361915154\n",
      "train loss:0.9083776863463617\n",
      "train loss:0.9984343950951804\n",
      "train loss:0.7897413301591167\n",
      "train loss:0.9295522472203338\n",
      "train loss:0.8129641945317492\n",
      "train loss:0.8627190726781552\n",
      "train loss:0.9065464954939515\n",
      "train loss:0.712983239381389\n",
      "train loss:0.8920250299835853\n",
      "train loss:0.886071505077628\n",
      "train loss:0.7025745736722463\n",
      "train loss:0.7273593280853595\n",
      "train loss:0.8415709835842854\n",
      "train loss:1.0111950197971615\n",
      "train loss:0.9094785568104724\n",
      "train loss:0.9411427750700248\n",
      "train loss:0.8486083559608578\n",
      "train loss:1.0172664076848077\n",
      "train loss:0.7822841041485635\n",
      "train loss:0.8532424467970694\n",
      "train loss:0.6745871487156154\n",
      "train loss:0.7174031072151105\n",
      "train loss:0.8464095020040766\n",
      "train loss:0.8235551144038864\n",
      "train loss:0.7382179884022761\n",
      "train loss:0.8594083846196016\n",
      "train loss:0.9415731481016304\n",
      "train loss:0.9863419721468216\n",
      "train loss:1.0047187365208334\n",
      "train loss:0.9411689573224842\n",
      "train loss:0.93239926052482\n",
      "train loss:1.051179624047907\n",
      "train loss:1.1006469619462096\n",
      "train loss:0.8487195663363258\n",
      "train loss:0.9650518057272159\n",
      "train loss:0.8097810237308815\n",
      "train loss:0.792770718827359\n",
      "train loss:0.8409782435952304\n",
      "train loss:0.75721138629385\n",
      "train loss:0.958036733000693\n",
      "train loss:0.8889823261309138\n",
      "train loss:0.8045541824531642\n",
      "train loss:1.0223891932033171\n",
      "train loss:0.9945113479868378\n",
      "train loss:0.8583523612638885\n",
      "train loss:0.9121511950119089\n",
      "train loss:0.7590218405960131\n",
      "train loss:1.0274985028977608\n",
      "train loss:0.8232785025190057\n",
      "train loss:0.8721478876209459\n",
      "train loss:0.8474877298291937\n",
      "train loss:0.8767474037753468\n",
      "train loss:0.9019029703714259\n",
      "train loss:1.0011929785908378\n",
      "train loss:0.9819297252129795\n",
      "train loss:0.8256603432171036\n",
      "train loss:0.7901449782223353\n",
      "train loss:0.8852032300937611\n",
      "train loss:1.0064725570691504\n",
      "train loss:0.9290823473536076\n",
      "train loss:0.8888029154455159\n",
      "train loss:0.8953181060836034\n",
      "train loss:0.8456950672348689\n",
      "train loss:0.8841865288800911\n",
      "train loss:0.8013211985399247\n",
      "train loss:0.9569616964124912\n",
      "train loss:0.8853036157513613\n",
      "train loss:0.8082490723987575\n",
      "train loss:0.9619012468619067\n",
      "train loss:0.8972134703899327\n",
      "train loss:1.0549923978107234\n",
      "train loss:0.8832453254481023\n",
      "train loss:0.8357871449871638\n",
      "train loss:0.8314693163796413\n",
      "train loss:0.8341658262374171\n",
      "train loss:0.7867054415820448\n",
      "train loss:0.8463234119703891\n",
      "train loss:0.9489087108218826\n",
      "train loss:0.9430067388011989\n",
      "train loss:1.1569820262484027\n",
      "train loss:0.8118647604836159\n",
      "train loss:0.9309971560866827\n",
      "train loss:0.9490773567622186\n",
      "train loss:0.9780813608985899\n",
      "train loss:0.960012069142229\n",
      "train loss:1.0485001104869207\n",
      "train loss:1.0159628383401333\n",
      "train loss:0.8652311844882158\n",
      "train loss:0.9470292357767572\n",
      "train loss:0.9611685966260296\n",
      "train loss:0.8964799335295097\n",
      "train loss:1.0188103280561835\n",
      "train loss:0.9362571723820302\n",
      "train loss:0.898979437187793\n",
      "train loss:0.8633333108765416\n",
      "train loss:0.947907336458875\n",
      "train loss:1.0076957681245973\n",
      "train loss:0.901884741912517\n",
      "train loss:0.9107137625071259\n",
      "train loss:0.7177649337760462\n",
      "train loss:0.985989075985789\n",
      "train loss:0.8568233061123202\n",
      "train loss:0.7177723824756731\n",
      "train loss:0.9376004997309139\n",
      "train loss:0.9384012292301833\n",
      "train loss:0.8214621574495168\n",
      "train loss:0.9327472599086291\n",
      "train loss:0.8049741906892962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0332775687070237\n",
      "train loss:0.920546244391634\n",
      "train loss:0.9502706797710574\n",
      "train loss:0.8964338500821117\n",
      "train loss:0.9289226879478604\n",
      "train loss:0.8449276851654308\n",
      "train loss:0.7933498463718746\n",
      "train loss:0.8974846437715976\n",
      "train loss:0.8839996246528032\n",
      "train loss:0.9147098039531152\n",
      "train loss:0.9194263444160569\n",
      "train loss:0.9572167295875742\n",
      "train loss:0.807026402355457\n",
      "train loss:0.9122457731586572\n",
      "train loss:0.7707965411433758\n",
      "train loss:0.8358231623996402\n",
      "train loss:0.881902752651769\n",
      "train loss:0.7577255300663828\n",
      "train loss:0.8911332201970326\n",
      "train loss:0.9221608631040499\n",
      "train loss:0.9672868182912809\n",
      "train loss:0.9017114802434137\n",
      "train loss:0.8598154288470999\n",
      "train loss:0.7859061921070482\n",
      "train loss:0.918645608513502\n",
      "train loss:0.9059716627497006\n",
      "train loss:0.8582924231895247\n",
      "train loss:0.8623243405606842\n",
      "train loss:0.8827653414679533\n",
      "train loss:0.9680645247559627\n",
      "train loss:0.8390608552321348\n",
      "train loss:0.9705714306703023\n",
      "train loss:1.1177428608423463\n",
      "train loss:0.8243719315661656\n",
      "train loss:0.8535908955137036\n",
      "train loss:1.003168734765871\n",
      "train loss:1.0748117974345617\n",
      "train loss:0.9616079015866028\n",
      "train loss:0.9683982192205169\n",
      "train loss:0.9964869988455759\n",
      "train loss:1.0191913063332962\n",
      "train loss:0.9593168153039887\n",
      "train loss:0.7836223815833081\n",
      "train loss:0.8890022443115216\n",
      "train loss:0.9988555658493341\n",
      "train loss:0.8996646407860156\n",
      "train loss:0.6325905189285171\n",
      "train loss:0.7956387168820706\n",
      "train loss:0.7469012362051609\n",
      "train loss:0.9358402591448608\n",
      "train loss:0.7819151439960725\n",
      "train loss:0.9417183724630996\n",
      "train loss:0.9807854827507693\n",
      "=== epoch:2, train acc:0.99, test acc:0.988 ===\n",
      "train loss:0.8135821044769048\n",
      "train loss:1.111473991588439\n",
      "train loss:0.8756948358207692\n",
      "train loss:0.7940096274363987\n",
      "train loss:0.8472995910445085\n",
      "train loss:0.7858219684123913\n",
      "train loss:0.7937471996008568\n",
      "train loss:0.8752839497206562\n",
      "train loss:0.6660918853363975\n",
      "train loss:0.9668050124655875\n",
      "train loss:0.7821785729231394\n",
      "train loss:1.0026013100617053\n",
      "train loss:0.8720269947548848\n",
      "train loss:1.0807881900795377\n",
      "train loss:0.7963507928834018\n",
      "train loss:0.8924804329797554\n",
      "train loss:0.8031814300459209\n",
      "train loss:0.9902620654998867\n",
      "train loss:0.7690194262405111\n",
      "train loss:0.8430268721469839\n",
      "train loss:1.0403303033664821\n",
      "train loss:1.1564662029197783\n",
      "train loss:0.9887990835152394\n",
      "train loss:1.014797420142555\n",
      "train loss:0.8527554708474718\n",
      "train loss:1.0431185361247604\n",
      "train loss:0.742950088452259\n",
      "train loss:0.8895817101758232\n",
      "train loss:0.9192707803183012\n",
      "train loss:0.8553688005916689\n",
      "train loss:0.9112566604190877\n",
      "train loss:0.9295277209408471\n",
      "train loss:0.8533925382668427\n",
      "train loss:0.7955517128894314\n",
      "train loss:1.021252096470152\n",
      "train loss:0.982287464286475\n",
      "train loss:0.9745890754204009\n",
      "train loss:0.8887661642895978\n",
      "train loss:1.0195387369699211\n",
      "train loss:0.8587338574655509\n",
      "train loss:1.0712840484188006\n",
      "train loss:0.8714773809055956\n",
      "train loss:0.8484691957751653\n",
      "train loss:0.8593520858617281\n",
      "train loss:0.7108550279711509\n",
      "train loss:0.9799127396319686\n",
      "train loss:0.9351934125092657\n",
      "train loss:0.9305965121838585\n",
      "train loss:0.9207448830026571\n",
      "train loss:0.8679776591254464\n",
      "train loss:0.9722401365256783\n",
      "train loss:0.8580486642485974\n",
      "train loss:0.8879629651223747\n",
      "train loss:0.8230321299371457\n",
      "train loss:0.9617323078305016\n",
      "train loss:0.8824467277513292\n",
      "train loss:0.8259112150057412\n",
      "train loss:0.7946452644943194\n",
      "train loss:0.9570650920118644\n",
      "train loss:0.7869913444010497\n",
      "train loss:0.9209669949113394\n",
      "train loss:0.9459057888812324\n",
      "train loss:0.9502725337911133\n",
      "train loss:0.9279831297728509\n",
      "train loss:0.9063458445387691\n",
      "train loss:0.9874570346009698\n",
      "train loss:0.8214261808247311\n",
      "train loss:0.8077424438564869\n",
      "train loss:0.8084531397630849\n",
      "train loss:0.8988804441769644\n",
      "train loss:1.0085236977976486\n",
      "train loss:0.907243292861594\n",
      "train loss:0.9631758351196451\n",
      "train loss:0.8796902490249349\n",
      "train loss:1.0149847935610259\n",
      "train loss:0.9085827646679213\n",
      "train loss:1.0485754757758623\n",
      "train loss:0.8204945526589923\n",
      "train loss:0.921295536975751\n",
      "train loss:0.9012322370208895\n",
      "train loss:0.9266681099837267\n",
      "train loss:0.8375628972731815\n",
      "train loss:1.1292573554289553\n",
      "train loss:1.0265231693126067\n",
      "train loss:0.813006651659136\n",
      "train loss:0.823656859847824\n",
      "train loss:0.9034450982422731\n",
      "train loss:0.7637795269109223\n",
      "train loss:0.9355490133738882\n",
      "train loss:0.6994621577332353\n",
      "train loss:0.7578697071882279\n",
      "train loss:0.9839355200633989\n",
      "train loss:0.8649891874960862\n",
      "train loss:0.9808085094899563\n",
      "train loss:0.844703994027963\n",
      "train loss:0.7436695748672633\n",
      "train loss:0.8364495609155155\n",
      "train loss:0.8471517498855744\n",
      "train loss:0.9445944143830962\n",
      "train loss:0.7660324473252855\n",
      "train loss:0.9497586898283149\n",
      "train loss:0.8373699941657633\n",
      "train loss:0.9799209580522352\n",
      "train loss:0.8148682534003169\n",
      "train loss:0.8848599278833406\n",
      "train loss:0.8866946812221806\n",
      "train loss:1.0921401003021667\n",
      "train loss:0.948059710058731\n",
      "train loss:1.0905100489747077\n",
      "train loss:0.7068625424020013\n",
      "train loss:0.8418903602853232\n",
      "train loss:0.9348441433198367\n",
      "train loss:1.0781961327523542\n",
      "train loss:0.9634895324928485\n",
      "train loss:0.8480036867702266\n",
      "train loss:0.8329397004290956\n",
      "train loss:0.7777311060251831\n",
      "train loss:0.8121986069229026\n",
      "train loss:0.9009953780947408\n",
      "train loss:1.0198087436841725\n",
      "train loss:0.8708032983300424\n",
      "train loss:0.820462777964783\n",
      "train loss:0.9079129608749058\n",
      "train loss:1.0241154299125903\n",
      "train loss:1.0196380388489181\n",
      "train loss:0.9073979839090359\n",
      "train loss:0.9103040681941337\n",
      "train loss:0.9447418564118019\n",
      "train loss:0.9038736333451505\n",
      "train loss:0.9234411098410306\n",
      "train loss:0.7038311448149838\n",
      "train loss:0.9925957711191387\n",
      "train loss:0.9267205839385767\n",
      "train loss:0.9130130403227167\n",
      "train loss:0.8450247560745665\n",
      "train loss:0.9378155315670547\n",
      "train loss:0.8680268698829349\n",
      "train loss:0.9682453697220095\n",
      "train loss:0.9054675540570565\n",
      "train loss:0.810030089687768\n",
      "train loss:0.9160276195082513\n",
      "train loss:0.8956306636760272\n",
      "train loss:0.9819288672728894\n",
      "train loss:0.9341656749046946\n",
      "train loss:0.8763007144206931\n",
      "train loss:0.8881541592759723\n",
      "train loss:1.03863072949458\n",
      "train loss:0.8683674032202877\n",
      "train loss:0.9547169345489082\n",
      "train loss:0.7564964056366805\n",
      "train loss:0.9372879820640673\n",
      "train loss:1.0281573082239654\n",
      "train loss:0.87100762395211\n",
      "train loss:0.7429530753362844\n",
      "train loss:0.7240559003180225\n",
      "train loss:0.9640858401859647\n",
      "train loss:0.8650006208513227\n",
      "train loss:0.8093810669965095\n",
      "train loss:0.8950406903926775\n",
      "train loss:0.8555156034978241\n",
      "train loss:0.7839673506584361\n",
      "train loss:0.9310060855637665\n",
      "train loss:1.0152990126945987\n",
      "train loss:0.946841427003215\n",
      "train loss:0.9139025866543414\n",
      "train loss:1.0305143807680919\n",
      "train loss:0.702481699546474\n",
      "train loss:0.8533986750749862\n",
      "train loss:0.9604248780345661\n",
      "train loss:0.915266776540897\n",
      "train loss:1.0743939505534863\n",
      "train loss:0.838221075228586\n",
      "train loss:0.9232412229699495\n",
      "train loss:0.9893821851324934\n",
      "train loss:0.8291280973859366\n",
      "train loss:0.79308871249158\n",
      "train loss:0.722458731562837\n",
      "train loss:1.0133245009516718\n",
      "train loss:1.037405982722411\n",
      "train loss:0.8679983621065531\n",
      "train loss:0.7881451997467883\n",
      "train loss:0.842710078964566\n",
      "train loss:0.856883702706609\n",
      "train loss:0.9759766682133207\n",
      "train loss:0.7680887423789471\n",
      "train loss:0.8614717331306703\n",
      "train loss:0.8972580817903055\n",
      "train loss:1.0013984481277733\n",
      "train loss:0.9290791202385\n",
      "train loss:1.0162736814405782\n",
      "train loss:0.7465031737748282\n",
      "train loss:0.9409276194439248\n",
      "train loss:0.7653674545306847\n",
      "train loss:0.9344584201022554\n",
      "train loss:0.8215848927813387\n",
      "train loss:0.9850787205205681\n",
      "train loss:0.9495239826771307\n",
      "train loss:0.9030086179871439\n",
      "train loss:0.7493996217776331\n",
      "train loss:0.9273081128929006\n",
      "train loss:0.7616160737338507\n",
      "train loss:0.7342289770925428\n",
      "train loss:0.9257864961684742\n",
      "train loss:0.8822274793178038\n",
      "train loss:0.9743501630446706\n",
      "train loss:0.8156532415943327\n",
      "train loss:0.8825459542338856\n",
      "train loss:1.0342453103066584\n",
      "train loss:0.7990372941900716\n",
      "train loss:0.7707662945078669\n",
      "train loss:0.7465179039655527\n",
      "train loss:0.7937144418543768\n",
      "train loss:0.9734978584686383\n",
      "train loss:1.0358958104687142\n",
      "train loss:0.7192331629533492\n",
      "train loss:0.9721699999101822\n",
      "train loss:1.0078527372758008\n",
      "train loss:0.9182983174131326\n",
      "train loss:0.8760251059352843\n",
      "train loss:0.884082737414413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.7790536557426351\n",
      "train loss:0.8333747940144235\n",
      "train loss:1.0759313408919018\n",
      "train loss:0.835607281545504\n",
      "train loss:0.7802329480162377\n",
      "train loss:0.9134693383045156\n",
      "train loss:0.7243239479407398\n",
      "train loss:0.7177022150678701\n",
      "train loss:0.7229214939898944\n",
      "train loss:0.8772653602364708\n",
      "train loss:1.0371632720981407\n",
      "train loss:0.6677606249567372\n",
      "train loss:0.8621655667783571\n",
      "train loss:0.9040970539176385\n",
      "train loss:0.9651732786246631\n",
      "train loss:0.8173927558191468\n",
      "train loss:0.7849568386128927\n",
      "train loss:0.7790002571866935\n",
      "train loss:1.1027035051239777\n",
      "train loss:0.7417503170567444\n",
      "train loss:0.898678236364361\n",
      "train loss:0.9648104046948721\n",
      "train loss:0.881941862135804\n",
      "train loss:0.7863033870831798\n",
      "train loss:0.8093135372646661\n",
      "train loss:1.0138359667043217\n",
      "train loss:0.9100335120893049\n",
      "train loss:0.7446380172656804\n",
      "train loss:0.7862646023849439\n",
      "train loss:0.655332089523559\n",
      "train loss:0.8032578745689221\n",
      "train loss:0.8268472656841892\n",
      "train loss:1.002831588276542\n",
      "train loss:0.9869082140668511\n",
      "train loss:0.8264357522314171\n",
      "train loss:0.8117908642431793\n",
      "train loss:0.7589478322335111\n",
      "train loss:0.9599430803033926\n",
      "train loss:0.8462326087015716\n",
      "train loss:0.9809787767803613\n",
      "train loss:1.14177487772391\n",
      "train loss:0.8881714762136503\n",
      "train loss:0.803070749954575\n",
      "train loss:1.0319060098410175\n",
      "train loss:0.8565644294828259\n",
      "train loss:0.912198273328951\n",
      "train loss:0.8910820302632996\n",
      "train loss:0.9486887668502281\n",
      "train loss:0.922099775705942\n",
      "train loss:0.9187379895927299\n",
      "train loss:0.8404090408920293\n",
      "train loss:0.7203091775419124\n",
      "train loss:0.9544992788749233\n",
      "train loss:0.8579645598550758\n",
      "train loss:0.7762887067062668\n",
      "train loss:0.8030717203930472\n",
      "train loss:0.9683831663221288\n",
      "train loss:0.9124090942871997\n",
      "train loss:0.8990430351546783\n",
      "train loss:0.8778931005387893\n",
      "train loss:0.9411537232075395\n",
      "train loss:0.937098617914615\n",
      "train loss:1.0092567983191858\n",
      "train loss:0.7857640872355417\n",
      "train loss:0.7938386473478445\n",
      "train loss:0.8012313268282812\n",
      "train loss:0.7304062672321355\n",
      "train loss:0.8851326390109482\n",
      "train loss:0.7749867106494907\n",
      "train loss:0.7639173884176376\n",
      "train loss:0.8215422126675149\n",
      "train loss:1.087285565697503\n",
      "train loss:0.9329314054718632\n",
      "train loss:0.8305010507953964\n",
      "train loss:1.0660004062426505\n",
      "train loss:0.9496225483785419\n",
      "train loss:0.8116426871424988\n",
      "train loss:0.9573637140024333\n",
      "train loss:0.9470838923363826\n",
      "train loss:0.935097761495426\n",
      "train loss:0.9066422220013169\n",
      "train loss:0.8042527852631614\n",
      "train loss:0.9183378649215331\n",
      "train loss:0.8941786349252178\n",
      "train loss:0.9026953043498713\n",
      "train loss:0.9008861028526307\n",
      "train loss:0.8478190920145653\n",
      "train loss:0.7519952093662806\n",
      "train loss:1.0347232498850827\n",
      "train loss:1.0445320649481562\n",
      "train loss:1.1313995773932655\n",
      "train loss:0.99262164200659\n",
      "train loss:0.8455640841146296\n",
      "train loss:0.8549002779668102\n",
      "train loss:0.7482157059329617\n",
      "train loss:1.1332652541883004\n",
      "train loss:0.8061399367686668\n",
      "train loss:0.7362155963306641\n",
      "train loss:0.7705681438305576\n",
      "train loss:1.0923437772706912\n",
      "train loss:0.8950560872169216\n",
      "train loss:0.9173507589142228\n",
      "train loss:0.9185502703927656\n",
      "train loss:0.8679267543965403\n",
      "train loss:0.9456841561796261\n",
      "train loss:0.8854926049477878\n",
      "train loss:0.9531517459849163\n",
      "train loss:0.9294432716872852\n",
      "train loss:0.8938048264535916\n",
      "train loss:0.8916325541468028\n",
      "train loss:0.8159765542910811\n",
      "train loss:0.7186505395679496\n",
      "train loss:0.8639183793605579\n",
      "train loss:0.9348079563596612\n",
      "train loss:0.8174387599831003\n",
      "train loss:0.9349306277031489\n",
      "train loss:0.8831188126805651\n",
      "train loss:0.8707752762474518\n",
      "train loss:1.0737201049801453\n",
      "train loss:0.9107648605369287\n",
      "train loss:0.8892012149434554\n",
      "train loss:0.8411777147188298\n",
      "train loss:0.817043302725559\n",
      "train loss:0.9078900609383709\n",
      "train loss:0.8098985288819839\n",
      "train loss:0.9179693734902484\n",
      "train loss:0.7553957516012537\n",
      "train loss:0.7233290936950622\n",
      "train loss:0.8235934205326079\n",
      "train loss:0.8743333610965829\n",
      "train loss:0.9752987944581492\n",
      "train loss:0.7288264280373511\n",
      "train loss:0.8717762895270751\n",
      "train loss:0.989797850197861\n",
      "train loss:0.9128285063243022\n",
      "train loss:0.8979631180900816\n",
      "train loss:0.8294216528222671\n",
      "train loss:0.9092566386451254\n",
      "train loss:0.9141781144184691\n",
      "train loss:0.656758962246068\n",
      "train loss:1.014821482019883\n",
      "train loss:0.8109751642247626\n",
      "train loss:0.9285093848340683\n",
      "train loss:0.8785654738684384\n",
      "train loss:0.9153887142673934\n",
      "train loss:1.0605336287440397\n",
      "train loss:0.760017941082114\n",
      "train loss:1.0042402157004446\n",
      "train loss:0.9630496242653311\n",
      "train loss:1.1253036501867197\n",
      "train loss:1.032203066523936\n",
      "train loss:0.8669904831420596\n",
      "train loss:0.8709063865644139\n",
      "train loss:0.8441569393300433\n",
      "train loss:0.8471865927279792\n",
      "train loss:0.9610673347030209\n",
      "train loss:0.8544290043179038\n",
      "train loss:0.8636390287730229\n",
      "train loss:0.735028758216044\n",
      "train loss:0.8049314742346704\n",
      "train loss:0.8911566010835074\n",
      "train loss:0.9553011157801304\n",
      "train loss:0.8246975188614687\n",
      "train loss:0.7331418933409245\n",
      "train loss:0.8810740752796243\n",
      "train loss:0.8984648485959003\n",
      "train loss:0.7619196228914913\n",
      "train loss:0.8731614652197122\n",
      "train loss:0.969272540772252\n",
      "train loss:0.936577160657903\n",
      "train loss:1.0484075565891193\n",
      "train loss:0.870506546722924\n",
      "train loss:0.723122329249184\n",
      "train loss:0.8624206488797935\n",
      "train loss:0.9202380393523788\n",
      "train loss:0.8672527234758273\n",
      "train loss:0.7598849916092982\n",
      "train loss:0.9378802750245225\n",
      "train loss:0.7829048748385619\n",
      "train loss:1.2396126409698924\n",
      "train loss:0.7655877123445015\n",
      "train loss:1.0068291228172435\n",
      "train loss:0.883364034959314\n",
      "train loss:0.7800591535174722\n",
      "train loss:0.8292172896921629\n",
      "train loss:0.7634594831143267\n",
      "train loss:0.774620501531534\n",
      "train loss:1.0290219555123363\n",
      "train loss:0.8386341945538777\n",
      "train loss:0.7714413678292587\n",
      "train loss:1.0474892913502751\n",
      "train loss:0.8417703087271788\n",
      "train loss:0.868985074754364\n",
      "train loss:0.8634159897125799\n",
      "train loss:0.8771834529691287\n",
      "train loss:0.9508571524618737\n",
      "train loss:0.89200652707306\n",
      "train loss:0.829619782303798\n",
      "train loss:0.7554931681271758\n",
      "train loss:0.8853641214786875\n",
      "train loss:1.1264914233802528\n",
      "train loss:0.8815447712959233\n",
      "train loss:0.8182259742770356\n",
      "train loss:1.159869285165736\n",
      "train loss:0.8435167903166061\n",
      "train loss:0.8320856082047331\n",
      "train loss:1.040731044473139\n",
      "train loss:0.9970155753594079\n",
      "train loss:0.7606831920891917\n",
      "train loss:0.9089093527556171\n",
      "train loss:0.9929376801496836\n",
      "train loss:0.8705334757960083\n",
      "train loss:0.7901029006885305\n",
      "train loss:0.9095852043510352\n",
      "train loss:0.784512414416439\n",
      "train loss:0.7405553769826373\n",
      "train loss:0.8847309401174321\n",
      "train loss:0.8420704032528245\n",
      "train loss:0.9658216134601108\n",
      "train loss:1.0069447312313184\n",
      "train loss:0.9719893944371166\n",
      "train loss:0.9234404740502999\n",
      "train loss:0.8369560896894896\n",
      "train loss:0.9874203422743689\n",
      "train loss:0.8555678231173484\n",
      "train loss:0.8708463417340871\n",
      "train loss:0.8127081977282158\n",
      "train loss:0.8806258914698886\n",
      "train loss:0.8056457410100354\n",
      "train loss:1.017428840151242\n",
      "train loss:0.8421819635428247\n",
      "train loss:0.9455153742012291\n",
      "train loss:0.9361920901520829\n",
      "train loss:0.8812758834410181\n",
      "train loss:0.8627454124677583\n",
      "train loss:0.7770943484501036\n",
      "train loss:0.9801181823920035\n",
      "train loss:0.8581917687291768\n",
      "train loss:0.9766058265280393\n",
      "train loss:0.8854507702964631\n",
      "train loss:0.7300890800637706\n",
      "train loss:0.8740762301128285\n",
      "train loss:0.9783708183915742\n",
      "train loss:0.8396597068636682\n",
      "train loss:0.9644491637994467\n",
      "train loss:0.8902509828761938\n",
      "train loss:0.916179336034328\n",
      "train loss:0.8649823645158774\n",
      "train loss:0.8301428488161815\n",
      "train loss:0.8621291677257494\n",
      "train loss:0.7777629555187531\n",
      "train loss:0.9238376912021442\n",
      "train loss:0.7751424285976275\n",
      "train loss:0.9769411222861116\n",
      "train loss:0.810379053766975\n",
      "train loss:0.7524388416130579\n",
      "train loss:0.980448931320364\n",
      "train loss:0.9617555463822833\n",
      "train loss:0.7366119691226615\n",
      "train loss:0.9011939737731904\n",
      "train loss:0.8974761940230092\n",
      "train loss:0.7976115929698742\n",
      "train loss:1.0506821083864644\n",
      "train loss:0.8963849826259329\n",
      "train loss:0.830655667176385\n",
      "train loss:0.6966075484416581\n",
      "train loss:0.9661928244915882\n",
      "train loss:0.915555149964189\n",
      "train loss:0.8519613420163584\n",
      "train loss:0.6890017998744947\n",
      "train loss:0.8028926847325297\n",
      "train loss:0.933507869587038\n",
      "train loss:0.7362261486753033\n",
      "train loss:0.8990191798047426\n",
      "train loss:0.9176271810534522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8691457139287533\n",
      "train loss:0.8797896789298183\n",
      "train loss:1.0958987344013273\n",
      "train loss:0.8201036863331476\n",
      "train loss:0.9999654248459973\n",
      "train loss:0.9655820710959081\n",
      "train loss:0.8099640925525807\n",
      "train loss:0.9523332243270078\n",
      "train loss:0.9950500212518805\n",
      "train loss:0.9530068166036101\n",
      "train loss:0.9195551235498337\n",
      "train loss:0.8455309975322455\n",
      "train loss:0.784864919333719\n",
      "train loss:0.8478258230382142\n",
      "train loss:0.8796837263917139\n",
      "train loss:0.8420805080219086\n",
      "train loss:0.861505218148893\n",
      "train loss:0.8744938613654462\n",
      "train loss:0.9586216274573479\n",
      "train loss:0.9381292241371837\n",
      "train loss:0.7074657686484558\n",
      "train loss:1.0979562183088984\n",
      "train loss:0.852643383482948\n",
      "train loss:0.9841591676552387\n",
      "train loss:0.9468427399989281\n",
      "train loss:0.882064108332387\n",
      "train loss:0.8833326852605637\n",
      "train loss:0.8919863537844407\n",
      "train loss:0.8640582846752959\n",
      "train loss:0.8639585230857126\n",
      "train loss:0.7265108948866493\n",
      "train loss:0.9077419162531233\n",
      "train loss:1.0381693121623004\n",
      "train loss:1.1163805954476265\n",
      "train loss:0.9191415984373528\n",
      "train loss:1.0153362794259966\n",
      "train loss:0.9372619044476616\n",
      "train loss:0.8948249087246016\n",
      "train loss:0.7589916896808485\n",
      "train loss:0.9892106245830173\n",
      "train loss:0.6730831105231384\n",
      "train loss:0.8634016061698407\n",
      "train loss:0.9578837395814461\n",
      "train loss:0.7788067946425135\n",
      "train loss:0.8216428723740671\n",
      "train loss:0.8234301531558059\n",
      "train loss:0.8184877669224813\n",
      "train loss:1.0537473714075698\n",
      "train loss:0.7797892483024718\n",
      "train loss:0.8240983262233779\n",
      "train loss:0.9478220335178541\n",
      "train loss:0.885745740327727\n",
      "train loss:0.8445936996628347\n",
      "train loss:0.9721397019614264\n",
      "train loss:0.8138805829248676\n",
      "train loss:0.7559739292890124\n",
      "train loss:1.0814900030313437\n",
      "train loss:0.805648374286026\n",
      "train loss:0.8941812867385615\n",
      "train loss:1.080561583049854\n",
      "train loss:0.9557504952487668\n",
      "train loss:0.95794570039806\n",
      "train loss:0.982187714395874\n",
      "train loss:0.7611840241249795\n",
      "train loss:1.005067993924819\n",
      "train loss:0.7669867860366198\n",
      "train loss:0.9109212172156336\n",
      "train loss:1.0014010838797733\n",
      "train loss:1.027185377841564\n",
      "train loss:0.811276801186661\n",
      "train loss:0.8856504700122443\n",
      "train loss:0.8587770958934048\n",
      "train loss:0.9066764022080684\n",
      "train loss:0.854765825133874\n",
      "train loss:0.8779220231456379\n",
      "train loss:0.8427107087277002\n",
      "train loss:0.9202744080848819\n",
      "train loss:0.7435842808775739\n",
      "train loss:0.9246533244866041\n",
      "train loss:0.9313308546128398\n",
      "train loss:0.9353929479419656\n",
      "train loss:0.7874409433128013\n",
      "train loss:0.9864726056571095\n",
      "train loss:0.7992276701516183\n",
      "train loss:0.9392147744393776\n",
      "train loss:0.7342631152153585\n",
      "train loss:0.8914585411268674\n",
      "train loss:0.9717473747555426\n",
      "train loss:0.8982542671685847\n",
      "train loss:0.920994323299593\n",
      "train loss:0.8349844714927943\n",
      "train loss:0.9222824309104857\n",
      "train loss:0.8631394507657489\n",
      "train loss:0.8177865561264186\n",
      "train loss:0.8682185215779064\n",
      "train loss:1.07113833430078\n",
      "train loss:0.8942027123119207\n",
      "train loss:0.8490667203608442\n",
      "train loss:0.9320573741358257\n",
      "train loss:0.975449544086699\n",
      "train loss:1.0334970500975726\n",
      "train loss:0.880271126097321\n",
      "train loss:0.8629528273309006\n",
      "train loss:0.9161859143018053\n",
      "train loss:0.7432206849014019\n",
      "=== epoch:3, train acc:0.994, test acc:0.988 ===\n",
      "train loss:0.7852706812161132\n",
      "train loss:1.0452153453132111\n",
      "train loss:0.827321374599197\n",
      "train loss:1.0594700205171177\n",
      "train loss:0.8816813614194519\n",
      "train loss:0.7432618642441448\n",
      "train loss:0.7833893895969439\n",
      "train loss:0.9982847871717113\n",
      "train loss:0.9344776589420981\n",
      "train loss:0.9759025855143303\n",
      "train loss:0.7520915108737402\n",
      "train loss:0.8574556787026394\n",
      "train loss:1.0277594222983333\n",
      "train loss:1.0543862259426906\n",
      "train loss:0.8454058633796262\n",
      "train loss:0.8489613213957335\n",
      "train loss:0.9111109725242822\n",
      "train loss:0.9012232520033934\n",
      "train loss:0.9399101114142071\n",
      "train loss:0.8669374831710818\n",
      "train loss:1.0253063279970556\n",
      "train loss:0.9800356339239924\n",
      "train loss:0.7607282721608151\n",
      "train loss:0.6827170500614343\n",
      "train loss:0.7685311244002636\n",
      "train loss:1.0052049044945135\n",
      "train loss:1.0049669894796445\n",
      "train loss:0.8375010138470489\n",
      "train loss:0.7433860647960338\n",
      "train loss:0.9927513978761722\n",
      "train loss:0.9156685092419602\n",
      "train loss:0.9576233598969562\n",
      "train loss:1.0548295112277914\n",
      "train loss:0.8766079583042554\n",
      "train loss:0.808033199728737\n",
      "train loss:0.940830233751671\n",
      "train loss:0.850421307339432\n",
      "train loss:0.9863349076493573\n",
      "train loss:1.1738018395978236\n",
      "train loss:1.0843067029696782\n",
      "train loss:0.8972014379828538\n",
      "train loss:0.9307804260722204\n",
      "train loss:1.0574027998116582\n",
      "train loss:1.0844380342334108\n",
      "train loss:0.742199415861134\n",
      "train loss:0.8991954954185697\n",
      "train loss:0.6576652847537287\n",
      "train loss:0.9514518785707281\n",
      "train loss:1.010527724460717\n",
      "train loss:0.7587870774418483\n",
      "train loss:0.9404346110877979\n",
      "train loss:1.1025740146237906\n",
      "train loss:0.9746939105337514\n",
      "train loss:0.961059318121\n",
      "train loss:0.8380004807006867\n",
      "train loss:0.8439720542234258\n",
      "train loss:1.0136558283292678\n",
      "train loss:1.0143633964505134\n",
      "train loss:0.7589203545161604\n",
      "train loss:0.8484716759388806\n",
      "train loss:0.7811347435112417\n",
      "train loss:0.96535484012042\n",
      "train loss:0.8095635903458605\n",
      "train loss:0.8793521416989375\n",
      "train loss:0.9016098495451059\n",
      "train loss:0.8385316879981518\n",
      "train loss:0.9347432780147901\n",
      "train loss:0.9239691761292137\n",
      "train loss:0.8619938072274609\n",
      "train loss:1.0206709966098564\n",
      "train loss:0.9470181738477098\n",
      "train loss:0.8537842421669005\n",
      "train loss:0.9729621814189278\n",
      "train loss:0.8030991494437941\n",
      "train loss:0.9787599717231515\n",
      "train loss:0.8623141920402021\n",
      "train loss:0.8281877658955723\n",
      "train loss:0.866628417958282\n",
      "train loss:0.8332458929991098\n",
      "train loss:0.9353034723167519\n",
      "train loss:0.7296916995047951\n",
      "train loss:0.9495637356887399\n",
      "train loss:0.7930089789687352\n",
      "train loss:0.9282439462395985\n",
      "train loss:0.8981875246040518\n",
      "train loss:0.8784905923915234\n",
      "train loss:0.8444840716953411\n",
      "train loss:0.9067204390902875\n",
      "train loss:0.8086928531820344\n",
      "train loss:0.883073944735326\n",
      "train loss:0.8073795345934031\n",
      "train loss:0.8691914604147706\n",
      "train loss:0.9158569382804983\n",
      "train loss:0.923562481959122\n",
      "train loss:0.7977616936846234\n",
      "train loss:1.0507764352078326\n",
      "train loss:0.8507345756943667\n",
      "train loss:0.9871627317190361\n",
      "train loss:0.7849580115327011\n",
      "train loss:0.7549726624178154\n",
      "train loss:0.6863761514525748\n",
      "train loss:0.8250187944922112\n",
      "train loss:0.9311396195034304\n",
      "train loss:0.7836824281914303\n",
      "train loss:0.8841008970815597\n",
      "train loss:0.8982106933029993\n",
      "train loss:0.9428223165141829\n",
      "train loss:0.6950155239828213\n",
      "train loss:0.8155300992852464\n",
      "train loss:0.8477464783433217\n",
      "train loss:0.8415458941700998\n",
      "train loss:1.1393665284321752\n",
      "train loss:0.9493394047173499\n",
      "train loss:0.9428917506472662\n",
      "train loss:0.8984106154757334\n",
      "train loss:0.8162242693824063\n",
      "train loss:0.8525639516756861\n",
      "train loss:0.8625176400702224\n",
      "train loss:0.8849301878364544\n",
      "train loss:1.0428416569098378\n",
      "train loss:0.9283430648756535\n",
      "train loss:0.859014407470897\n",
      "train loss:0.995439456290301\n",
      "train loss:0.7967166867748198\n",
      "train loss:0.7453709321157035\n",
      "train loss:0.9850837633835907\n",
      "train loss:0.8485682392012923\n",
      "train loss:0.9275634851019778\n",
      "train loss:0.9459327993696067\n",
      "train loss:0.8879024340551893\n",
      "train loss:0.9649325601710779\n",
      "train loss:0.8603549138550263\n",
      "train loss:0.8492474713474273\n",
      "train loss:0.9537541228324221\n",
      "train loss:0.8674311139462774\n",
      "train loss:0.8140730136172291\n",
      "train loss:1.0827160658766222\n",
      "train loss:0.9141484703435172\n",
      "train loss:0.9762471038554003\n",
      "train loss:0.8656341940191763\n",
      "train loss:1.0002957794965233\n",
      "train loss:0.8207992985197049\n",
      "train loss:0.9381977244202813\n",
      "train loss:0.9759777463733861\n",
      "train loss:0.9106702464908245\n",
      "train loss:1.0325777537042198\n",
      "train loss:0.9791540242696081\n",
      "train loss:0.9203073014109837\n",
      "train loss:0.8118737293505024\n",
      "train loss:0.9160261833293147\n",
      "train loss:0.8123038336813494\n",
      "train loss:0.9092374843815694\n",
      "train loss:0.7661463399267315\n",
      "train loss:0.8348276906094383\n",
      "train loss:1.0104145617083844\n",
      "train loss:1.0262872835696526\n",
      "train loss:1.0013604201823822\n",
      "train loss:0.8685256566089257\n",
      "train loss:1.068872333313451\n",
      "train loss:1.0352341788169561\n",
      "train loss:0.9338835468767739\n",
      "train loss:0.9497140468696944\n",
      "train loss:0.7919579560143927\n",
      "train loss:0.8370187850062714\n",
      "train loss:0.7507057388829596\n",
      "train loss:0.9248525100772738\n",
      "train loss:0.8066659555186318\n",
      "train loss:0.845850608079539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8958635224525793\n",
      "train loss:0.9301027194116449\n",
      "train loss:0.738841636654808\n",
      "train loss:0.8714529774512273\n",
      "train loss:0.9626050106848156\n",
      "train loss:0.9700522709074022\n",
      "train loss:0.8588062490806283\n",
      "train loss:0.9081829979865899\n",
      "train loss:0.8891487028315364\n",
      "train loss:0.9117389940218915\n",
      "train loss:0.8426408308807107\n",
      "train loss:0.724166987497399\n",
      "train loss:0.9441813339942868\n",
      "train loss:0.8826832989568574\n",
      "train loss:0.8838106115338392\n",
      "train loss:0.9475093247437231\n",
      "train loss:1.0222333533957804\n",
      "train loss:0.8169608674735548\n",
      "train loss:0.8697433955336165\n",
      "train loss:0.8047540106075957\n",
      "train loss:0.8744514835635012\n",
      "train loss:0.74053678306839\n",
      "train loss:0.8990759239694571\n",
      "train loss:0.9196865650095697\n",
      "train loss:0.9744990736840887\n",
      "train loss:0.7906819285132327\n",
      "train loss:1.003932703507453\n",
      "train loss:0.8718495037270035\n",
      "train loss:0.7072437365237398\n",
      "train loss:0.9453173492269226\n",
      "train loss:0.8665918233026609\n",
      "train loss:0.7603175332622976\n",
      "train loss:0.6795220151959925\n",
      "train loss:0.9728427019312295\n",
      "train loss:0.916557001133535\n",
      "train loss:0.7825603249425701\n",
      "train loss:1.0432452185317598\n",
      "train loss:0.9350433046139648\n",
      "train loss:0.8463351383352085\n",
      "train loss:0.6306225677841583\n",
      "train loss:0.8954371387216834\n",
      "train loss:0.9720120902460495\n",
      "train loss:0.9047171020139079\n",
      "train loss:0.9841282175123094\n",
      "train loss:0.8096979066424785\n",
      "train loss:0.9530067151745218\n",
      "train loss:0.8788610265497568\n",
      "train loss:0.8982347373260651\n",
      "train loss:0.9130415726180762\n",
      "train loss:1.0567316669321807\n",
      "train loss:0.9407858189633187\n",
      "train loss:0.8547173670985505\n",
      "train loss:1.0405332222199595\n",
      "train loss:0.8200724187011771\n",
      "train loss:0.9427956305091036\n",
      "train loss:0.8755397398835031\n",
      "train loss:0.8992594738071253\n",
      "train loss:1.0005595231380406\n",
      "train loss:0.8523118566855241\n",
      "train loss:0.8929368747488945\n",
      "train loss:0.9105404440072112\n",
      "train loss:0.882612207498384\n",
      "train loss:0.8392148908784062\n",
      "train loss:0.9106362652541226\n",
      "train loss:0.8606146501278491\n",
      "train loss:0.785184682713094\n",
      "train loss:0.980472115619211\n",
      "train loss:0.7411296184012195\n",
      "train loss:0.9647890664700753\n",
      "train loss:0.7992633740848448\n",
      "train loss:0.9229284820808875\n",
      "train loss:0.9613695140392997\n",
      "train loss:0.8342003289537119\n",
      "train loss:0.814753364033041\n",
      "train loss:1.076116817736962\n",
      "train loss:0.9055251670272088\n",
      "train loss:0.892946948555388\n",
      "train loss:1.0392830660146999\n",
      "train loss:0.9177257511795623\n",
      "train loss:0.9281308282419678\n",
      "train loss:0.7993165437108263\n",
      "train loss:1.008577379523128\n",
      "train loss:0.8479887145727332\n",
      "train loss:0.8651917449814194\n",
      "train loss:0.8065093628510265\n",
      "train loss:0.8946599872457215\n",
      "train loss:0.9640176965999879\n",
      "train loss:0.9665571097919067\n",
      "train loss:0.8883369092092191\n",
      "train loss:0.8756687134859443\n",
      "train loss:0.9464740805032558\n",
      "train loss:0.9237338492784818\n",
      "train loss:0.8706456263575945\n",
      "train loss:1.115122898443409\n",
      "train loss:0.8520358685974828\n",
      "train loss:0.9277059347367426\n",
      "train loss:1.0822128027616074\n",
      "train loss:0.9383485773574968\n",
      "train loss:1.016506662868904\n",
      "train loss:0.9152139334161717\n",
      "train loss:1.0325831514494896\n",
      "train loss:0.9211174418822213\n",
      "train loss:0.84599908462919\n",
      "train loss:0.8696905663545124\n",
      "train loss:0.8704640217099399\n",
      "train loss:0.8309457249952839\n",
      "train loss:0.9325530210762172\n",
      "train loss:0.8983825164308867\n",
      "train loss:0.9259012838723844\n",
      "train loss:0.8343300975996831\n",
      "train loss:0.9458463992388317\n",
      "train loss:0.8236386996689788\n",
      "train loss:0.8676706352651052\n",
      "train loss:0.9871561078941259\n",
      "train loss:0.9015468739731491\n",
      "train loss:0.7511847596135555\n",
      "train loss:0.8736207848912875\n",
      "train loss:0.9925433343312676\n",
      "train loss:1.0512472165206679\n",
      "train loss:0.9434511295144911\n",
      "train loss:0.7805538339644468\n",
      "train loss:0.7433405799900161\n",
      "train loss:0.9586254799392327\n",
      "train loss:0.9720373514574899\n",
      "train loss:0.8641406643360328\n",
      "train loss:0.8668281584738915\n",
      "train loss:0.8081138724757104\n",
      "train loss:0.7564766807385795\n",
      "train loss:0.9595187430498981\n",
      "train loss:0.999371749492697\n",
      "train loss:0.8441446890395556\n",
      "train loss:0.907866711045401\n",
      "train loss:0.8470757789014982\n",
      "train loss:1.0659519694494008\n",
      "train loss:0.9467938599504145\n",
      "train loss:0.8250703133776099\n",
      "train loss:0.87005995918784\n",
      "train loss:0.9738515664752293\n",
      "train loss:0.9042307676327013\n",
      "train loss:0.8073465463362206\n",
      "train loss:0.8534499877601165\n",
      "train loss:0.7601811560638706\n",
      "train loss:0.787690705357993\n",
      "train loss:0.912557107060174\n",
      "train loss:0.8464385013103322\n",
      "train loss:0.8323614830928168\n",
      "train loss:0.8736181469893606\n",
      "train loss:0.9579566331541565\n",
      "train loss:1.1442486600673942\n",
      "train loss:0.6870154060214084\n",
      "train loss:1.0392798357071553\n",
      "train loss:0.8515624348048796\n",
      "train loss:0.8077556019891307\n",
      "train loss:0.7961371542956229\n",
      "train loss:0.9092235393504705\n",
      "train loss:1.0258608149379242\n",
      "train loss:0.9564432876390854\n",
      "train loss:0.8360491635157525\n",
      "train loss:0.895445731094198\n",
      "train loss:1.0460424868175495\n",
      "train loss:0.8413416850891644\n",
      "train loss:0.7688712924854333\n",
      "train loss:0.9312117804057342\n",
      "train loss:0.8750925825815103\n",
      "train loss:0.8989439666066807\n",
      "train loss:0.8287435411242161\n",
      "train loss:0.8285718251296028\n",
      "train loss:0.7870624746395426\n",
      "train loss:0.9153794847525047\n",
      "train loss:0.9327416057408562\n",
      "train loss:0.8020060496012988\n",
      "train loss:0.9046051102039029\n",
      "train loss:0.7956547696330111\n",
      "train loss:0.8300734544835848\n",
      "train loss:1.0839566798772011\n",
      "train loss:0.9443508603963555\n",
      "train loss:0.9615474405019502\n",
      "train loss:0.797185320539466\n",
      "train loss:0.7506944022005032\n",
      "train loss:0.7432047129674024\n",
      "train loss:0.8115508314858547\n",
      "train loss:0.9797043046847651\n",
      "train loss:0.7978607115149333\n",
      "train loss:0.8533145125320124\n",
      "train loss:0.9151211473805811\n",
      "train loss:0.8386391668572974\n",
      "train loss:0.9530078778543103\n",
      "train loss:0.950705880974949\n",
      "train loss:1.0303822652536077\n",
      "train loss:0.8819605351685154\n",
      "train loss:0.954446375640474\n",
      "train loss:0.7960814465830848\n",
      "train loss:0.7679862014016254\n",
      "train loss:0.7654448946675164\n",
      "train loss:0.8532515660201796\n",
      "train loss:0.8187895019947758\n",
      "train loss:0.8247223812297745\n",
      "train loss:0.9907216975623926\n",
      "train loss:0.7483664840937917\n",
      "train loss:1.0161521911438103\n",
      "train loss:1.0766799633254154\n",
      "train loss:0.9727336497740106\n",
      "train loss:0.9787175062216867\n",
      "train loss:0.9173026655771064\n",
      "train loss:0.9258094861612239\n",
      "train loss:1.0220608031462588\n",
      "train loss:0.8555046048495365\n",
      "train loss:0.9451755423447333\n",
      "train loss:0.778331670996154\n",
      "train loss:0.8589565996906243\n",
      "train loss:0.8458805094581565\n",
      "train loss:0.7357736873208423\n",
      "train loss:1.017013719110066\n",
      "train loss:0.8999946783140577\n",
      "train loss:0.8358058917888009\n",
      "train loss:0.997636430077812\n",
      "train loss:0.7815727466131288\n",
      "train loss:0.8838552442141372\n",
      "train loss:0.9289720040847983\n",
      "train loss:0.9785617032177\n",
      "train loss:0.955399190352931\n",
      "train loss:0.9347038459709477\n",
      "train loss:0.9111521848512765\n",
      "train loss:0.9128260750340301\n",
      "train loss:0.8313031010652228\n",
      "train loss:0.7389232068682162\n",
      "train loss:0.8941898132380084\n",
      "train loss:0.9214835481718515\n",
      "train loss:0.8223346274522438\n",
      "train loss:0.8602183511111634\n",
      "train loss:1.0838907485635516\n",
      "train loss:0.8540228867054637\n",
      "train loss:0.794829887514092\n",
      "train loss:1.0040818933361417\n",
      "train loss:0.7987861861281661\n",
      "train loss:0.8673237541808524\n",
      "train loss:0.8355303630146321\n",
      "train loss:1.141705539504852\n",
      "train loss:0.8329833639270003\n",
      "train loss:0.8823889182004563\n",
      "train loss:0.7479499275155816\n",
      "train loss:0.9415622357250122\n",
      "train loss:0.8690849348780404\n",
      "train loss:0.901549079394012\n",
      "train loss:0.8804351916786649\n",
      "train loss:0.9878028729144726\n",
      "train loss:0.9666236920926033\n",
      "train loss:0.8635917766588941\n",
      "train loss:0.7990391320314686\n",
      "train loss:0.9138712100339638\n",
      "train loss:0.8871807814531762\n",
      "train loss:0.8438551856686779\n",
      "train loss:1.0139269014417414\n",
      "train loss:0.777864673971646\n",
      "train loss:0.9259510293390781\n",
      "train loss:0.7885956308345419\n",
      "train loss:1.0391930961304952\n",
      "train loss:1.002740347494765\n",
      "train loss:0.9263374114436825\n",
      "train loss:0.8934978508866689\n",
      "train loss:0.9150934138996996\n",
      "train loss:1.0374850404917044\n",
      "train loss:0.7857555581290362\n",
      "train loss:0.9669637065970886\n",
      "train loss:0.9393481769077588\n",
      "train loss:0.9613630943138135\n",
      "train loss:0.8844370706259604\n",
      "train loss:0.739047292644764\n",
      "train loss:0.8982992570092003\n",
      "train loss:0.91053999013266\n",
      "train loss:0.9196077600482524\n",
      "train loss:0.9427599175970645\n",
      "train loss:0.9450653140506887\n",
      "train loss:0.7657943706025401\n",
      "train loss:0.8387330281785644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9756025827502404\n",
      "train loss:0.8640069656719304\n",
      "train loss:0.9609874961824457\n",
      "train loss:0.7322164017968815\n",
      "train loss:0.9527260804387255\n",
      "train loss:0.7353724026123032\n",
      "train loss:0.9363945985510178\n",
      "train loss:0.8118583166460326\n",
      "train loss:0.9380763964056935\n",
      "train loss:0.8337350280593676\n",
      "train loss:0.849519282251394\n",
      "train loss:1.0750261514587154\n",
      "train loss:0.7842968676208036\n",
      "train loss:1.0294424572042054\n",
      "train loss:0.9009871551812192\n",
      "train loss:0.8242333434882105\n",
      "train loss:0.8036352305763479\n",
      "train loss:0.9407106477749614\n",
      "train loss:0.8045863437444284\n",
      "train loss:0.7377317520687651\n",
      "train loss:0.846270790483973\n",
      "train loss:0.9335520045827779\n",
      "train loss:0.8672463069265284\n",
      "train loss:0.9239369531254019\n",
      "train loss:0.9445786836331516\n",
      "train loss:0.8724845503680179\n",
      "train loss:0.8789346178302505\n",
      "train loss:0.9162336763208259\n",
      "train loss:0.9289739084419046\n",
      "train loss:0.8108370731684409\n",
      "train loss:0.8255697978026565\n",
      "train loss:1.0327237798920137\n",
      "train loss:0.9993864929898538\n",
      "train loss:0.8448193909597579\n",
      "train loss:1.0472371704829444\n",
      "train loss:1.014843440833305\n",
      "train loss:0.8948882818074982\n",
      "train loss:0.9101997826232465\n",
      "train loss:0.9839043030048314\n",
      "train loss:0.8159802058084034\n",
      "train loss:0.8205222239114334\n",
      "train loss:0.9601792498602802\n",
      "train loss:0.658437846397936\n",
      "train loss:0.9119849668854405\n",
      "train loss:1.0084892954797018\n",
      "train loss:0.9066285189338531\n",
      "train loss:0.7086294829750802\n",
      "train loss:0.8866508673327063\n",
      "train loss:1.0270731122864922\n",
      "train loss:0.9629607515009233\n",
      "train loss:0.9368142035800117\n",
      "train loss:1.0636179649252147\n",
      "train loss:0.8754111127160507\n",
      "train loss:1.1410788493101292\n",
      "train loss:0.7251012193975578\n",
      "train loss:0.8879111886828587\n",
      "train loss:0.7850579976423986\n",
      "train loss:0.8806578658651303\n",
      "train loss:0.7939857827184795\n",
      "train loss:0.9328801899461681\n",
      "train loss:0.9786425591349582\n",
      "train loss:0.8022131711077586\n",
      "train loss:0.8850624760854663\n",
      "train loss:0.9015402835230982\n",
      "train loss:0.7145077761096645\n",
      "train loss:0.9056806991036223\n",
      "train loss:0.837999151932785\n",
      "train loss:0.8903283320378733\n",
      "train loss:0.8277681566786887\n",
      "train loss:0.9221903373902264\n",
      "train loss:0.8725869688548684\n",
      "train loss:0.8540437496843153\n",
      "train loss:0.9044113586596022\n",
      "train loss:0.8954693720528022\n",
      "train loss:0.7792571378086569\n",
      "train loss:0.9810695023512213\n",
      "train loss:0.843154671545503\n",
      "train loss:0.8781310204501658\n",
      "train loss:0.9870298175539304\n",
      "train loss:0.9575588445806753\n",
      "train loss:1.0069310447264193\n",
      "train loss:0.8223024241316098\n",
      "train loss:0.9044217063504617\n",
      "train loss:0.7912116892729582\n",
      "train loss:0.8734509155201928\n",
      "train loss:0.9491056889546015\n",
      "train loss:0.8747056729371743\n",
      "train loss:0.8662621606519562\n",
      "train loss:0.7903902544032977\n",
      "train loss:0.8019776016392116\n",
      "train loss:0.7636852214004594\n",
      "train loss:0.8625890471540191\n",
      "train loss:0.7172885330569799\n",
      "train loss:0.7801753251862898\n",
      "train loss:0.9119823072000789\n",
      "train loss:0.9004622128579038\n",
      "train loss:0.8273491399241546\n",
      "train loss:0.8234471130977998\n",
      "train loss:0.9210737842227887\n",
      "train loss:0.8693605343148074\n",
      "train loss:0.8382586874360379\n",
      "train loss:0.7823430181693577\n",
      "train loss:0.7683899488390777\n",
      "train loss:0.7119308922838015\n",
      "train loss:0.7403017346523487\n",
      "train loss:0.8954555234626304\n",
      "train loss:0.741469719792468\n",
      "train loss:1.0030383638340348\n",
      "train loss:0.9985649602807469\n",
      "train loss:0.822745981772699\n",
      "train loss:0.8411410153405211\n",
      "train loss:0.795950980317636\n",
      "train loss:0.7839409452636318\n",
      "train loss:0.739284941979979\n",
      "train loss:0.9171465809756394\n",
      "train loss:0.8394793854114658\n",
      "train loss:0.8909413695431289\n",
      "train loss:0.8934783041419336\n",
      "train loss:1.014885282930133\n",
      "train loss:1.0479981173172745\n",
      "train loss:0.9741814583598172\n",
      "train loss:0.7488631587829541\n",
      "train loss:0.8197838215538183\n",
      "train loss:0.8218638705914977\n",
      "train loss:0.9364281220862477\n",
      "train loss:0.7436325300893707\n",
      "train loss:0.8440715466626451\n",
      "train loss:0.9645011487940569\n",
      "train loss:1.0225813069350946\n",
      "train loss:0.8755332524288413\n",
      "train loss:0.8020006748457404\n",
      "train loss:1.1094980209915686\n",
      "train loss:0.9364503118512729\n",
      "train loss:0.8049229727556176\n",
      "train loss:0.8843559604173459\n",
      "train loss:0.9775431006115987\n",
      "train loss:0.874014263954986\n",
      "train loss:0.9428785328617557\n",
      "train loss:0.8213130617730022\n",
      "train loss:0.9423452749378708\n",
      "train loss:0.9864775477048928\n",
      "train loss:1.0532030244657828\n",
      "train loss:0.9012466065678812\n",
      "train loss:0.9108778810162478\n",
      "train loss:0.9807539559069633\n",
      "train loss:0.8524352906548994\n",
      "train loss:0.8933014060398066\n",
      "train loss:0.8942489940349633\n",
      "train loss:0.8206193456788696\n",
      "train loss:0.9550722745472993\n",
      "train loss:0.9321207866355335\n",
      "train loss:0.9266474274742051\n",
      "train loss:1.129442936315661\n",
      "train loss:0.9135011635064084\n",
      "train loss:0.8979040375875232\n",
      "train loss:0.8808903111058984\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9928\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(network, noised_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# パラメータの保存\n",
    "network.save_params(\"deep_convnet_params_noise.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('graph_noise.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ノイズ込み学習後\n",
      "無加工に対する正答率： 0.9928\n",
      "ランダムノイズに対する正答率： 0.9899\n"
     ]
    }
   ],
   "source": [
    "print(\"ノイズ込み学習後\")\n",
    "print(\"無加工に対する正答率：\", network.accuracy(x_test, t_test))\n",
    "print(\"ランダムノイズに対する正答率：\", network.accuracy(noised_test, t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
