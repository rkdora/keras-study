{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from fgsm.deep_convnet import DeepConvNet\n",
    "%matplotlib inline\n",
    "\n",
    "from common.functions import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
    "\n",
    "# # 1次元へ整形\n",
    "# x_train, x_test = x_train.reshape(-1, 784), x_test.reshape(-1, 784)\n",
    "\n",
    "##  4次元へ整形\n",
    "x_train_shape = x_train.shape\n",
    "x_train = x_train.reshape(x_train_shape[0], 1, x_train_shape[1], x_train_shape[2])\n",
    "\n",
    "x_test_shape = x_test.shape\n",
    "x_test = x_test.reshape(x_test_shape[0], 1, x_test_shape[1], x_test_shape[2])\n",
    "\n",
    "\n",
    "# 正規化\n",
    "x_train, x_test = x_train.astype(np.float32) / 255.0, x_test.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "X_train = x_train.reshape(60000, 784)\n",
    "X_test  = x_test.reshape(10000, 784)\n",
    "\n",
    "y_train = keras.utils.to_categorical(t_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(t_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1126 13:37:03.297451 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1126 13:37:03.316697 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1126 13:37:03.335005 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1126 13:37:03.335930 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1126 13:37:03.346987 140736894542784 deprecation.py:506] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1126 13:37:03.492784 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1126 13:37:03.564960 140736894542784 deprecation_wrapper.py:119] From /Users/ryuto/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# モデルを読み込む\n",
    "model = model_from_json(open('../keras_sample/mnist_mlp_model.json').read())\n",
    "\n",
    "# 学習結果を読み込む\n",
    "model.load_weights('../keras_sample/mnist_mlp_weights.h5')\n",
    "\n",
    "model.summary();\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルBの正答率： 0.9842\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test loss :', score[0])\n",
    "print('モデルBの正答率：', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = DeepConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.320470326088453\n",
      "=== epoch:1, train acc:0.096, test acc:0.091 ===\n",
      "train loss:2.2819979088427007\n",
      "train loss:2.255390165101905\n",
      "train loss:2.3106572900275437\n",
      "train loss:2.2527619181452296\n",
      "train loss:2.281321633649598\n",
      "train loss:2.2768527733585007\n",
      "train loss:2.2889505932883814\n",
      "train loss:2.2654666500590563\n",
      "train loss:2.2806492951772186\n",
      "train loss:2.2679907618468755\n",
      "train loss:2.2255559989914975\n",
      "train loss:2.234859120326865\n",
      "train loss:2.2596639431253775\n",
      "train loss:2.2240487436720806\n",
      "train loss:2.211743116899381\n",
      "train loss:2.188780532247126\n",
      "train loss:2.1535424956519975\n",
      "train loss:2.162045995079754\n",
      "train loss:2.1305191718197007\n",
      "train loss:2.136933256624773\n",
      "train loss:2.044128757882742\n",
      "train loss:2.137101849676413\n",
      "train loss:2.026687739721172\n",
      "train loss:2.0877092246941023\n",
      "train loss:1.9427944562557582\n",
      "train loss:1.9335513798865405\n",
      "train loss:2.0866833467540076\n",
      "train loss:1.9291867577060264\n",
      "train loss:2.001007351378713\n",
      "train loss:2.111322714078462\n",
      "train loss:2.022653917524261\n",
      "train loss:2.063716770285421\n",
      "train loss:1.8456541862746867\n",
      "train loss:1.833820481987441\n",
      "train loss:1.8655366476197743\n",
      "train loss:1.909495918618709\n",
      "train loss:1.7946408558955413\n",
      "train loss:1.8762254179242817\n",
      "train loss:1.9825754462892713\n",
      "train loss:1.669806993683817\n",
      "train loss:1.8285084790090746\n",
      "train loss:1.8578041194879082\n",
      "train loss:1.703949899785421\n",
      "train loss:1.8725635402524665\n",
      "train loss:1.8788822348606826\n",
      "train loss:1.894924583247971\n",
      "train loss:1.7259766953688813\n",
      "train loss:1.7198328968245375\n",
      "train loss:1.7264436129254526\n",
      "train loss:1.7951541919508762\n",
      "train loss:1.70829335245586\n",
      "train loss:1.7821113740174537\n",
      "train loss:1.6920145203292436\n",
      "train loss:1.7414611917886957\n",
      "train loss:1.8030349277524869\n",
      "train loss:1.9262103541697784\n",
      "train loss:1.8113723316611745\n",
      "train loss:1.712550769107464\n",
      "train loss:1.7338465713533453\n",
      "train loss:1.659157797936971\n",
      "train loss:1.7436274385518096\n",
      "train loss:1.6362603466125438\n",
      "train loss:1.6477866361644333\n",
      "train loss:1.6564027998835436\n",
      "train loss:1.6653207290090524\n",
      "train loss:1.7522693632862936\n",
      "train loss:1.588123233291232\n",
      "train loss:1.5803692053828808\n",
      "train loss:1.6570526840108335\n",
      "train loss:1.8006706052589974\n",
      "train loss:1.4893166333942363\n",
      "train loss:1.6424345473204724\n",
      "train loss:1.7462978934070101\n",
      "train loss:1.3664511117795586\n",
      "train loss:1.590776386011997\n",
      "train loss:1.6281821258476554\n",
      "train loss:1.5028020723400213\n",
      "train loss:1.5495283309453014\n",
      "train loss:1.5822682436365334\n",
      "train loss:1.6357496084592784\n",
      "train loss:1.831447594923431\n",
      "train loss:1.5693323316096095\n",
      "train loss:1.4712054296262729\n",
      "train loss:1.6162331115123965\n",
      "train loss:1.6560138276412324\n",
      "train loss:1.754232030402689\n",
      "train loss:1.4522223700305696\n",
      "train loss:1.7437215606216363\n",
      "train loss:1.578761385086515\n",
      "train loss:1.585227960522077\n",
      "train loss:1.46000258299564\n",
      "train loss:1.5548343646727405\n",
      "train loss:1.710487421949746\n",
      "train loss:1.43072412144343\n",
      "train loss:1.536347711371667\n",
      "train loss:1.515671526474123\n",
      "train loss:1.557839113567594\n",
      "train loss:1.5272145991710937\n",
      "train loss:1.5024818221414966\n",
      "train loss:1.4428625578523029\n",
      "train loss:1.3982132296317473\n",
      "train loss:1.5769094147649612\n",
      "train loss:1.4349205025403586\n",
      "train loss:1.4269703187137823\n",
      "train loss:1.5757027414194724\n",
      "train loss:1.5367451999331794\n",
      "train loss:1.603676701541814\n",
      "train loss:1.4614980478499782\n",
      "train loss:1.6529085970858417\n",
      "train loss:1.4990404740219088\n",
      "train loss:1.6517209379654967\n",
      "train loss:1.4715263557227578\n",
      "train loss:1.5802128331733203\n",
      "train loss:1.5121179798986752\n",
      "train loss:1.4116483657605536\n",
      "train loss:1.6476913516755822\n",
      "train loss:1.5829374104484843\n",
      "train loss:1.472970558231223\n",
      "train loss:1.6335694136381878\n",
      "train loss:1.4980865028038977\n",
      "train loss:1.5009710606474727\n",
      "train loss:1.4048481817082918\n",
      "train loss:1.5404123479651508\n",
      "train loss:1.4671980732873953\n",
      "train loss:1.498015176910184\n",
      "train loss:1.3931695908168897\n",
      "train loss:1.7750921891374174\n",
      "train loss:1.5701273180105955\n",
      "train loss:1.6098468873751772\n",
      "train loss:1.4082538543152603\n",
      "train loss:1.5531504393054698\n",
      "train loss:1.3483157998376134\n",
      "train loss:1.4539673835794065\n",
      "train loss:1.5989068731145346\n",
      "train loss:1.4419318623563868\n",
      "train loss:1.57036411665856\n",
      "train loss:1.3922554172501815\n",
      "train loss:1.6421923780606438\n",
      "train loss:1.4215433150570789\n",
      "train loss:1.2786130885058673\n",
      "train loss:1.4642264567764542\n",
      "train loss:1.357237644151697\n",
      "train loss:1.342957022436516\n",
      "train loss:1.3876408273625271\n",
      "train loss:1.438732882927873\n",
      "train loss:1.2335221794618705\n",
      "train loss:1.4280256393697723\n",
      "train loss:1.2261503548670245\n",
      "train loss:1.568507359290254\n",
      "train loss:1.293237833066301\n",
      "train loss:1.506987402337703\n",
      "train loss:1.4318581695134498\n",
      "train loss:1.4673734819126278\n",
      "train loss:1.4318559159736508\n",
      "train loss:1.3710248411606276\n",
      "train loss:1.4469326848854667\n",
      "train loss:1.4002458477082866\n",
      "train loss:1.5010502812856126\n",
      "train loss:1.2571130755827151\n",
      "train loss:1.3135080257379392\n",
      "train loss:1.3380665042556683\n",
      "train loss:1.372755571085246\n",
      "train loss:1.5171805079476912\n",
      "train loss:1.4140335222159797\n",
      "train loss:1.5380015252006514\n",
      "train loss:1.458442664270524\n",
      "train loss:1.5609258998059385\n",
      "train loss:1.5355632611003551\n",
      "train loss:1.3654994449329283\n",
      "train loss:1.4310051886698354\n",
      "train loss:1.383284448645264\n",
      "train loss:1.317027517742866\n",
      "train loss:1.2987285361312424\n",
      "train loss:1.2688570203320118\n",
      "train loss:1.30162331148866\n",
      "train loss:1.3658215657084853\n",
      "train loss:1.2164367854961307\n",
      "train loss:1.3027445466422634\n",
      "train loss:1.3647180035471471\n",
      "train loss:1.308131239348957\n",
      "train loss:1.1417804061239285\n",
      "train loss:1.4010231885232631\n",
      "train loss:1.302079760913459\n",
      "train loss:1.4249589067560589\n",
      "train loss:1.2837425718312983\n",
      "train loss:1.247433320240011\n",
      "train loss:1.2819671841189753\n",
      "train loss:1.2146220117910895\n",
      "train loss:1.4172644680262863\n",
      "train loss:1.2613357431212358\n",
      "train loss:1.4091361547162975\n",
      "train loss:1.2528601221079678\n",
      "train loss:1.303683307379915\n",
      "train loss:1.2099059359485056\n",
      "train loss:1.3117876374567257\n",
      "train loss:1.2038673275717373\n",
      "train loss:1.127522751798248\n",
      "train loss:1.2797692750672731\n",
      "train loss:1.1408242673816504\n",
      "train loss:1.3742203655768217\n",
      "train loss:1.254473949537508\n",
      "train loss:1.277113097017175\n",
      "train loss:1.1185327116760384\n",
      "train loss:1.301467322603812\n",
      "train loss:1.166061695505241\n",
      "train loss:1.2619356406611832\n",
      "train loss:1.2389553097187156\n",
      "train loss:1.1864844086136348\n",
      "train loss:1.188299095709092\n",
      "train loss:1.1274821191516187\n",
      "train loss:1.114136098310174\n",
      "train loss:1.2415325180799357\n",
      "train loss:1.1789002171091136\n",
      "train loss:1.282904091227684\n",
      "train loss:1.327698354569622\n",
      "train loss:1.3280102868982642\n",
      "train loss:1.3631299192411857\n",
      "train loss:1.2009221953013378\n",
      "train loss:1.159867825178273\n",
      "train loss:1.146976797687204\n",
      "train loss:1.2686626447689873\n",
      "train loss:1.2347422573317\n",
      "train loss:1.0088348207647395\n",
      "train loss:1.2654485997756024\n",
      "train loss:1.2372826174598703\n",
      "train loss:1.2267486807838397\n",
      "train loss:1.3949077794227664\n",
      "train loss:1.265912997486771\n",
      "train loss:1.2733752487147791\n",
      "train loss:1.3972900209437287\n",
      "train loss:1.0450727053341295\n",
      "train loss:1.1664139547663448\n",
      "train loss:1.189370001858935\n",
      "train loss:1.1103617348975425\n",
      "train loss:1.1788152555889184\n",
      "train loss:1.1481697088265315\n",
      "train loss:1.2778031078315342\n",
      "train loss:1.1327594997531896\n",
      "train loss:1.1782993335521084\n",
      "train loss:1.3431302176241702\n",
      "train loss:1.2332063489408263\n",
      "train loss:1.3882624917314792\n",
      "train loss:1.338850088547727\n",
      "train loss:1.0360662117162192\n",
      "train loss:1.186388238871477\n",
      "train loss:1.202498082543882\n",
      "train loss:1.3455008499604202\n",
      "train loss:1.2957152212851983\n",
      "train loss:1.3305564446348717\n",
      "train loss:1.1988491991957353\n",
      "train loss:1.3507167612696416\n",
      "train loss:1.1147495011739843\n",
      "train loss:1.153254362748915\n",
      "train loss:1.239024624535673\n",
      "train loss:1.146345361902591\n",
      "train loss:1.1092396262442106\n",
      "train loss:1.216581859538894\n",
      "train loss:1.1711697704608712\n",
      "train loss:1.0044092948952024\n",
      "train loss:1.1897964111694699\n",
      "train loss:0.9174620689688662\n",
      "train loss:1.201648426712661\n",
      "train loss:1.2469781850774526\n",
      "train loss:1.3077050653181208\n",
      "train loss:1.2196528348049953\n",
      "train loss:1.197445631713333\n",
      "train loss:1.1869126586460492\n",
      "train loss:1.2972641371775038\n",
      "train loss:1.2478602143468875\n",
      "train loss:1.0139290945372785\n",
      "train loss:1.105021388634353\n",
      "train loss:0.8761665607852366\n",
      "train loss:1.249684793659416\n",
      "train loss:1.3413102171664864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9906449881387657\n",
      "train loss:0.9678127482896849\n",
      "train loss:1.175878645673404\n",
      "train loss:1.3146171879084492\n",
      "train loss:1.179348528754526\n",
      "train loss:1.1490441931878168\n",
      "train loss:1.2011006277053111\n",
      "train loss:1.2524028383489116\n",
      "train loss:1.0451757142130786\n",
      "train loss:1.3376623090451012\n",
      "train loss:1.2289433077544414\n",
      "train loss:1.139675968594685\n",
      "train loss:1.0309092701908509\n",
      "train loss:1.1997126118607349\n",
      "train loss:1.2396478777580056\n",
      "train loss:1.122958957140247\n",
      "train loss:1.17114410230904\n",
      "train loss:0.9443772638172143\n",
      "train loss:1.049659674892791\n",
      "train loss:1.1788276826989117\n",
      "train loss:1.1175220704993751\n",
      "train loss:1.1523119494274177\n",
      "train loss:1.3013322067884914\n",
      "train loss:1.1677476835724396\n",
      "train loss:1.0685461940501622\n",
      "train loss:1.0419292056426883\n",
      "train loss:1.0646085914276653\n",
      "train loss:1.101386449753477\n",
      "train loss:1.137747880041971\n",
      "train loss:0.8894974427103927\n",
      "train loss:1.0939826939932435\n",
      "train loss:1.1518849206050685\n",
      "train loss:1.0901908946814332\n",
      "train loss:1.079942914465646\n",
      "train loss:1.1919362946341412\n",
      "train loss:1.1791565050267272\n",
      "train loss:1.031132933978777\n",
      "train loss:1.2513882833753167\n",
      "train loss:1.1759653230546143\n",
      "train loss:1.113511411713915\n",
      "train loss:1.0129583411797736\n",
      "train loss:1.3379469727184499\n",
      "train loss:1.2517099132907865\n",
      "train loss:1.2502973737964738\n",
      "train loss:1.2852532080611263\n",
      "train loss:1.0240646224192835\n",
      "train loss:1.1635303036174311\n",
      "train loss:1.2141209674913107\n",
      "train loss:1.1464125057734056\n",
      "train loss:1.1105343744239684\n",
      "train loss:1.1590351458706678\n",
      "train loss:1.231397808556435\n",
      "train loss:1.0446350455662454\n",
      "train loss:1.2898353089267705\n",
      "train loss:1.2351835993015616\n",
      "train loss:1.024384380440754\n",
      "train loss:1.180230924841423\n",
      "train loss:1.0431605705242686\n",
      "train loss:1.090343586512409\n",
      "train loss:1.0525495825368183\n",
      "train loss:1.1252881725563406\n",
      "train loss:1.3110942728763235\n",
      "train loss:1.2900606214986394\n",
      "train loss:1.119770527999834\n",
      "train loss:1.164178376016685\n",
      "train loss:1.2013458873477878\n",
      "train loss:1.211640643001571\n",
      "train loss:1.2555055384612193\n",
      "train loss:0.9840224799563783\n",
      "train loss:1.2161611505179024\n",
      "train loss:1.062153280498282\n",
      "train loss:1.0681756037455898\n",
      "train loss:0.9992323359251849\n",
      "train loss:1.2059568097403506\n",
      "train loss:1.232625039140739\n",
      "train loss:1.1528471347954228\n",
      "train loss:0.9188211148201084\n",
      "train loss:0.9741627490731046\n",
      "train loss:1.0549764004670672\n",
      "train loss:1.0665393382033381\n",
      "train loss:0.9868182554242135\n",
      "train loss:1.0705639214640335\n",
      "train loss:1.0258414129352984\n",
      "train loss:1.1152580519881292\n",
      "train loss:1.071632747556905\n",
      "train loss:1.0411684996319364\n",
      "train loss:1.1053760488864106\n",
      "train loss:0.9940128482297497\n",
      "train loss:1.0496586926858453\n",
      "train loss:1.1423696071167329\n",
      "train loss:0.9708653358164601\n",
      "train loss:1.2233493513133775\n",
      "train loss:1.0797850703059526\n",
      "train loss:1.0852408109884792\n",
      "train loss:0.9792167824962316\n",
      "train loss:0.942834472776159\n",
      "train loss:1.1441940620604576\n",
      "train loss:1.0952244399792053\n",
      "train loss:1.0629133737384975\n",
      "train loss:1.0643040508887858\n",
      "train loss:1.1902374106986433\n",
      "train loss:0.9515234864465372\n",
      "train loss:1.0901059631117\n",
      "train loss:0.9993524072679841\n",
      "train loss:1.118143106570892\n",
      "train loss:0.9949593678186285\n",
      "train loss:1.0585329411537499\n",
      "train loss:1.0782643037189614\n",
      "train loss:1.0010737541727082\n",
      "train loss:1.1440501309998508\n",
      "train loss:0.968060036374777\n",
      "train loss:1.1025995084769133\n",
      "train loss:1.2300183045690323\n",
      "train loss:1.0181176126325746\n",
      "train loss:1.369578832088838\n",
      "train loss:1.1428142236357537\n",
      "train loss:1.2013872554565914\n",
      "train loss:1.1470693741649598\n",
      "train loss:1.1629398750965512\n",
      "train loss:1.1476251045855026\n",
      "train loss:1.0874580400795875\n",
      "train loss:1.1937993819043449\n",
      "train loss:1.130226313819143\n",
      "train loss:1.1537743165807697\n",
      "train loss:1.1522469216693707\n",
      "train loss:1.1466872510539552\n",
      "train loss:1.0642786268530036\n",
      "train loss:0.9942033262046543\n",
      "train loss:1.0678271455442554\n",
      "train loss:1.0827937115040098\n",
      "train loss:1.1375928115342995\n",
      "train loss:1.0933152860697364\n",
      "train loss:1.2267479232705525\n",
      "train loss:1.105199708391849\n",
      "train loss:1.041106053496297\n",
      "train loss:0.9879160352804268\n",
      "train loss:0.9873709868152232\n",
      "train loss:1.0771097782053412\n",
      "train loss:1.1410331778760185\n",
      "train loss:1.0245823460264363\n",
      "train loss:1.2769600461800243\n",
      "train loss:1.2754656981452934\n",
      "train loss:1.112747165491981\n",
      "train loss:1.0705198177512794\n",
      "train loss:1.0481356038295855\n",
      "train loss:1.229704032974363\n",
      "train loss:1.046817160856572\n",
      "train loss:0.9143790011444467\n",
      "train loss:0.9725716212583864\n",
      "train loss:1.1363021160685627\n",
      "train loss:1.1839948889167304\n",
      "train loss:1.1430309215433363\n",
      "train loss:1.2108763627264616\n",
      "train loss:1.1346006679952432\n",
      "train loss:1.0064478695250316\n",
      "train loss:0.9430576846109153\n",
      "train loss:0.9630610649741205\n",
      "train loss:0.9714744977922483\n",
      "train loss:1.061405423001824\n",
      "train loss:1.233388019320149\n",
      "train loss:1.0120632379865913\n",
      "train loss:1.0506970713630914\n",
      "train loss:1.0401979141698872\n",
      "train loss:1.133378083688666\n",
      "train loss:0.9705375834520737\n",
      "train loss:1.2279751366287046\n",
      "train loss:1.3267089525596145\n",
      "train loss:1.086573052997384\n",
      "train loss:1.1145906960734042\n",
      "train loss:0.9655689855208898\n",
      "train loss:1.1481130462714932\n",
      "train loss:1.0811474253744708\n",
      "train loss:1.0321773729238564\n",
      "train loss:1.1561164895995788\n",
      "train loss:0.9756889324649732\n",
      "train loss:1.0351798732015198\n",
      "train loss:1.055084444657326\n",
      "train loss:0.9122741244813767\n",
      "train loss:1.038161165676928\n",
      "train loss:1.16737823987086\n",
      "train loss:1.076785732921616\n",
      "train loss:1.043553580132077\n",
      "train loss:1.0676918348810363\n",
      "train loss:1.0468523582057407\n",
      "train loss:0.9830135103783333\n",
      "train loss:0.9469901186248955\n",
      "train loss:0.9745125173898912\n",
      "train loss:1.2290608716773732\n",
      "train loss:1.0117239024214617\n",
      "train loss:1.008174426284421\n",
      "train loss:1.1590443242469939\n",
      "train loss:1.0850138995044845\n",
      "train loss:1.02951473478384\n",
      "train loss:1.268641193748372\n",
      "train loss:0.9705879037173105\n",
      "train loss:1.0899897511543946\n",
      "train loss:1.0205963999970171\n",
      "train loss:0.8468249003334488\n",
      "train loss:1.0062842924579127\n",
      "train loss:1.0493080447910066\n",
      "train loss:1.0544123081070453\n",
      "train loss:1.032299364680022\n",
      "train loss:1.1323844889323442\n",
      "train loss:0.9971525145459559\n",
      "train loss:1.015499972218881\n",
      "train loss:1.0332829322170332\n",
      "train loss:0.9568258842502277\n",
      "train loss:1.1897264407869257\n",
      "train loss:1.0464967522266775\n",
      "train loss:1.0459276018202612\n",
      "train loss:0.995921487093073\n",
      "train loss:0.967318136723319\n",
      "train loss:0.9798294400329879\n",
      "train loss:1.2118304894795602\n",
      "train loss:1.1519073592836582\n",
      "train loss:0.9353887778295402\n",
      "train loss:0.9005966493180355\n",
      "train loss:1.055474302106939\n",
      "train loss:0.9935104179999763\n",
      "train loss:1.1227109436058946\n",
      "train loss:1.051106003172729\n",
      "train loss:1.013257415494398\n",
      "train loss:1.011000534450848\n",
      "train loss:1.2213558237263589\n",
      "train loss:1.1944679521943455\n",
      "train loss:1.094871086903748\n",
      "train loss:0.9205346280536953\n",
      "train loss:1.003464031558427\n",
      "train loss:0.9169814549611246\n",
      "train loss:1.0165994392235258\n",
      "train loss:1.105408027327597\n",
      "train loss:0.9763179002062491\n",
      "train loss:1.006717539090219\n",
      "train loss:1.0395186092483528\n",
      "train loss:0.972891630911174\n",
      "train loss:1.0296326534777684\n",
      "train loss:1.1353764560154525\n",
      "train loss:1.1368795554044535\n",
      "train loss:0.9308333239989517\n",
      "train loss:1.1277927645862058\n",
      "train loss:0.9918363789083697\n",
      "train loss:1.091215924861299\n",
      "train loss:0.9093244311816288\n",
      "train loss:0.8759430069007135\n",
      "train loss:1.0116839851949724\n",
      "train loss:1.0429223789733266\n",
      "train loss:1.008003759390307\n",
      "train loss:1.1218248542066314\n",
      "train loss:0.9122007032641436\n",
      "train loss:1.0333380457095007\n",
      "train loss:1.1222339317390484\n",
      "train loss:1.0798523369241577\n",
      "train loss:0.9523312249762406\n",
      "train loss:0.905290167808997\n",
      "train loss:0.9961819815672459\n",
      "train loss:1.1165118156673843\n",
      "train loss:1.1641328485089482\n",
      "train loss:1.0763843984404784\n",
      "train loss:1.1252679957159095\n",
      "train loss:1.1256570642207906\n",
      "train loss:0.9763220830980183\n",
      "train loss:0.964568616704871\n",
      "train loss:1.1469241701662576\n",
      "train loss:0.9823749770065864\n",
      "train loss:1.0302626441219886\n",
      "train loss:1.0693002702139847\n",
      "train loss:1.0181596754546576\n",
      "train loss:0.9957931509078614\n",
      "train loss:0.9595011149007668\n",
      "train loss:0.9662702199927172\n",
      "train loss:1.2050931491220682\n",
      "train loss:1.018977715329685\n",
      "train loss:1.0441548730728785\n",
      "train loss:1.0610764761976375\n",
      "train loss:0.9756146421844442\n",
      "train loss:1.1564080640940093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.057042482919014\n",
      "train loss:0.9489608516911132\n",
      "train loss:1.0760455461919738\n",
      "train loss:0.9569494946625086\n",
      "train loss:1.1629465819749794\n",
      "train loss:1.178642138235727\n",
      "train loss:1.0864043612782117\n",
      "train loss:1.0441774260740413\n",
      "train loss:0.885492733338958\n",
      "train loss:0.9972761626072487\n",
      "train loss:1.1358870375914107\n",
      "train loss:1.2124777284786368\n",
      "train loss:1.2124670443233203\n",
      "train loss:1.1681995796290496\n",
      "train loss:1.1265294183948757\n",
      "train loss:0.945739721976789\n",
      "train loss:1.0778046219548958\n",
      "train loss:0.9895212480828737\n",
      "train loss:1.016989142108291\n",
      "train loss:1.0198693000003676\n",
      "train loss:1.094574559657392\n",
      "train loss:1.2906894227799213\n",
      "train loss:0.9740927047926148\n",
      "train loss:1.2446581354120398\n",
      "train loss:0.9670692590601963\n",
      "train loss:1.1943349134356862\n",
      "train loss:1.0143981612995043\n",
      "train loss:1.1864972634704518\n",
      "train loss:0.9935498796785625\n",
      "train loss:1.1019059020448767\n",
      "train loss:1.0290295159656602\n",
      "train loss:1.0245154332486062\n",
      "train loss:0.8764126361551164\n",
      "train loss:1.067358534739408\n",
      "train loss:1.0288758067289796\n",
      "train loss:0.9737236046574086\n",
      "train loss:0.8703435266954945\n",
      "train loss:1.0351909631199137\n",
      "train loss:0.9212517900987025\n",
      "train loss:0.9701606367983995\n",
      "train loss:0.9007810488621392\n",
      "train loss:1.072645717132825\n",
      "train loss:1.192332285617708\n",
      "train loss:0.9061936758797475\n",
      "train loss:1.1262436601483896\n",
      "train loss:1.3080726838840977\n",
      "train loss:1.1552012478360578\n",
      "train loss:0.9347020381839813\n",
      "train loss:1.0274950891523917\n",
      "train loss:1.048729231798776\n",
      "=== epoch:2, train acc:0.971, test acc:0.978 ===\n",
      "train loss:1.0352772989349832\n",
      "train loss:1.0332181772051017\n",
      "train loss:0.7524809462377621\n",
      "train loss:0.9836339250786517\n",
      "train loss:1.0585733622608255\n",
      "train loss:1.194077282962143\n",
      "train loss:1.009560536020754\n",
      "train loss:1.0334737283709485\n",
      "train loss:1.1701007189348342\n",
      "train loss:1.0665808963134296\n",
      "train loss:1.0702132812947804\n",
      "train loss:0.9402919277009539\n",
      "train loss:1.1005543680347558\n",
      "train loss:1.079986578999036\n",
      "train loss:1.063787108003703\n",
      "train loss:0.8902921817233294\n",
      "train loss:1.006652903477808\n",
      "train loss:1.1029996449307031\n",
      "train loss:1.137262369104718\n",
      "train loss:1.0345670905998294\n",
      "train loss:1.00609455223485\n",
      "train loss:1.0791905052827395\n",
      "train loss:1.1120946160324932\n",
      "train loss:1.1201205258807188\n",
      "train loss:0.9152063707977587\n",
      "train loss:0.8581417224824606\n",
      "train loss:0.9314759328651747\n",
      "train loss:1.0156520746379896\n",
      "train loss:1.015216276828268\n",
      "train loss:1.1881383836469797\n",
      "train loss:0.7936149496317675\n",
      "train loss:1.0776506021931833\n",
      "train loss:0.973514446358993\n",
      "train loss:0.9081476964274067\n",
      "train loss:1.0238936135265302\n",
      "train loss:1.178312926329264\n",
      "train loss:0.9853536357122192\n",
      "train loss:1.0020418926663355\n",
      "train loss:0.8787754337467006\n",
      "train loss:0.9669295429094709\n",
      "train loss:0.8714206063499315\n",
      "train loss:1.1620123103174218\n",
      "train loss:0.7144704276415937\n",
      "train loss:0.9569060538911307\n",
      "train loss:1.0524493235413495\n",
      "train loss:1.0948072865140352\n",
      "train loss:0.973688798590642\n",
      "train loss:1.247690657755345\n",
      "train loss:0.9051004415769927\n",
      "train loss:0.8930961204411207\n",
      "train loss:0.8991359779314316\n",
      "train loss:1.0194637177373163\n",
      "train loss:1.0938572734577208\n",
      "train loss:1.0099957364887546\n",
      "train loss:1.0977381572633715\n",
      "train loss:0.9908757068578242\n",
      "train loss:1.0143229114757102\n",
      "train loss:1.010975494898756\n",
      "train loss:0.9156299529976621\n",
      "train loss:1.0485563555614583\n",
      "train loss:1.008622370302854\n",
      "train loss:0.9694838225783927\n",
      "train loss:1.0501842875557204\n",
      "train loss:0.9525559909145166\n",
      "train loss:0.8228161974790573\n",
      "train loss:1.2353508918850564\n",
      "train loss:0.9159955075267768\n",
      "train loss:1.1308116211882335\n",
      "train loss:0.928924790453306\n",
      "train loss:0.9764545468763777\n",
      "train loss:0.9888055920054067\n",
      "train loss:0.9386916753252522\n",
      "train loss:0.9075442630327241\n",
      "train loss:0.9919644315580228\n",
      "train loss:1.0007974004623705\n",
      "train loss:0.9033164526704454\n",
      "train loss:0.9020877894515226\n",
      "train loss:1.0673059798999158\n",
      "train loss:1.0352823266556461\n",
      "train loss:1.031320100908497\n",
      "train loss:0.945459717631201\n",
      "train loss:0.8712270623448143\n",
      "train loss:0.9841281390316294\n",
      "train loss:0.9876096187337546\n",
      "train loss:1.1744713670309865\n",
      "train loss:1.028207163580392\n",
      "train loss:0.9539195879005676\n",
      "train loss:1.0918705278345417\n",
      "train loss:1.0091948050877926\n",
      "train loss:0.9707555716501959\n",
      "train loss:1.1141115419051801\n",
      "train loss:0.9160433512082833\n",
      "train loss:1.1016752879154696\n",
      "train loss:1.147104463598488\n",
      "train loss:1.0456489099103234\n",
      "train loss:1.0465368642458135\n",
      "train loss:0.8811377504424617\n",
      "train loss:0.7739142777056106\n",
      "train loss:0.9013253689260573\n",
      "train loss:1.132427940559672\n",
      "train loss:0.9473946912340129\n",
      "train loss:1.0057973931024955\n",
      "train loss:0.8843360560705678\n",
      "train loss:0.8649597842326888\n",
      "train loss:1.0155852580279032\n",
      "train loss:0.9974810358579158\n",
      "train loss:0.9640968647373637\n",
      "train loss:0.9154657003912458\n",
      "train loss:1.0951283764409117\n",
      "train loss:0.9323265423389553\n",
      "train loss:0.8999073541401721\n",
      "train loss:1.066787855983836\n",
      "train loss:1.0571764328430082\n",
      "train loss:0.990451338314671\n",
      "train loss:0.9227153000862846\n",
      "train loss:0.9999331630006043\n",
      "train loss:0.8791570990600759\n",
      "train loss:1.0855303456664236\n",
      "train loss:0.8693378983510285\n",
      "train loss:0.9975202554972732\n",
      "train loss:0.9271659775126037\n",
      "train loss:0.8694333824269666\n",
      "train loss:0.9888377150578278\n",
      "train loss:0.9686113422349816\n",
      "train loss:0.7982019985754981\n",
      "train loss:0.942187403618432\n",
      "train loss:0.9152569751850899\n",
      "train loss:0.9499087172996511\n",
      "train loss:1.0109425210811986\n",
      "train loss:0.993337325830705\n",
      "train loss:0.9776133704164635\n",
      "train loss:0.942833776227843\n",
      "train loss:0.9069724830388249\n",
      "train loss:0.920883433512682\n",
      "train loss:1.0519768682812107\n",
      "train loss:1.0331689480991577\n",
      "train loss:0.9341956830035578\n",
      "train loss:1.1276195597556804\n",
      "train loss:0.9461307064091927\n",
      "train loss:1.0407812360392874\n",
      "train loss:1.0213543481628296\n",
      "train loss:0.8991272854751378\n",
      "train loss:1.0931500111162349\n",
      "train loss:0.9416642773334477\n",
      "train loss:0.939733390182073\n",
      "train loss:1.0346452127753865\n",
      "train loss:0.9822366877896503\n",
      "train loss:0.9935664713977045\n",
      "train loss:0.9758723753611116\n",
      "train loss:0.9743434170822196\n",
      "train loss:0.8210158651069186\n",
      "train loss:0.9199080094820623\n",
      "train loss:1.055557550193605\n",
      "train loss:0.9695023657575041\n",
      "train loss:1.0681737230105142\n",
      "train loss:0.87695024591837\n",
      "train loss:0.8851490125789786\n",
      "train loss:1.0339946030360267\n",
      "train loss:0.9984054560471314\n",
      "train loss:1.0640021265179638\n",
      "train loss:0.9543373456789565\n",
      "train loss:0.9607248312087039\n",
      "train loss:0.9763282396092977\n",
      "train loss:0.9102991937884781\n",
      "train loss:0.8087876498497293\n",
      "train loss:0.932604983386426\n",
      "train loss:1.0029572793495354\n",
      "train loss:0.9139435663949503\n",
      "train loss:1.0057104313896585\n",
      "train loss:1.0540512852023776\n",
      "train loss:1.029052392462446\n",
      "train loss:1.047857557540844\n",
      "train loss:0.9955207247998815\n",
      "train loss:0.9595854866950814\n",
      "train loss:1.0069495642285882\n",
      "train loss:0.9089275383004238\n",
      "train loss:0.7763771219625246\n",
      "train loss:0.9009924724261035\n",
      "train loss:1.0646453767238353\n",
      "train loss:1.0208958854025665\n",
      "train loss:0.9850294815505388\n",
      "train loss:1.0467531323617711\n",
      "train loss:0.9321409339956801\n",
      "train loss:0.9208048720521744\n",
      "train loss:1.075872267202429\n",
      "train loss:1.0435606882813417\n",
      "train loss:0.9851798707565296\n",
      "train loss:0.77089130069877\n",
      "train loss:1.0776822623995617\n",
      "train loss:0.9305320164408406\n",
      "train loss:1.0074465925304878\n",
      "train loss:1.090800201336445\n",
      "train loss:1.2177070974262363\n",
      "train loss:0.9600062141872394\n",
      "train loss:0.7868338346913231\n",
      "train loss:1.1045769917872001\n",
      "train loss:1.0529654935734853\n",
      "train loss:0.9940558531743133\n",
      "train loss:1.1446368063284473\n",
      "train loss:0.8790316476355846\n",
      "train loss:0.7937113505584931\n",
      "train loss:0.9418821496591839\n",
      "train loss:1.0434714602057524\n",
      "train loss:0.9628177513088555\n",
      "train loss:1.0730889988376158\n",
      "train loss:0.9972128595300324\n",
      "train loss:0.94387020464323\n",
      "train loss:1.0357782162898979\n",
      "train loss:0.92635628225012\n",
      "train loss:0.9153275067049468\n",
      "train loss:1.0109962070253307\n",
      "train loss:0.9112275173148207\n",
      "train loss:1.043363073165093\n",
      "train loss:1.0045331210154755\n",
      "train loss:0.9357821857832589\n",
      "train loss:0.9560613382765117\n",
      "train loss:1.0527771992495523\n",
      "train loss:1.0268257029475487\n",
      "train loss:0.909006340163429\n",
      "train loss:0.9881744619787599\n",
      "train loss:0.969400343786377\n",
      "train loss:0.6934597865995655\n",
      "train loss:0.9866105550336078\n",
      "train loss:0.9325886013289495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9266420132014845\n",
      "train loss:1.2446066752562017\n",
      "train loss:1.0779104047520878\n",
      "train loss:1.1228427864979427\n",
      "train loss:0.9628068846339453\n",
      "train loss:1.0422385601715403\n",
      "train loss:1.0706277865807985\n",
      "train loss:0.9427304079746542\n",
      "train loss:0.9590159417102727\n",
      "train loss:0.8444174722602255\n",
      "train loss:1.0743559535307388\n",
      "train loss:1.0845489112507711\n",
      "train loss:1.2425764867123823\n",
      "train loss:1.1196169001737777\n",
      "train loss:0.9302674026854855\n",
      "train loss:1.1075119654637466\n",
      "train loss:1.0079472526340512\n",
      "train loss:0.8880489515890005\n",
      "train loss:1.0130763754178214\n",
      "train loss:0.9711460444898928\n",
      "train loss:1.1014091562552246\n",
      "train loss:0.8463846833547106\n",
      "train loss:0.9323816940890681\n",
      "train loss:0.890062744703161\n",
      "train loss:0.9695793337698206\n",
      "train loss:0.9296896203358296\n",
      "train loss:0.9505499710805785\n",
      "train loss:0.9568253644524419\n",
      "train loss:1.037472872188452\n",
      "train loss:1.1305643739287659\n",
      "train loss:1.0953239092065044\n",
      "train loss:0.863584857103493\n",
      "train loss:0.8440251197444313\n",
      "train loss:0.9309867772199238\n",
      "train loss:1.0509175640819308\n",
      "train loss:1.0690243994737392\n",
      "train loss:1.0102400540281673\n",
      "train loss:1.0607253673123465\n",
      "train loss:1.0414862541466967\n",
      "train loss:1.1240694214702365\n",
      "train loss:1.1376174020485115\n",
      "train loss:1.1604068817033903\n",
      "train loss:0.9331917451150317\n",
      "train loss:1.0094466143010425\n",
      "train loss:0.9880125399097048\n",
      "train loss:1.0807418421027364\n",
      "train loss:1.0629618453129883\n",
      "train loss:0.9732489318622042\n",
      "train loss:1.0255655846841696\n",
      "train loss:1.1914498631064925\n",
      "train loss:0.952207772040646\n",
      "train loss:0.9769141233511401\n",
      "train loss:0.8477794568366412\n",
      "train loss:0.9324925002838228\n",
      "train loss:0.88852910319204\n",
      "train loss:1.0499592638169788\n",
      "train loss:1.1300447006301502\n",
      "train loss:0.9566866804364438\n",
      "train loss:0.9840358005974581\n",
      "train loss:0.9042806875116719\n",
      "train loss:1.0295936861648796\n",
      "train loss:1.023877568338888\n",
      "train loss:0.9470077654701622\n",
      "train loss:1.0582109174816576\n",
      "train loss:0.9759824060666124\n",
      "train loss:1.0997399234966663\n",
      "train loss:0.935272448891068\n",
      "train loss:1.0111792422735175\n",
      "train loss:0.986238859546376\n",
      "train loss:1.1985925363803194\n",
      "train loss:1.0188232062565905\n",
      "train loss:0.7918390989874986\n",
      "train loss:0.9642493532173323\n",
      "train loss:0.9461476007332766\n",
      "train loss:1.0053832027070084\n",
      "train loss:1.0766397372684553\n",
      "train loss:0.993908505451611\n",
      "train loss:0.9098842092200813\n",
      "train loss:0.9130288978251007\n",
      "train loss:0.8829750910147338\n",
      "train loss:0.8606508755714062\n",
      "train loss:1.1711317712560494\n",
      "train loss:0.9027116424970265\n",
      "train loss:0.9927407662026577\n",
      "train loss:0.929423303226614\n",
      "train loss:1.0398100900454597\n",
      "train loss:0.881914334629533\n",
      "train loss:0.9966881409875178\n",
      "train loss:0.9711187988298398\n",
      "train loss:0.9886052605157629\n",
      "train loss:0.8889655653183864\n",
      "train loss:0.9826024130114979\n",
      "train loss:0.8967037904841445\n",
      "train loss:0.744074917391371\n",
      "train loss:0.9925082597943994\n",
      "train loss:0.9451681508203122\n",
      "train loss:0.9812995027390916\n",
      "train loss:0.8493299964893046\n",
      "train loss:1.0479770618817057\n",
      "train loss:0.8600358378072385\n",
      "train loss:0.9681513074869542\n",
      "train loss:1.12669093053964\n",
      "train loss:1.1148063818578344\n",
      "train loss:1.1223985745259708\n",
      "train loss:1.0054855163032999\n",
      "train loss:0.8846542380042799\n",
      "train loss:0.9713189949972951\n",
      "train loss:0.8372912186656294\n",
      "train loss:1.0457276365378918\n",
      "train loss:0.7652490621866899\n",
      "train loss:0.9756226519550478\n",
      "train loss:1.129808372866099\n",
      "train loss:0.9440530719493124\n",
      "train loss:0.8421414632268214\n",
      "train loss:1.2229364304139978\n",
      "train loss:0.7428433894502892\n",
      "train loss:0.8319273696466553\n",
      "train loss:1.0100311003660367\n",
      "train loss:0.8701401799478856\n",
      "train loss:1.1270631546472913\n",
      "train loss:0.947617228675528\n",
      "train loss:0.9686444416273068\n",
      "train loss:1.02901269735862\n",
      "train loss:1.0601323687161395\n",
      "train loss:1.1404983941308025\n",
      "train loss:0.8978451847219493\n",
      "train loss:1.1818086615875327\n",
      "train loss:1.0078838659480487\n",
      "train loss:0.9308680531954445\n",
      "train loss:0.9611928935748023\n",
      "train loss:1.0615673786846949\n",
      "train loss:0.8458570050896769\n",
      "train loss:0.9907046647432358\n",
      "train loss:1.1366809142118304\n",
      "train loss:1.0860547048797793\n",
      "train loss:0.9571734583789566\n",
      "train loss:0.9299976702884406\n",
      "train loss:0.9096908616616328\n",
      "train loss:0.9023452671949276\n",
      "train loss:0.9126264300616388\n",
      "train loss:0.9017582239825532\n",
      "train loss:0.8737044502754381\n",
      "train loss:1.0112410465361128\n",
      "train loss:0.9774820922965745\n",
      "train loss:0.9940986699403733\n",
      "train loss:0.9209518896433596\n",
      "train loss:0.9767249004149394\n",
      "train loss:1.0008394926881352\n",
      "train loss:1.0467111100039954\n",
      "train loss:0.885391603526265\n",
      "train loss:0.965040172945987\n",
      "train loss:0.9156774039767898\n",
      "train loss:0.9044424893605647\n",
      "train loss:0.9549006654193929\n",
      "train loss:1.1153031357178322\n",
      "train loss:0.9881473165103504\n",
      "train loss:0.9727578240309143\n",
      "train loss:0.9510306618829176\n",
      "train loss:1.061561516598182\n",
      "train loss:0.8139635767277698\n",
      "train loss:0.8051347345524019\n",
      "train loss:0.9410045038882073\n",
      "train loss:0.9628133281341112\n",
      "train loss:0.9705855935510117\n",
      "train loss:1.0297033701035296\n",
      "train loss:0.8487488947571297\n",
      "train loss:1.0474242112861096\n",
      "train loss:0.7878959970229488\n",
      "train loss:0.7332376683095301\n",
      "train loss:1.063639708933862\n",
      "train loss:1.0767860862785463\n",
      "train loss:1.0463064210385469\n",
      "train loss:0.9419641972074766\n",
      "train loss:1.1602663451295048\n",
      "train loss:0.9327531997739554\n",
      "train loss:1.0839560570607385\n",
      "train loss:0.9034645633028717\n",
      "train loss:0.9733810356814628\n",
      "train loss:1.1412174620226436\n",
      "train loss:0.9103775293964479\n",
      "train loss:1.0016613797839704\n",
      "train loss:0.9918696593215164\n",
      "train loss:0.9389000027675887\n",
      "train loss:0.9654489702636311\n",
      "train loss:0.9375067395393437\n",
      "train loss:0.8944962508848765\n",
      "train loss:0.9667315269704981\n",
      "train loss:0.8129213994354236\n",
      "train loss:1.1270244648987606\n",
      "train loss:1.119545077869239\n",
      "train loss:0.96377974434276\n",
      "train loss:1.15629488287372\n",
      "train loss:0.8917442321737521\n",
      "train loss:0.9613533359785201\n",
      "train loss:0.9518547446159141\n",
      "train loss:0.949792754074679\n",
      "train loss:0.9003684452360251\n",
      "train loss:0.7966685625912785\n",
      "train loss:0.9133259537918584\n",
      "train loss:0.8192349994299173\n",
      "train loss:1.137158306320175\n",
      "train loss:0.8511644971700849\n",
      "train loss:1.0019965039632894\n",
      "train loss:1.0103926510971\n",
      "train loss:1.1193057750089546\n",
      "train loss:0.9069150783284468\n",
      "train loss:0.8865939443872355\n",
      "train loss:0.8965179185784098\n",
      "train loss:0.948265132007067\n",
      "train loss:0.9198851630595685\n",
      "train loss:0.9957299093430122\n",
      "train loss:0.8932233861072075\n",
      "train loss:0.9555831645998777\n",
      "train loss:1.066501934917078\n",
      "train loss:1.0945405939051778\n",
      "train loss:0.938877540442538\n",
      "train loss:1.042601678394132\n",
      "train loss:0.8380146834289302\n",
      "train loss:0.9651021954261317\n",
      "train loss:1.113582299682415\n",
      "train loss:0.9843631045233476\n",
      "train loss:0.9794707166921464\n",
      "train loss:1.0536773836963074\n",
      "train loss:0.8522257975901901\n",
      "train loss:0.8066213225962434\n",
      "train loss:1.0082849449065272\n",
      "train loss:0.9570144050586289\n",
      "train loss:0.9290362481437776\n",
      "train loss:1.0763744762831715\n",
      "train loss:0.8069026862369619\n",
      "train loss:0.9750879591503164\n",
      "train loss:1.0891630116660216\n",
      "train loss:1.1211503731581742\n",
      "train loss:1.0500997418435385\n",
      "train loss:0.921841057426538\n",
      "train loss:0.8711433431435931\n",
      "train loss:0.9153132763347714\n",
      "train loss:1.0338691244560472\n",
      "train loss:0.8744112439396879\n",
      "train loss:0.9770076230003685\n",
      "train loss:0.925931894844103\n",
      "train loss:0.9716641072436454\n",
      "train loss:1.120195476116024\n",
      "train loss:1.033220507859701\n",
      "train loss:1.1310245308915403\n",
      "train loss:0.9543305417603133\n",
      "train loss:1.0308576523388633\n",
      "train loss:0.9607852600437691\n",
      "train loss:0.881379481621122\n",
      "train loss:1.0872459648202286\n",
      "train loss:0.9704340258181812\n",
      "train loss:0.8281055673094858\n",
      "train loss:0.8076075993103687\n",
      "train loss:0.9186813530965605\n",
      "train loss:0.9562094285755657\n",
      "train loss:0.8843434708267126\n",
      "train loss:1.1306546419383774\n",
      "train loss:1.044348760749571\n",
      "train loss:0.899192417925963\n",
      "train loss:1.0845613495685569\n",
      "train loss:0.9159974982643891\n",
      "train loss:1.0987932542150998\n",
      "train loss:1.0314063957458866\n",
      "train loss:0.7470571881718411\n",
      "train loss:0.8728202412549461\n",
      "train loss:0.9652923381461029\n",
      "train loss:0.968306344022364\n",
      "train loss:1.223493253540027\n",
      "train loss:1.0388585445324803\n",
      "train loss:1.007903190125317\n",
      "train loss:1.150838255585528\n",
      "train loss:0.9050327572514262\n",
      "train loss:1.033497849591218\n",
      "train loss:0.8075041070805313\n",
      "train loss:0.8138370596666171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0383092107230236\n",
      "train loss:1.111724346725386\n",
      "train loss:0.8784723457440932\n",
      "train loss:1.1300108553505042\n",
      "train loss:0.8238988271838832\n",
      "train loss:0.8314948220661489\n",
      "train loss:1.131232385351023\n",
      "train loss:1.0752993993994417\n",
      "train loss:0.8963341619692079\n",
      "train loss:0.9655118346609\n",
      "train loss:0.8131210597711624\n",
      "train loss:0.8961791721061425\n",
      "train loss:0.9055143648679866\n",
      "train loss:0.9124955246917988\n",
      "train loss:1.0662204442971868\n",
      "train loss:1.1033814148256837\n",
      "train loss:0.8828057440754159\n",
      "train loss:0.789414179125439\n",
      "train loss:0.9009422461308801\n",
      "train loss:0.7728434585871092\n",
      "train loss:0.776515325156716\n",
      "train loss:0.8830872693516\n",
      "train loss:0.9374835402329461\n",
      "train loss:0.9448653749478101\n",
      "train loss:0.9665900415577725\n",
      "train loss:0.9458140547056835\n",
      "train loss:0.9681632855656533\n",
      "train loss:0.9803517468264737\n",
      "train loss:0.8600362081067792\n",
      "train loss:0.8768420836604837\n",
      "train loss:0.6904719083339629\n",
      "train loss:0.9536935246465329\n",
      "train loss:1.1006070497547873\n",
      "train loss:0.7346761395928625\n",
      "train loss:0.9292978736566948\n",
      "train loss:0.8962045472690583\n",
      "train loss:0.9711074800072416\n",
      "train loss:0.8274548129380928\n",
      "train loss:1.0901385556093515\n",
      "train loss:0.886383567286076\n",
      "train loss:0.9121861499661615\n",
      "train loss:0.8518055518331519\n",
      "train loss:1.0163757354207694\n",
      "train loss:1.1275075194971642\n",
      "train loss:1.0415815336375207\n",
      "train loss:0.9945834096823496\n",
      "train loss:0.9632032412954393\n",
      "train loss:0.7793442727206482\n",
      "train loss:0.8522052139308061\n",
      "train loss:0.9775568944351385\n",
      "train loss:0.9381440036846853\n",
      "train loss:1.0426124645871342\n",
      "train loss:0.8989088157885123\n",
      "train loss:1.0087450224954653\n",
      "train loss:0.9375492274917474\n",
      "train loss:0.9320567282174846\n",
      "train loss:0.8179160109536886\n",
      "train loss:1.0414847216702339\n",
      "train loss:0.8822886281710063\n",
      "train loss:0.9527055080295668\n",
      "train loss:1.0115077505180903\n",
      "train loss:0.9153532982607839\n",
      "train loss:1.190260452605368\n",
      "train loss:1.0000891090031963\n",
      "train loss:0.7247758274205826\n",
      "train loss:0.9680473696809629\n",
      "train loss:0.8893302639451443\n",
      "train loss:0.904568466080351\n",
      "train loss:0.7566465171067367\n",
      "train loss:0.9451682479128545\n",
      "train loss:1.051357537979209\n",
      "train loss:1.0252546625352932\n",
      "train loss:1.1608355667002919\n",
      "train loss:0.7523806330040198\n",
      "train loss:0.8410397609631818\n",
      "train loss:0.7865056854092974\n",
      "train loss:1.0072252187485464\n",
      "train loss:0.9841437216844554\n",
      "train loss:1.0606961620648427\n",
      "train loss:0.9325792868420681\n",
      "train loss:0.9887329011620628\n",
      "train loss:0.919908229797618\n",
      "train loss:0.9277431503700998\n",
      "train loss:0.9145492321167277\n",
      "train loss:1.0229378207414206\n",
      "train loss:0.9142259069599337\n",
      "train loss:0.9917455861262465\n",
      "train loss:0.9038576570393783\n",
      "train loss:0.9487725923284914\n",
      "train loss:1.1629811801586516\n",
      "train loss:1.1960580897615747\n",
      "train loss:0.8494375043327992\n",
      "train loss:1.0618012760717856\n",
      "train loss:1.0214156789823083\n",
      "train loss:1.0098414702800884\n",
      "train loss:0.8953096491498388\n",
      "train loss:0.7697639215044273\n",
      "train loss:0.8823116990114758\n",
      "train loss:0.9276140654021263\n",
      "train loss:0.7959432216307653\n",
      "train loss:1.0488030186241755\n",
      "=== epoch:3, train acc:0.988, test acc:0.974 ===\n",
      "train loss:1.0698048654752386\n",
      "train loss:1.0548473454546745\n",
      "train loss:0.9590788933800024\n",
      "train loss:0.9316363618069599\n",
      "train loss:0.9700958348047206\n",
      "train loss:1.030333513212455\n",
      "train loss:1.0214822774469798\n",
      "train loss:1.014146205689655\n",
      "train loss:1.0325854995729369\n",
      "train loss:1.0486064405321411\n",
      "train loss:0.9341791970281429\n",
      "train loss:0.9350289406221388\n",
      "train loss:0.9959821685998408\n",
      "train loss:0.8285739673532795\n",
      "train loss:1.0905358897697053\n",
      "train loss:1.102078532078958\n",
      "train loss:1.0011583015713228\n",
      "train loss:1.0590758933859963\n",
      "train loss:0.8768527470350428\n",
      "train loss:0.9806676870570155\n",
      "train loss:0.9418049424421139\n",
      "train loss:0.9744900711497191\n",
      "train loss:0.8336781316001941\n",
      "train loss:0.9765946809073555\n",
      "train loss:0.9910845702704154\n",
      "train loss:0.932318487389907\n",
      "train loss:0.9667356754446351\n",
      "train loss:0.7687905095262857\n",
      "train loss:0.8940522668709439\n",
      "train loss:1.0991604948308344\n",
      "train loss:0.7990056152375001\n",
      "train loss:0.995506320470249\n",
      "train loss:0.9363648847753396\n",
      "train loss:0.936496203174648\n",
      "train loss:0.7960450417548444\n",
      "train loss:0.9728720842883664\n",
      "train loss:0.9359844168127484\n",
      "train loss:1.056018705667248\n",
      "train loss:0.8875385311624447\n",
      "train loss:0.8250782518005327\n",
      "train loss:1.0092246701386407\n",
      "train loss:0.8184445450241877\n",
      "train loss:0.8680332039123648\n",
      "train loss:0.7951588237895774\n",
      "train loss:0.9478651863393586\n",
      "train loss:0.8954116524856163\n",
      "train loss:0.9855974823871378\n",
      "train loss:0.9504677451502944\n",
      "train loss:0.8654045422691832\n",
      "train loss:0.7884963484348024\n",
      "train loss:0.8594539464383172\n",
      "train loss:0.9648268097909614\n",
      "train loss:0.8761143159645058\n",
      "train loss:0.9252912020229382\n",
      "train loss:0.8149756190691406\n",
      "train loss:0.9289535296361361\n",
      "train loss:0.9674494663535955\n",
      "train loss:1.0221088158547826\n",
      "train loss:0.7904010091816694\n",
      "train loss:0.936854777969375\n",
      "train loss:0.9370260952215119\n",
      "train loss:0.9551034251653029\n",
      "train loss:1.0825180742045328\n",
      "train loss:0.9338950762618785\n",
      "train loss:0.9792739142327058\n",
      "train loss:0.9919119962967214\n",
      "train loss:0.7166700591951773\n",
      "train loss:0.9738003687352184\n",
      "train loss:0.9901187333297845\n",
      "train loss:0.9409063687826804\n",
      "train loss:1.0146067523319244\n",
      "train loss:0.9954646248867917\n",
      "train loss:1.0046254743865035\n",
      "train loss:0.9422291505702556\n",
      "train loss:0.9853808863595925\n",
      "train loss:0.8684726534724874\n",
      "train loss:0.946357594353299\n",
      "train loss:0.8717318086056548\n",
      "train loss:0.7593365897013963\n",
      "train loss:0.9285821296028076\n",
      "train loss:0.8681445542182992\n",
      "train loss:0.9857588084339446\n",
      "train loss:0.9694339465772039\n",
      "train loss:1.2241902525024089\n",
      "train loss:0.9267781537782741\n",
      "train loss:1.0148836940195398\n",
      "train loss:0.794591847373416\n",
      "train loss:0.7203488343810396\n",
      "train loss:0.8030384878595324\n",
      "train loss:0.8228357605988667\n",
      "train loss:1.0798786243519358\n",
      "train loss:1.0515837187881945\n",
      "train loss:1.14983468484252\n",
      "train loss:0.9762677110259141\n",
      "train loss:1.0667558945463254\n",
      "train loss:1.0246032988350076\n",
      "train loss:0.8938069937893808\n",
      "train loss:0.8832855462327268\n",
      "train loss:0.8167108394105038\n",
      "train loss:0.8512469587149785\n",
      "train loss:0.8147237827742031\n",
      "train loss:1.029497733345388\n",
      "train loss:0.9011779370454421\n",
      "train loss:0.9530270422096634\n",
      "train loss:0.790194422254009\n",
      "train loss:1.0095194587378467\n",
      "train loss:0.9625760232442048\n",
      "train loss:0.9552710809160573\n",
      "train loss:0.8737053795800125\n",
      "train loss:0.9373283061710345\n",
      "train loss:0.9497233447246687\n",
      "train loss:0.8545057789052118\n",
      "train loss:0.9438947623115003\n",
      "train loss:0.8399218379020662\n",
      "train loss:0.9922215916690064\n",
      "train loss:0.847411553341044\n",
      "train loss:0.9782982113556771\n",
      "train loss:0.9007362933279736\n",
      "train loss:0.9571836821584685\n",
      "train loss:0.9195216517299795\n",
      "train loss:0.8827492947517913\n",
      "train loss:0.8699085503827985\n",
      "train loss:0.8291427275642822\n",
      "train loss:1.0134843814177399\n",
      "train loss:1.048849851559744\n",
      "train loss:0.8603471483993654\n",
      "train loss:0.8659796993022587\n",
      "train loss:0.9224913020207601\n",
      "train loss:0.8172967622455475\n",
      "train loss:1.240193261380755\n",
      "train loss:0.8205759896639726\n",
      "train loss:0.9615020313364977\n",
      "train loss:0.9550772381727758\n",
      "train loss:0.8429022081063252\n",
      "train loss:0.938223323930448\n",
      "train loss:0.8084457969276224\n",
      "train loss:1.103459855564215\n",
      "train loss:0.9632363927476824\n",
      "train loss:0.9596070734658135\n",
      "train loss:0.9228475815728793\n",
      "train loss:0.9600298399735802\n",
      "train loss:0.8233744694405257\n",
      "train loss:0.8676402391982181\n",
      "train loss:1.0510543786196154\n",
      "train loss:0.8623471221167993\n",
      "train loss:1.0615211400486562\n",
      "train loss:0.8835747022781569\n",
      "train loss:0.934588783696732\n",
      "train loss:0.8103005600758059\n",
      "train loss:0.8782482693557898\n",
      "train loss:0.8376222306958447\n",
      "train loss:0.9507238770275069\n",
      "train loss:1.026825526693779\n",
      "train loss:0.839280143762866\n",
      "train loss:0.8494868051712271\n",
      "train loss:0.8971985935075049\n",
      "train loss:0.9025924524009363\n",
      "train loss:0.986916944193093\n",
      "train loss:1.1048535806344162\n",
      "train loss:1.0063980858056905\n",
      "train loss:0.8363382654227343\n",
      "train loss:0.8446278217626028\n",
      "train loss:0.9309169153205553\n",
      "train loss:1.0874297828267279\n",
      "train loss:0.9538451126298177\n",
      "train loss:0.9537573089289362\n",
      "train loss:0.8466754547003547\n",
      "train loss:0.7449119478619408\n",
      "train loss:0.9186163389919912\n",
      "train loss:1.0040217917550311\n",
      "train loss:1.000506327435607\n",
      "train loss:0.8136289681042418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9725544648636607\n",
      "train loss:0.927577935012832\n",
      "train loss:1.0237054028501098\n",
      "train loss:1.1039557818061474\n",
      "train loss:1.1006188358367996\n",
      "train loss:0.7547783416147085\n",
      "train loss:0.9928367838684551\n",
      "train loss:0.902784864406545\n",
      "train loss:1.0183526547297452\n",
      "train loss:0.7599673308610322\n",
      "train loss:0.8836028689622336\n",
      "train loss:1.0617669056268837\n",
      "train loss:1.1026085233268426\n",
      "train loss:1.038823578432346\n",
      "train loss:1.1131088932613507\n",
      "train loss:0.8215224051002454\n",
      "train loss:0.9530953469210537\n",
      "train loss:0.9997527998796155\n",
      "train loss:1.1157362344572777\n",
      "train loss:0.8210306231031246\n",
      "train loss:1.0103741616833937\n",
      "train loss:0.9206088113846694\n",
      "train loss:0.9137667178919255\n",
      "train loss:1.0665052943434\n",
      "train loss:1.046583136885408\n",
      "train loss:1.0060341591737874\n",
      "train loss:1.1454897138147329\n",
      "train loss:0.791531254942307\n",
      "train loss:0.8497146903105361\n",
      "train loss:1.1077046351158675\n",
      "train loss:1.009335208123928\n",
      "train loss:1.001157042376153\n",
      "train loss:1.057527679123845\n",
      "train loss:0.8973672394141765\n",
      "train loss:0.9978220833583146\n",
      "train loss:0.929149147566368\n",
      "train loss:0.9575944217143735\n",
      "train loss:0.9503133806143385\n",
      "train loss:0.9625415392805199\n",
      "train loss:0.9849812589453638\n",
      "train loss:0.7750895253477245\n",
      "train loss:0.8801969544717916\n",
      "train loss:0.9751158511114193\n",
      "train loss:0.8289495193404006\n",
      "train loss:1.0163229416725332\n",
      "train loss:0.8966028175309867\n",
      "train loss:0.9005745160794134\n",
      "train loss:1.0522186581697819\n",
      "train loss:1.0011382903276116\n",
      "train loss:1.0135795565298837\n",
      "train loss:0.8673785040176597\n",
      "train loss:0.9791528255143291\n",
      "train loss:0.7684831420363962\n",
      "train loss:0.9597251452954785\n",
      "train loss:0.8672985171118627\n",
      "train loss:0.7896304208808835\n",
      "train loss:0.9265396291565569\n",
      "train loss:0.8605425225609222\n",
      "train loss:1.0505213755042309\n",
      "train loss:0.7756409792291465\n",
      "train loss:0.834872167408357\n",
      "train loss:0.8995015415462764\n",
      "train loss:0.8939345508237747\n",
      "train loss:0.8800007141460856\n",
      "train loss:0.9253950213055342\n",
      "train loss:0.9206734785042466\n",
      "train loss:0.9699136340504623\n",
      "train loss:0.8960139481113433\n",
      "train loss:1.0338354483495669\n",
      "train loss:0.9837600510303232\n",
      "train loss:1.0018273879210908\n",
      "train loss:0.8191837105593788\n",
      "train loss:0.887988597759911\n",
      "train loss:0.859718766253496\n",
      "train loss:1.1298470567871177\n",
      "train loss:0.9888470053794475\n",
      "train loss:0.8332350301516829\n",
      "train loss:0.8897039801638145\n",
      "train loss:0.7537813045447176\n",
      "train loss:0.9544936597309349\n",
      "train loss:0.9629439494866834\n",
      "train loss:0.8039597824081238\n",
      "train loss:1.1723489439770516\n",
      "train loss:0.9569073882118844\n",
      "train loss:0.8234961983452486\n",
      "train loss:0.8443640530224333\n",
      "train loss:1.0326156394747337\n",
      "train loss:0.9159713252618644\n",
      "train loss:1.0255623490132517\n",
      "train loss:0.8898933927850128\n",
      "train loss:1.0404163352418416\n",
      "train loss:0.89774082353199\n",
      "train loss:0.8917576401121188\n",
      "train loss:0.9345782154202937\n",
      "train loss:1.0045984596730662\n",
      "train loss:0.8004275370421571\n",
      "train loss:0.9272645436111396\n",
      "train loss:0.8646201878219902\n",
      "train loss:0.950490463091605\n",
      "train loss:1.1095271695036408\n",
      "train loss:0.9718793720214116\n",
      "train loss:0.9885528452754866\n",
      "train loss:1.0324670125916913\n",
      "train loss:0.882482433531494\n",
      "train loss:0.8975142378016743\n",
      "train loss:1.1480420318944586\n",
      "train loss:0.9213405211337496\n",
      "train loss:0.9960673098057028\n",
      "train loss:1.0098486525915953\n",
      "train loss:0.9144966988473087\n",
      "train loss:0.8269218077286756\n",
      "train loss:0.9691917003612542\n",
      "train loss:0.9260930345086859\n",
      "train loss:1.012929369026999\n",
      "train loss:0.9087660367212763\n",
      "train loss:0.6792132028017099\n",
      "train loss:0.9202201069956977\n",
      "train loss:0.9690467876824433\n",
      "train loss:0.800242592661117\n",
      "train loss:1.009165567910854\n",
      "train loss:0.906462711834866\n",
      "train loss:0.9218217124806326\n",
      "train loss:0.9759751579908393\n",
      "train loss:1.1071635459311266\n",
      "train loss:1.0173246214642442\n",
      "train loss:0.8513370204991998\n",
      "train loss:0.9435736416417506\n",
      "train loss:1.090368728348099\n",
      "train loss:0.8268013852157963\n",
      "train loss:0.9080011748710218\n",
      "train loss:0.9231990043896918\n",
      "train loss:0.9078714611182683\n",
      "train loss:0.9036196962266223\n",
      "train loss:1.004674792721946\n",
      "train loss:1.0092714554040727\n",
      "train loss:0.8799221140947857\n",
      "train loss:0.8030894501248719\n",
      "train loss:1.035974643266644\n",
      "train loss:1.0314015527676554\n",
      "train loss:1.0448497927288196\n",
      "train loss:1.0293008369848464\n",
      "train loss:1.00333833682106\n",
      "train loss:0.9047976417478358\n",
      "train loss:0.7315967894019169\n",
      "train loss:0.9366495763232976\n",
      "train loss:1.0017009443076594\n",
      "train loss:0.8870314651597683\n",
      "train loss:1.0491068424882917\n",
      "train loss:0.9632244423582428\n",
      "train loss:0.9089425659785441\n",
      "train loss:1.0042529172848098\n",
      "train loss:1.0315131528334236\n",
      "train loss:0.988075659266572\n",
      "train loss:1.0440310402758812\n",
      "train loss:0.9578791637851855\n",
      "train loss:1.0870386963793295\n",
      "train loss:0.8893046590812496\n",
      "train loss:0.978821861360456\n",
      "train loss:0.9593828286313242\n",
      "train loss:0.9154534699299738\n",
      "train loss:1.045868895068817\n",
      "train loss:0.9759045287702969\n",
      "train loss:1.0338204361012877\n",
      "train loss:0.6826059926469016\n",
      "train loss:1.032099314533815\n",
      "train loss:0.8614145108388367\n",
      "train loss:0.8736640152171918\n",
      "train loss:0.9753338706133663\n",
      "train loss:1.0970442737800297\n",
      "train loss:0.8448657708989221\n",
      "train loss:0.8739628826923935\n",
      "train loss:0.8814596191817553\n",
      "train loss:0.914519274865473\n",
      "train loss:0.8071341149159789\n",
      "train loss:0.8353371890834722\n",
      "train loss:0.9061046572544248\n",
      "train loss:0.8772307617464534\n",
      "train loss:1.0179304743370587\n",
      "train loss:0.8770237657424521\n",
      "train loss:0.9918939320014131\n",
      "train loss:0.9556598233793643\n",
      "train loss:0.9924334627683361\n",
      "train loss:0.9975684770505003\n",
      "train loss:1.0205553459674455\n",
      "train loss:1.1312286673783196\n",
      "train loss:0.8191347797852351\n",
      "train loss:0.9442555882586215\n",
      "train loss:1.0381507053877181\n",
      "train loss:0.9031683930781491\n",
      "train loss:0.9872359264627765\n",
      "train loss:0.7588409346909912\n",
      "train loss:0.9908095905142433\n",
      "train loss:1.0061156398818607\n",
      "train loss:0.9598311666831885\n",
      "train loss:0.8248439448975123\n",
      "train loss:0.8499806410683455\n",
      "train loss:0.9641388841700249\n",
      "train loss:0.972886478136815\n",
      "train loss:1.0406708359143422\n",
      "train loss:0.826092822953275\n",
      "train loss:0.7337600626138877\n",
      "train loss:1.1337783966520538\n",
      "train loss:1.0669009490278478\n",
      "train loss:1.1237236426601533\n",
      "train loss:0.8569579156211975\n",
      "train loss:0.9233665256510445\n",
      "train loss:1.058919544870229\n",
      "train loss:0.9664093189424776\n",
      "train loss:0.984712452392639\n",
      "train loss:0.9237152039876493\n",
      "train loss:0.8672645409688262\n",
      "train loss:0.9011431221264238\n",
      "train loss:1.0167738695290631\n",
      "train loss:0.6896451499674455\n",
      "train loss:0.9433814585713414\n",
      "train loss:1.0252084098045762\n",
      "train loss:0.7456239421242183\n",
      "train loss:0.746073194555248\n",
      "train loss:0.9723676034953496\n",
      "train loss:0.9748910480254984\n",
      "train loss:0.9602528822786277\n",
      "train loss:0.8560457312458977\n",
      "train loss:0.8568195430939065\n",
      "train loss:1.0285129217041213\n",
      "train loss:0.8953261088981102\n",
      "train loss:0.8167637317751192\n",
      "train loss:0.8982314174425329\n",
      "train loss:0.821891552674473\n",
      "train loss:0.8223096389178682\n",
      "train loss:0.9552989624529025\n",
      "train loss:0.9870027217742426\n",
      "train loss:0.7748904666445567\n",
      "train loss:0.9805152704638473\n",
      "train loss:1.0120028335895912\n",
      "train loss:0.7769490790522862\n",
      "train loss:0.9413961934499067\n",
      "train loss:0.8371878601377759\n",
      "train loss:0.8060270723993627\n",
      "train loss:0.8328662463702667\n",
      "train loss:0.9067406024133181\n",
      "train loss:0.7966263780264596\n",
      "train loss:1.022908519051348\n",
      "train loss:0.7983862289750505\n",
      "train loss:1.0085905576039658\n",
      "train loss:0.8033618736502462\n",
      "train loss:0.9536389405290141\n",
      "train loss:0.8303810772465684\n",
      "train loss:1.0427649343296657\n",
      "train loss:0.9401888185063197\n",
      "train loss:0.8179801083924481\n",
      "train loss:1.0554394572282901\n",
      "train loss:1.0048920663074519\n",
      "train loss:0.7435155791206456\n",
      "train loss:1.133460415827278\n",
      "train loss:0.9214448089220951\n",
      "train loss:1.1834565731615705\n",
      "train loss:0.9034500156297828\n",
      "train loss:1.1127000258501685\n",
      "train loss:0.9804741448047289\n",
      "train loss:1.0553417066510715\n",
      "train loss:1.0364920070940875\n",
      "train loss:0.8707756031201411\n",
      "train loss:1.0181634532491817\n",
      "train loss:0.9139022367242317\n",
      "train loss:0.8856940186821518\n",
      "train loss:0.9235548324277899\n",
      "train loss:0.8229243727036664\n",
      "train loss:0.9116205806424859\n",
      "train loss:0.8068165422248295\n",
      "train loss:1.0200475704944925\n",
      "train loss:0.730361032166675\n",
      "train loss:1.043559395067155\n",
      "train loss:0.8239743368007539\n",
      "train loss:0.7221040334952263\n",
      "train loss:0.9751921741510668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0169267884209552\n",
      "train loss:0.9792234628078232\n",
      "train loss:0.950340867241526\n",
      "train loss:0.7622275694472269\n",
      "train loss:0.8212246046458733\n",
      "train loss:0.9191947124452412\n",
      "train loss:1.082048621603294\n",
      "train loss:0.9187306311642588\n",
      "train loss:0.8272626717124978\n",
      "train loss:1.0768104970240397\n",
      "train loss:0.8774969291268697\n",
      "train loss:0.8546868237201012\n",
      "train loss:0.7050808343574505\n",
      "train loss:0.8954436187488451\n",
      "train loss:1.101271286755468\n",
      "train loss:0.844577542640657\n",
      "train loss:0.7282475230347936\n",
      "train loss:1.0672992933656873\n",
      "train loss:0.9622883243932258\n",
      "train loss:0.9631617296260135\n",
      "train loss:0.8607296386126339\n",
      "train loss:0.919169291268221\n",
      "train loss:0.8493283276334498\n",
      "train loss:1.011273524158951\n",
      "train loss:1.0039767856081976\n",
      "train loss:1.1204075933873823\n",
      "train loss:0.9545543659066769\n",
      "train loss:0.9043248225912975\n",
      "train loss:0.8527489274285032\n",
      "train loss:0.8903009322995801\n",
      "train loss:0.8897152678933787\n",
      "train loss:1.086869067632208\n",
      "train loss:0.7499073149543891\n",
      "train loss:0.9164410415152665\n",
      "train loss:0.960816856009541\n",
      "train loss:0.9706940786176854\n",
      "train loss:0.8846846646500701\n",
      "train loss:0.8249420524999412\n",
      "train loss:0.7637328670154715\n",
      "train loss:0.9144368663517879\n",
      "train loss:0.9923249404673696\n",
      "train loss:0.9616417170050658\n",
      "train loss:0.8209938998232706\n",
      "train loss:0.8334079404217194\n",
      "train loss:0.9049931145673756\n",
      "train loss:0.903895264443262\n",
      "train loss:0.8951964514755534\n",
      "train loss:1.0069564057836993\n",
      "train loss:0.8569579135649882\n",
      "train loss:0.982502345261679\n",
      "train loss:0.7883121739060508\n",
      "train loss:0.7713047027728989\n",
      "train loss:1.0285735132687464\n",
      "train loss:0.980979657399\n",
      "train loss:0.9865471432948674\n",
      "train loss:0.9660919247520137\n",
      "train loss:0.9634043553051118\n",
      "train loss:0.8709989393717347\n",
      "train loss:0.7906464680585111\n",
      "train loss:0.8541030954650948\n",
      "train loss:0.7557075594098898\n",
      "train loss:0.8250533897515793\n",
      "train loss:0.7570201405845602\n",
      "train loss:0.9778407390432429\n",
      "train loss:0.9789403521391455\n",
      "train loss:1.1040193280172534\n",
      "train loss:0.9940713152907914\n",
      "train loss:1.0917909767057525\n",
      "train loss:0.8576164034876771\n",
      "train loss:0.9470452812974712\n",
      "train loss:0.9888163649221632\n",
      "train loss:1.0383644147855815\n",
      "train loss:0.8943021458393636\n",
      "train loss:1.0327680593669473\n",
      "train loss:0.8096207545047759\n",
      "train loss:1.0384612424526036\n",
      "train loss:0.8921937243632492\n",
      "train loss:1.1256662351830131\n",
      "train loss:0.883164416267499\n",
      "train loss:1.0826612680718273\n",
      "train loss:1.0042610520897086\n",
      "train loss:0.815910939815889\n",
      "train loss:0.8874831801017868\n",
      "train loss:0.7851270493556146\n",
      "train loss:0.9575196760735125\n",
      "train loss:0.9413560662389918\n",
      "train loss:0.9309997174913742\n",
      "train loss:1.0008025928832756\n",
      "train loss:0.9193008652273598\n",
      "train loss:0.746236407898145\n",
      "train loss:0.8066454920246118\n",
      "train loss:0.8538592483037759\n",
      "train loss:0.8911476200164082\n",
      "train loss:0.9763331841046936\n",
      "train loss:0.8136126114515028\n",
      "train loss:0.6576872831420191\n",
      "train loss:0.9536838457132886\n",
      "train loss:0.9084231023267134\n",
      "train loss:0.9914757893967794\n",
      "train loss:0.8563610609379747\n",
      "train loss:0.8233450028619823\n",
      "train loss:0.879285814787633\n",
      "train loss:0.9958440253957761\n",
      "train loss:0.9909986182108012\n",
      "train loss:0.9459353673294102\n",
      "train loss:0.866205385326224\n",
      "train loss:0.8907387238222949\n",
      "train loss:0.9644719734352911\n",
      "train loss:1.0375586472358713\n",
      "train loss:1.0447975232886935\n",
      "train loss:0.9120998243433799\n",
      "train loss:1.0139667034784012\n",
      "train loss:0.8996394054485408\n",
      "train loss:0.7756951510322864\n",
      "train loss:0.9699406901735804\n",
      "train loss:1.0287314055958658\n",
      "train loss:0.8657570444540162\n",
      "train loss:1.056187212635722\n",
      "train loss:0.9534425090618182\n",
      "train loss:0.8161877096848709\n",
      "train loss:0.8648268100314089\n",
      "train loss:1.017994538436511\n",
      "train loss:0.9355540363150375\n",
      "train loss:0.9583383504135015\n",
      "train loss:1.029690460340807\n",
      "train loss:0.9105859686144154\n",
      "train loss:0.886746216812969\n",
      "train loss:0.8408187641621043\n",
      "train loss:0.9229165343821129\n",
      "train loss:0.9561835427533564\n",
      "train loss:0.9317522510708542\n",
      "train loss:0.8371719030696382\n",
      "train loss:0.679715498720122\n",
      "train loss:0.9954030471554236\n",
      "train loss:0.9728493260109896\n",
      "train loss:1.0800191646753616\n",
      "train loss:0.8739796473524066\n",
      "train loss:1.0188519092253892\n",
      "train loss:0.8453168870190256\n",
      "train loss:0.9690904684509302\n",
      "train loss:0.9742838588533246\n",
      "train loss:0.8686390203107058\n",
      "train loss:0.9158359216264437\n",
      "train loss:0.8697556952260147\n",
      "train loss:0.9566192990136579\n",
      "train loss:1.01552806051188\n",
      "train loss:1.036103006540585\n",
      "train loss:0.9099150897616728\n",
      "train loss:0.9964149293374244\n",
      "train loss:1.0589844502591543\n",
      "train loss:0.8097942644298393\n",
      "train loss:0.7912378264428186\n",
      "train loss:1.1706168915324342\n",
      "=== epoch:4, train acc:0.985, test acc:0.974 ===\n",
      "train loss:0.9595293987643845\n",
      "train loss:1.0169710583795817\n",
      "train loss:0.8572115883198091\n",
      "train loss:0.9652625227140804\n",
      "train loss:0.9745181720535189\n",
      "train loss:1.1392431336088678\n",
      "train loss:0.8542193846324545\n",
      "train loss:0.8711026752678661\n",
      "train loss:0.7532178273744755\n",
      "train loss:0.9770322045141255\n",
      "train loss:0.988780049001426\n",
      "train loss:0.9023336780023716\n",
      "train loss:0.9496739177411173\n",
      "train loss:0.993234577554828\n",
      "train loss:0.9382700326931215\n",
      "train loss:0.8595182135240299\n",
      "train loss:0.9536181631134801\n",
      "train loss:1.0209288605906264\n",
      "train loss:0.8554518060460841\n",
      "train loss:0.8287733904388912\n",
      "train loss:0.828324934898665\n",
      "train loss:0.818728051336887\n",
      "train loss:0.9672084203288438\n",
      "train loss:0.8824829039257313\n",
      "train loss:1.0084214662844988\n",
      "train loss:0.7748230786011716\n",
      "train loss:0.8182075920618469\n",
      "train loss:0.9432877873255664\n",
      "train loss:0.8453150252962416\n",
      "train loss:0.9267455683799096\n",
      "train loss:0.9759691747865098\n",
      "train loss:0.9618304651849656\n",
      "train loss:1.0019255723239509\n",
      "train loss:0.9946167373521689\n",
      "train loss:0.9578492380702857\n",
      "train loss:1.0040215987602616\n",
      "train loss:0.9688235776377736\n",
      "train loss:0.9469046215188888\n",
      "train loss:0.9285257046055188\n",
      "train loss:1.020784284163813\n",
      "train loss:1.0162654260835111\n",
      "train loss:0.8357223772309308\n",
      "train loss:1.0353684202201072\n",
      "train loss:0.9628196159253776\n",
      "train loss:0.938357186353758\n",
      "train loss:0.7517324124849942\n",
      "train loss:0.981802049587875\n",
      "train loss:0.8870396378139249\n",
      "train loss:0.8987062338954348\n",
      "train loss:0.9081876787440063\n",
      "train loss:0.8941030242085037\n",
      "train loss:1.108274522562401\n",
      "train loss:0.8101503715910572\n",
      "train loss:0.7652745366545812\n",
      "train loss:0.9122867247498065\n",
      "train loss:0.9276454357458802\n",
      "train loss:0.771866747577235\n",
      "train loss:1.0016318069816605\n",
      "train loss:1.0613300887660735\n",
      "train loss:0.8893166784317864\n",
      "train loss:0.8629868427299846\n",
      "train loss:0.8100020066730003\n",
      "train loss:0.8213181334981886\n",
      "train loss:0.8586534547906822\n",
      "train loss:0.9016568727475641\n",
      "train loss:0.7630082258284197\n",
      "train loss:1.0203190844393921\n",
      "train loss:0.9400928098791869\n",
      "train loss:0.8704730969302364\n",
      "train loss:0.9058856338243779\n",
      "train loss:0.6680184151153489\n",
      "train loss:0.9111466491896434\n",
      "train loss:0.9364121311987396\n",
      "train loss:1.032884980432359\n",
      "train loss:0.8924600264606818\n",
      "train loss:1.021562315330403\n",
      "train loss:0.8347723190750945\n",
      "train loss:0.8311702006258015\n",
      "train loss:0.9154607400239523\n",
      "train loss:0.7963665200064916\n",
      "train loss:0.8846928750843216\n",
      "train loss:0.8681317856030819\n",
      "train loss:0.8818158405080371\n",
      "train loss:0.7770646954568926\n",
      "train loss:1.056093259687031\n",
      "train loss:0.797240923317916\n",
      "train loss:0.9751489576954028\n",
      "train loss:0.8686466399083491\n",
      "train loss:0.8767361968463987\n",
      "train loss:0.9149256656955777\n",
      "train loss:0.8340595869105626\n",
      "train loss:0.8633822774991258\n",
      "train loss:0.8404059517881154\n",
      "train loss:0.7399657600948109\n",
      "train loss:1.0252968837030827\n",
      "train loss:0.8712386026523952\n",
      "train loss:0.9113933246200753\n",
      "train loss:0.8325981326018004\n",
      "train loss:0.9032945836932401\n",
      "train loss:0.8576859198142978\n",
      "train loss:1.171371603691737\n",
      "train loss:1.0065696987799047\n",
      "train loss:0.8062147157601665\n",
      "train loss:0.8664342884539019\n",
      "train loss:1.0879584219332794\n",
      "train loss:0.8180366663385331\n",
      "train loss:0.8668694827983957\n",
      "train loss:0.9000012043667224\n",
      "train loss:0.95036481347681\n",
      "train loss:1.0454150988138842\n",
      "train loss:0.8942879407284723\n",
      "train loss:0.9576719619024168\n",
      "train loss:1.0158914298025135\n",
      "train loss:0.9675165586104544\n",
      "train loss:0.8554265784524193\n",
      "train loss:0.9524994198625271\n",
      "train loss:0.9449833486406517\n",
      "train loss:0.7303395197471363\n",
      "train loss:0.8222325708274424\n",
      "train loss:0.9097633391109876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8252079560943258\n",
      "train loss:0.8812737689885916\n",
      "train loss:1.155540187239651\n",
      "train loss:0.8956152592675156\n",
      "train loss:0.9824482554336292\n",
      "train loss:1.019017245043101\n",
      "train loss:0.864540722755336\n",
      "train loss:0.9553108672392507\n",
      "train loss:0.7816807418011354\n",
      "train loss:0.7841009971123275\n",
      "train loss:0.8895040007258035\n",
      "train loss:0.9601403180716553\n",
      "train loss:0.8173027497858284\n",
      "train loss:0.9144759652186903\n",
      "train loss:0.7685449802643373\n",
      "train loss:0.93435054324644\n",
      "train loss:0.9518312943971888\n",
      "train loss:1.0177377599923718\n",
      "train loss:0.8428647893782617\n",
      "train loss:0.741471280280232\n",
      "train loss:0.8863114582058604\n",
      "train loss:0.8803945193388085\n",
      "train loss:0.871822941630813\n",
      "train loss:0.8186509923868478\n",
      "train loss:0.8284294268819233\n",
      "train loss:1.011329356575149\n",
      "train loss:0.8771713225985538\n",
      "train loss:0.9746504513669612\n",
      "train loss:0.9499933090498645\n",
      "train loss:1.0046355420146111\n",
      "train loss:0.8998704934218148\n",
      "train loss:0.9332108921651353\n",
      "train loss:0.9198279211593433\n",
      "train loss:0.91243403611398\n",
      "train loss:0.9486749465740926\n",
      "train loss:0.9202476723249308\n",
      "train loss:1.0326856674019689\n",
      "train loss:0.8875017731498623\n",
      "train loss:0.8945008505291402\n",
      "train loss:0.9744389661770235\n",
      "train loss:0.9302924249732899\n",
      "train loss:0.703637109513881\n",
      "train loss:1.028583610600324\n",
      "train loss:0.7684901389575484\n",
      "train loss:1.0251925975923453\n",
      "train loss:1.0342972815634646\n",
      "train loss:0.9805980490919711\n",
      "train loss:1.0775672580215705\n",
      "train loss:0.8501371637790549\n",
      "train loss:0.8344848141722516\n",
      "train loss:0.9369928747433078\n",
      "train loss:0.9496687504056591\n",
      "train loss:0.9884909014977268\n",
      "train loss:0.7512400828160075\n",
      "train loss:0.987484399910519\n",
      "train loss:0.9950459582531191\n",
      "train loss:1.0266567191191194\n",
      "train loss:0.8048552006872738\n",
      "train loss:0.9800172474893738\n",
      "train loss:1.0777645636638769\n",
      "train loss:0.6448384869544\n",
      "train loss:0.8634023322308448\n",
      "train loss:0.9012651404713644\n",
      "train loss:0.8645761929813571\n",
      "train loss:0.9358839116420015\n",
      "train loss:0.9310624173573316\n",
      "train loss:0.8057808716511553\n",
      "train loss:0.9068719050011167\n",
      "train loss:1.0409037033732456\n",
      "train loss:0.7465834238513436\n",
      "train loss:0.9514194525642108\n",
      "train loss:0.8902614458281807\n",
      "train loss:1.0288319588230572\n",
      "train loss:0.8002637711462728\n",
      "train loss:0.806153216405216\n",
      "train loss:0.885264257816425\n",
      "train loss:0.9289160737487008\n",
      "train loss:0.9103462974959892\n",
      "train loss:0.9252512212881477\n",
      "train loss:0.8883723281608529\n",
      "train loss:0.9030292523034303\n",
      "train loss:0.8936501604506369\n",
      "train loss:0.7868188901193446\n",
      "train loss:0.8340586717564666\n",
      "train loss:1.1444521439533764\n",
      "train loss:1.1479315731784712\n",
      "train loss:0.8415690711767602\n",
      "train loss:0.8041272361213714\n",
      "train loss:0.9075941338513638\n",
      "train loss:0.9719267070693627\n",
      "train loss:0.7884951395453048\n",
      "train loss:1.19094041960369\n",
      "train loss:0.9412345013457951\n",
      "train loss:1.012376613921632\n",
      "train loss:0.975504354251724\n",
      "train loss:0.8489375888499103\n",
      "train loss:0.7381511188966415\n",
      "train loss:0.8998860232163534\n",
      "train loss:0.8982838633051382\n",
      "train loss:0.9453701401631514\n",
      "train loss:0.7679085317021069\n",
      "train loss:0.8379441085195479\n",
      "train loss:1.0119684196373837\n",
      "train loss:1.034921747899688\n",
      "train loss:0.9569489457582059\n",
      "train loss:0.9135933681442739\n",
      "train loss:0.9210035181435472\n",
      "train loss:0.9581620590365852\n",
      "train loss:0.8292350589111472\n",
      "train loss:0.8293509652495672\n",
      "train loss:0.8651900590091072\n",
      "train loss:1.1623434097613163\n",
      "train loss:1.0082123524604727\n",
      "train loss:0.8548548564583061\n",
      "train loss:0.921125212005546\n",
      "train loss:0.9379529100912497\n",
      "train loss:0.9129058968537174\n",
      "train loss:0.957772445424509\n",
      "train loss:0.8874444709798734\n",
      "train loss:0.9106707186251889\n",
      "train loss:0.9142136465431343\n",
      "train loss:1.0180919488952946\n",
      "train loss:0.8234794951766611\n",
      "train loss:0.7449531708345946\n",
      "train loss:0.9214010567095714\n",
      "train loss:0.991292738859238\n",
      "train loss:0.8952743714749388\n",
      "train loss:0.9180817465629473\n",
      "train loss:0.9885213043797466\n",
      "train loss:0.8982978834877793\n",
      "train loss:0.9861237735113495\n",
      "train loss:1.058556741993683\n",
      "train loss:0.933895723125279\n",
      "train loss:0.7797236939017875\n",
      "train loss:0.8446385282949129\n",
      "train loss:0.8289752735378645\n",
      "train loss:0.774947513791941\n",
      "train loss:1.0033088932819967\n",
      "train loss:1.0167849038999186\n",
      "train loss:0.8407962760231225\n",
      "train loss:1.02694240950973\n",
      "train loss:0.8871216152879248\n",
      "train loss:0.7505696635640843\n",
      "train loss:0.9193373349928275\n",
      "train loss:1.0703572436587272\n",
      "train loss:0.9878971066110667\n",
      "train loss:0.9246103954607078\n",
      "train loss:0.9097443878216543\n",
      "train loss:1.0657451474724007\n",
      "train loss:1.0851839513001875\n",
      "train loss:0.9137185799282999\n",
      "train loss:0.7946989474950259\n",
      "train loss:0.8547944181310777\n",
      "train loss:0.9429109861048827\n",
      "train loss:0.9078507478053577\n",
      "train loss:0.849512237521403\n",
      "train loss:0.8719644298490382\n",
      "train loss:0.9505446709698488\n",
      "train loss:0.8268112592375634\n",
      "train loss:0.8247482799314488\n",
      "train loss:0.8457571768988404\n",
      "train loss:0.9258441925389501\n",
      "train loss:0.8048249083078685\n",
      "train loss:0.9051522551629709\n",
      "train loss:1.0321578246852083\n",
      "train loss:1.1804337643921563\n",
      "train loss:0.7544138706129749\n",
      "train loss:0.7563114999190158\n",
      "train loss:0.7825432069021558\n",
      "train loss:0.8034717118565655\n",
      "train loss:0.8866472313363866\n",
      "train loss:0.9589010153764942\n",
      "train loss:0.8999572360515975\n",
      "train loss:0.9281271381071226\n",
      "train loss:0.8575242900087633\n",
      "train loss:1.0823154048905095\n",
      "train loss:0.9715287470304612\n",
      "train loss:0.9275949727014534\n",
      "train loss:1.0993180728213858\n",
      "train loss:0.933933215532857\n",
      "train loss:1.0408575748247704\n",
      "train loss:1.0203899147100468\n",
      "train loss:0.761569923566859\n",
      "train loss:0.9381309307668529\n",
      "train loss:0.8204514887452679\n",
      "train loss:0.9798482202017482\n",
      "train loss:0.8585587868208493\n",
      "train loss:0.9400316132301078\n",
      "train loss:0.9354502501854648\n",
      "train loss:0.8559723081392862\n",
      "train loss:0.9553776934320769\n",
      "train loss:0.9634437029052938\n",
      "train loss:0.9435700195660839\n",
      "train loss:0.9729633369885162\n",
      "train loss:0.867340066142097\n",
      "train loss:0.9547310304042985\n",
      "train loss:0.9488003244568409\n",
      "train loss:0.8898122218960215\n",
      "train loss:0.9274081345626926\n",
      "train loss:1.1299634523316833\n",
      "train loss:0.7966697056342091\n",
      "train loss:0.7166092140321462\n",
      "train loss:0.9291508518824051\n",
      "train loss:0.9927135185808317\n",
      "train loss:0.9314607197234703\n",
      "train loss:0.8101493381588263\n",
      "train loss:0.9870574807851503\n",
      "train loss:0.892300483237698\n",
      "train loss:0.9525715555570445\n",
      "train loss:0.869773123202464\n",
      "train loss:0.9258075009402398\n",
      "train loss:0.9542584212098189\n",
      "train loss:1.0527184245770604\n",
      "train loss:1.0063385425265046\n",
      "train loss:0.9462051145428475\n",
      "train loss:0.9598642942347044\n",
      "train loss:1.0216514245563426\n",
      "train loss:0.8668084852026681\n",
      "train loss:0.8554882078161367\n",
      "train loss:0.9867732554083958\n",
      "train loss:1.0108763331126966\n",
      "train loss:0.9446960925531195\n",
      "train loss:0.9360294244785631\n",
      "train loss:1.0638281958365459\n",
      "train loss:0.7624818019605842\n",
      "train loss:0.9801463310738748\n",
      "train loss:0.8679231178818201\n",
      "train loss:1.0672081426920057\n",
      "train loss:0.844023305382219\n",
      "train loss:1.0883259609290805\n",
      "train loss:0.9163883305881545\n",
      "train loss:1.0088078927989255\n",
      "train loss:0.9887919278333029\n",
      "train loss:0.8240269022610869\n",
      "train loss:0.8500658649173485\n",
      "train loss:0.785067870469026\n",
      "train loss:0.862038983923766\n",
      "train loss:0.9092018786772093\n",
      "train loss:0.7808552681484894\n",
      "train loss:0.8389558488530666\n",
      "train loss:0.9363793148172664\n",
      "train loss:0.9916274048233449\n",
      "train loss:0.7852684243484975\n",
      "train loss:0.8792556820945925\n",
      "train loss:0.8946115013064686\n",
      "train loss:0.8536256699775382\n",
      "train loss:0.8579630167766338\n",
      "train loss:0.9041206914285173\n",
      "train loss:0.8686221668573882\n",
      "train loss:1.0128323746069134\n",
      "train loss:0.9562072734464726\n",
      "train loss:0.8748736508286857\n",
      "train loss:0.9560025897292688\n",
      "train loss:0.7713051857766824\n",
      "train loss:0.9366715034184494\n",
      "train loss:0.9565117455135248\n",
      "train loss:0.9580087521718502\n",
      "train loss:1.046221845723665\n",
      "train loss:0.9847811689794324\n",
      "train loss:0.9383603823488852\n",
      "train loss:0.836028114522239\n",
      "train loss:0.8345570279831098\n",
      "train loss:1.0140850120803808\n",
      "train loss:0.8965576883873179\n",
      "train loss:0.8901132036574899\n",
      "train loss:0.664072181411391\n",
      "train loss:0.8116568083863653\n",
      "train loss:0.9019346352878245\n",
      "train loss:0.91438648017706\n",
      "train loss:0.814613093632187\n",
      "train loss:1.0159125679023662\n",
      "train loss:0.8937773906452589\n",
      "train loss:0.8755880009982469\n",
      "train loss:0.9123439977826686\n",
      "train loss:0.7381553554371564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9283335847366986\n",
      "train loss:0.9428654187236607\n",
      "train loss:1.113065953794852\n",
      "train loss:0.9026063061843874\n",
      "train loss:0.9575222545378841\n",
      "train loss:0.9494627876187838\n",
      "train loss:0.8212181096593549\n",
      "train loss:0.8936601648508166\n",
      "train loss:0.9642506924649379\n",
      "train loss:1.0462955596138857\n",
      "train loss:1.0264248329516177\n",
      "train loss:1.058079326910923\n",
      "train loss:0.8683803615580239\n",
      "train loss:0.8147745815361472\n",
      "train loss:0.8868336832707999\n",
      "train loss:0.8792044770123607\n",
      "train loss:0.8513835918818263\n",
      "train loss:0.8329970009790827\n",
      "train loss:0.874831773426789\n",
      "train loss:1.006537402951631\n",
      "train loss:1.015913288959714\n",
      "train loss:0.9681248994989632\n",
      "train loss:0.847849379179246\n",
      "train loss:1.032360312135615\n",
      "train loss:0.8503975514139047\n",
      "train loss:0.8816309311340993\n",
      "train loss:0.9381538662011525\n",
      "train loss:0.9527019450563858\n",
      "train loss:0.9660066697706078\n",
      "train loss:0.9771341837334281\n",
      "train loss:1.0705620858188767\n",
      "train loss:0.9394041158213479\n",
      "train loss:1.0435916062497832\n",
      "train loss:0.8987917831506036\n",
      "train loss:0.9038841341388796\n",
      "train loss:0.8052761946184074\n",
      "train loss:0.8710439335766771\n",
      "train loss:0.8666774592431585\n",
      "train loss:0.9347743959397502\n",
      "train loss:0.7365840898147136\n",
      "train loss:1.060850890534252\n",
      "train loss:0.999240451391363\n",
      "train loss:0.8682396039487272\n",
      "train loss:0.9254911990541898\n",
      "train loss:0.9061643955857636\n",
      "train loss:1.0594752515818628\n",
      "train loss:0.8565394235698733\n",
      "train loss:0.8514526265886131\n",
      "train loss:1.0679053532821903\n",
      "train loss:0.8559002790938146\n",
      "train loss:0.8686510016529837\n",
      "train loss:0.8367659979790167\n",
      "train loss:1.0638143998597176\n",
      "train loss:0.9962181240294271\n",
      "train loss:1.034431630012134\n",
      "train loss:0.8410635678064134\n",
      "train loss:0.8653304668656862\n",
      "train loss:0.9630933355190909\n",
      "train loss:0.8555340470241228\n",
      "train loss:1.1287989020543774\n",
      "train loss:0.7714883377699141\n",
      "train loss:1.083902645152286\n",
      "train loss:0.88382459500373\n",
      "train loss:0.8706207540816508\n",
      "train loss:0.9106664444347582\n",
      "train loss:0.7479753098559527\n",
      "train loss:0.9280121059085735\n",
      "train loss:0.945962440499915\n",
      "train loss:0.7273793590311607\n",
      "train loss:0.7423772048637521\n",
      "train loss:0.925713678839839\n",
      "train loss:0.9123935912376535\n",
      "train loss:0.8949508251945928\n",
      "train loss:0.961401989874792\n",
      "train loss:0.9504159949831795\n",
      "train loss:0.762413988416862\n",
      "train loss:0.9517192847226268\n",
      "train loss:0.964844960471134\n",
      "train loss:0.8166918305296317\n",
      "train loss:0.7802453989644558\n",
      "train loss:0.9523924298567883\n",
      "train loss:1.000254932860953\n",
      "train loss:0.9962627306609805\n",
      "train loss:0.7168167175551567\n",
      "train loss:0.8555841418420509\n",
      "train loss:1.0304744667254513\n",
      "train loss:0.9570561722980337\n",
      "train loss:0.7569731491581726\n",
      "train loss:0.8448196295822291\n",
      "train loss:0.8589507790994326\n",
      "train loss:0.9940904289374569\n",
      "train loss:0.929257783796853\n",
      "train loss:0.930639581849111\n",
      "train loss:0.8939604373449943\n",
      "train loss:0.9601357249176679\n",
      "train loss:0.9539188845450403\n",
      "train loss:0.7932215199539219\n",
      "train loss:0.921748246240228\n",
      "train loss:0.8140801175973327\n",
      "train loss:0.993121464382206\n",
      "train loss:1.02773407527759\n",
      "train loss:0.9576366877058403\n",
      "train loss:1.0879159068798292\n",
      "train loss:0.9614619751230132\n",
      "train loss:1.1020352762123906\n",
      "train loss:0.9646473623177819\n",
      "train loss:0.9350316045397621\n",
      "train loss:0.8158495026791571\n",
      "train loss:0.9641550110618953\n",
      "train loss:0.9636151716557102\n",
      "train loss:0.93584292597609\n",
      "train loss:0.8111747286058266\n",
      "train loss:1.052025929100298\n",
      "train loss:0.7265219012866667\n",
      "train loss:0.8427920732712536\n",
      "train loss:0.9136084372086498\n",
      "train loss:0.9187008732040376\n",
      "train loss:1.0756991653588575\n",
      "train loss:0.854970341290668\n",
      "train loss:0.9207647166220059\n",
      "train loss:0.8654199992241479\n",
      "train loss:0.7904467359382983\n",
      "train loss:0.8654798446792066\n",
      "train loss:0.9873487555353591\n",
      "train loss:0.9070673896243193\n",
      "train loss:0.8335704824930157\n",
      "train loss:1.0037213789604553\n",
      "train loss:0.82266941281959\n",
      "train loss:0.836909020626738\n",
      "train loss:0.8711809315155424\n",
      "train loss:0.815673356024767\n",
      "train loss:1.1011826145922354\n",
      "train loss:0.8906171779432438\n",
      "train loss:0.8426721635186764\n",
      "train loss:0.9566032659059703\n",
      "train loss:0.9577493116446333\n",
      "train loss:1.016340157858971\n",
      "train loss:1.0451318673005376\n",
      "train loss:0.9914128907413781\n",
      "train loss:0.9599834732005199\n",
      "train loss:0.9169156886597177\n",
      "train loss:0.9257974646261631\n",
      "train loss:0.7801910714148615\n",
      "train loss:1.0087041138515185\n",
      "train loss:0.8768835823469253\n",
      "train loss:0.9284808236475773\n",
      "train loss:0.8782397995574657\n",
      "train loss:0.83482764651668\n",
      "train loss:0.8936012485574172\n",
      "train loss:0.7680707431384204\n",
      "train loss:0.868117351962099\n",
      "train loss:0.9622003071025769\n",
      "train loss:0.8208021554057701\n",
      "train loss:0.795328401425009\n",
      "train loss:0.9624058947212794\n",
      "train loss:0.8694655099923118\n",
      "train loss:0.9407734289436851\n",
      "train loss:0.7889830040006732\n",
      "train loss:0.6764145603796394\n",
      "train loss:0.7971673215532329\n",
      "train loss:0.930258084446119\n",
      "train loss:0.8224506213891726\n",
      "train loss:0.7636841545946647\n",
      "train loss:0.8399154704268025\n",
      "train loss:0.7746375460726616\n",
      "train loss:0.9978140465759726\n",
      "train loss:0.9349692021861803\n",
      "train loss:1.0333174722040652\n",
      "train loss:0.956876665235437\n",
      "train loss:0.9361731934898246\n",
      "train loss:0.9530718447588508\n",
      "train loss:0.8820385408651622\n",
      "train loss:0.8330602552030651\n",
      "train loss:0.8994786318032233\n",
      "train loss:0.8983223436962602\n",
      "train loss:1.0256205580136357\n",
      "train loss:0.835587743683249\n",
      "train loss:0.9354716090455605\n",
      "train loss:0.9223364321553038\n",
      "train loss:0.7245167343088855\n",
      "train loss:1.0121169088454898\n",
      "train loss:0.7939735874168709\n",
      "train loss:0.9629434958773291\n",
      "train loss:0.94749888754616\n",
      "train loss:0.822522273995845\n",
      "train loss:0.8297224829908991\n",
      "train loss:0.9262566541712606\n",
      "train loss:1.0336894863383785\n",
      "train loss:0.956590230397412\n",
      "train loss:0.8035577155003603\n",
      "train loss:0.9889530477442644\n",
      "train loss:0.8128360177521161\n",
      "train loss:0.8138634114191367\n",
      "train loss:0.8249876096152029\n",
      "train loss:1.0037959344937581\n",
      "train loss:1.0196276828679536\n",
      "train loss:0.8569345862967604\n",
      "train loss:1.1672876192515422\n",
      "train loss:0.9956178970792848\n",
      "train loss:1.0330837706603202\n",
      "train loss:0.9030849790054023\n",
      "train loss:0.9110825216230782\n",
      "train loss:1.1208465436051205\n",
      "train loss:0.7588256049851791\n",
      "train loss:0.9281079080170503\n",
      "=== epoch:5, train acc:0.991, test acc:0.983 ===\n",
      "train loss:0.9420205171415442\n",
      "train loss:0.9042730739212628\n",
      "train loss:1.0695331318561458\n",
      "train loss:1.079755121595432\n",
      "train loss:0.9701238790823554\n",
      "train loss:0.9104942534530376\n",
      "train loss:1.0366289205548969\n",
      "train loss:0.977308106205695\n",
      "train loss:1.008686226596449\n",
      "train loss:1.169483738626968\n",
      "train loss:0.95343669917136\n",
      "train loss:0.8014958827252735\n",
      "train loss:0.8534439647752156\n",
      "train loss:0.99182323055181\n",
      "train loss:0.9911929342824325\n",
      "train loss:0.9399274569659006\n",
      "train loss:0.8566153247927015\n",
      "train loss:0.9508168378015256\n",
      "train loss:0.8640082761071696\n",
      "train loss:1.0192585198657909\n",
      "train loss:0.902676230744244\n",
      "train loss:0.8450629685284359\n",
      "train loss:0.9027776222269097\n",
      "train loss:0.8082321543434715\n",
      "train loss:0.7676415859504716\n",
      "train loss:0.8644629158119866\n",
      "train loss:1.1200480572554434\n",
      "train loss:0.885886504024277\n",
      "train loss:0.9073359826589585\n",
      "train loss:1.026454886817868\n",
      "train loss:0.6702622400502777\n",
      "train loss:0.8576728716856789\n",
      "train loss:0.8992894531056653\n",
      "train loss:0.9635084097672844\n",
      "train loss:1.0725159704314824\n",
      "train loss:0.7817150892224168\n",
      "train loss:0.8533045916355552\n",
      "train loss:0.8496833177659222\n",
      "train loss:0.7966734372999394\n",
      "train loss:0.9747091431598889\n",
      "train loss:0.9034202070494274\n",
      "train loss:0.8637685000683625\n",
      "train loss:0.9699359482348612\n",
      "train loss:0.9507503230432617\n",
      "train loss:0.9346120567465894\n",
      "train loss:0.781221791088493\n",
      "train loss:0.8872891372046914\n",
      "train loss:0.9227713550032592\n",
      "train loss:0.9010686045298939\n",
      "train loss:0.8964941345452386\n",
      "train loss:0.9310406842533159\n",
      "train loss:0.7729920949598263\n",
      "train loss:0.8062022361380252\n",
      "train loss:1.1197682273342118\n",
      "train loss:0.9801608526504175\n",
      "train loss:0.9229216329793679\n",
      "train loss:0.8314463035026889\n",
      "train loss:0.6622803691274501\n",
      "train loss:0.9888944487344733\n",
      "train loss:0.8444363183878367\n",
      "train loss:0.8686119573795161\n",
      "train loss:0.9933582478236133\n",
      "train loss:0.9042179212760977\n",
      "train loss:1.0214430130168932\n",
      "train loss:0.8371465566261145\n",
      "train loss:0.8847129225576083\n",
      "train loss:0.9038723043568281\n",
      "train loss:0.8607786955544554\n",
      "train loss:0.8713455015477076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9648638555290197\n",
      "train loss:0.9220962488033809\n",
      "train loss:0.9238457089816566\n",
      "train loss:0.9350811729832667\n",
      "train loss:0.900401349680224\n",
      "train loss:0.776130877036304\n",
      "train loss:0.9992804687531445\n",
      "train loss:0.8098594372001994\n",
      "train loss:0.8774701602137709\n",
      "train loss:0.9854490162090639\n",
      "train loss:0.9009691524269486\n",
      "train loss:0.9986268836620797\n",
      "train loss:0.7901232174416702\n",
      "train loss:0.9058917624552093\n",
      "train loss:1.0253534403696944\n",
      "train loss:0.7782979815489118\n",
      "train loss:0.8766931342433345\n",
      "train loss:0.875055540448\n",
      "train loss:0.8108806502389585\n",
      "train loss:0.8741419164170549\n",
      "train loss:1.0038907339317467\n",
      "train loss:0.9222644925159588\n",
      "train loss:1.1423189302055747\n",
      "train loss:1.000209686622769\n",
      "train loss:1.0450535818913622\n",
      "train loss:0.9916083274719105\n",
      "train loss:0.7175645447419984\n",
      "train loss:1.000661450063398\n",
      "train loss:0.9831476122291306\n",
      "train loss:0.8291031821669061\n",
      "train loss:0.7918079440312259\n",
      "train loss:0.9495071652407026\n",
      "train loss:0.8851680406841399\n",
      "train loss:0.8853937335753767\n",
      "train loss:0.7363845919601896\n",
      "train loss:0.7759111435600926\n",
      "train loss:0.9184082595211115\n",
      "train loss:0.9415654680434586\n",
      "train loss:0.8441516065339943\n",
      "train loss:0.8867154296604077\n",
      "train loss:0.9370874417833467\n",
      "train loss:0.9971548145523239\n",
      "train loss:0.8606163021093116\n",
      "train loss:0.7803576180420376\n",
      "train loss:0.9262837957996743\n",
      "train loss:0.8419773157823319\n",
      "train loss:0.8726601720545575\n",
      "train loss:0.9140253227597279\n",
      "train loss:0.9230334628456027\n",
      "train loss:0.8610542351627334\n",
      "train loss:0.95725291389166\n",
      "train loss:0.908123072359506\n",
      "train loss:0.789190076883185\n",
      "train loss:0.962931163504694\n",
      "train loss:0.9020007186907556\n",
      "train loss:0.9668352676051994\n",
      "train loss:0.7895151738224457\n",
      "train loss:0.9242002185960395\n",
      "train loss:0.8242493305349208\n",
      "train loss:0.9825244584627041\n",
      "train loss:0.8246163540662681\n",
      "train loss:0.7651121504272566\n",
      "train loss:0.7892665574087419\n",
      "train loss:0.9288656282866509\n",
      "train loss:0.8073764007281784\n",
      "train loss:0.9959510209871394\n",
      "train loss:0.9756822089425465\n",
      "train loss:1.0406484335314579\n",
      "train loss:0.7908937765712293\n",
      "train loss:0.7232509558084886\n",
      "train loss:1.028982177767699\n",
      "train loss:0.935221485209305\n",
      "train loss:0.8948635277069628\n",
      "train loss:0.9188961395893859\n",
      "train loss:1.0321321148006402\n",
      "train loss:0.918588132110081\n",
      "train loss:0.8386355516116987\n",
      "train loss:0.8657565260568785\n",
      "train loss:0.849290959976879\n",
      "train loss:0.8134367960525145\n",
      "train loss:1.0236354986970897\n",
      "train loss:0.892239771401399\n",
      "train loss:0.8851994899310576\n",
      "train loss:0.9613864848642887\n",
      "train loss:0.8731517588712093\n",
      "train loss:0.742540293672551\n",
      "train loss:0.9357121384209428\n",
      "train loss:0.9197750804551993\n",
      "train loss:0.7591735590544441\n",
      "train loss:0.7066954645710433\n",
      "train loss:0.7712726746777071\n",
      "train loss:1.1001270394972638\n",
      "train loss:0.8055073086513427\n",
      "train loss:0.9339715540228554\n",
      "train loss:0.8988096772841692\n",
      "train loss:0.7462958864395516\n",
      "train loss:0.8280090868039214\n",
      "train loss:0.8979897027839304\n",
      "train loss:1.0036816026031852\n",
      "train loss:0.974249140389765\n",
      "train loss:0.9051410108856166\n",
      "train loss:0.8266433228999729\n",
      "train loss:0.980123495345582\n",
      "train loss:0.8955555478371864\n",
      "train loss:1.1652222910523415\n",
      "train loss:0.8329051669032681\n",
      "train loss:0.950988617103849\n",
      "train loss:0.8245063940841907\n",
      "train loss:1.0076446882466532\n",
      "train loss:0.9480101714508984\n",
      "train loss:1.0042697113073975\n",
      "train loss:1.1701615580654683\n",
      "train loss:0.9316140828423418\n",
      "train loss:0.9684275392825766\n",
      "train loss:1.0346329393131566\n",
      "train loss:0.8688703653158768\n",
      "train loss:0.9025005131869893\n",
      "train loss:0.7961712059997642\n",
      "train loss:0.899079906256765\n",
      "train loss:0.9315494155394997\n",
      "train loss:0.9114629565586658\n",
      "train loss:0.9417931982220494\n",
      "train loss:1.1050603577355864\n",
      "train loss:0.8110714980806754\n",
      "train loss:0.9464813955682039\n",
      "train loss:0.7572361939968278\n",
      "train loss:0.9241191236304328\n",
      "train loss:0.9213640386184649\n",
      "train loss:0.7986117137161176\n",
      "train loss:0.864148312069777\n",
      "train loss:0.9934352912924905\n",
      "train loss:0.9119735593010848\n",
      "train loss:0.9360691790545761\n",
      "train loss:1.014663511136006\n",
      "train loss:0.9268727906177463\n",
      "train loss:0.89892893861825\n",
      "train loss:0.9034604836779556\n",
      "train loss:0.8342216088730673\n",
      "train loss:0.8854278890216698\n",
      "train loss:0.9901623103431193\n",
      "train loss:0.8370859211835285\n",
      "train loss:0.9670781361229477\n",
      "train loss:0.9252948945644647\n",
      "train loss:0.9156783843075087\n",
      "train loss:0.9361323560471497\n",
      "train loss:0.8183954816800038\n",
      "train loss:0.8376891795142128\n",
      "train loss:0.875213962720585\n",
      "train loss:0.9918335758992294\n",
      "train loss:0.8293820209933973\n",
      "train loss:1.1017740607623496\n",
      "train loss:0.918259648961678\n",
      "train loss:0.8542427227053077\n",
      "train loss:0.961503508234756\n",
      "train loss:0.8052039074313597\n",
      "train loss:1.0151762125925483\n",
      "train loss:0.8874143383239063\n",
      "train loss:0.9312311312455168\n",
      "train loss:0.8380135060635543\n",
      "train loss:0.9370905562985863\n",
      "train loss:0.9228365372212198\n",
      "train loss:0.9789980413539787\n",
      "train loss:1.1373073198311419\n",
      "train loss:0.8900485624407836\n",
      "train loss:0.8197948799409207\n",
      "train loss:0.8712553142171103\n",
      "train loss:0.9034815021620329\n",
      "train loss:0.8925556961394845\n",
      "train loss:0.7259796053058304\n",
      "train loss:0.9908426194708967\n",
      "train loss:0.7687508719962725\n",
      "train loss:0.9765587208075824\n",
      "train loss:0.8319821604779295\n",
      "train loss:1.1106913237514373\n",
      "train loss:1.0273102801417773\n",
      "train loss:0.9745158555850222\n",
      "train loss:0.9929063360793358\n",
      "train loss:0.9496405966473194\n",
      "train loss:1.0062844793940002\n",
      "train loss:0.9476635137943936\n",
      "train loss:0.7870312861425568\n",
      "train loss:0.9649380876201288\n",
      "train loss:0.7751133870233337\n",
      "train loss:1.06782049174777\n",
      "train loss:0.9323687569975279\n",
      "train loss:0.9121791210038793\n",
      "train loss:0.9104755820972461\n",
      "train loss:0.9795313090021481\n",
      "train loss:1.021485452852161\n",
      "train loss:1.038860144199683\n",
      "train loss:0.8749344524464778\n",
      "train loss:0.8380975806651915\n",
      "train loss:0.874899299841951\n",
      "train loss:0.9020943735385148\n",
      "train loss:0.8439298635393528\n",
      "train loss:0.9570546871893351\n",
      "train loss:0.9261739253196346\n",
      "train loss:0.9004139429110591\n",
      "train loss:0.8246379321348966\n",
      "train loss:0.7915687571709537\n",
      "train loss:0.9654257959034862\n",
      "train loss:0.9611231989291131\n",
      "train loss:0.9046059921924576\n",
      "train loss:0.9791183123117482\n",
      "train loss:0.8644697987322054\n",
      "train loss:0.9876698069582808\n",
      "train loss:0.8577159111258164\n",
      "train loss:0.8864256688450902\n",
      "train loss:1.292264605977112\n",
      "train loss:0.8563405483298042\n",
      "train loss:0.9543563528805591\n",
      "train loss:0.815553208922941\n",
      "train loss:1.062705403070805\n",
      "train loss:1.0450820891284438\n",
      "train loss:0.7858240144670053\n",
      "train loss:0.8559079472636647\n",
      "train loss:0.792415934791327\n",
      "train loss:0.8318478492165283\n",
      "train loss:0.7489730008806862\n",
      "train loss:0.9377145279707659\n",
      "train loss:0.8245927378242931\n",
      "train loss:0.8571609266314966\n",
      "train loss:1.0390120437594956\n",
      "train loss:0.8966365876523161\n",
      "train loss:0.8614454291215504\n",
      "train loss:0.9628264598222398\n",
      "train loss:1.090335691656878\n",
      "train loss:0.7023564777014701\n",
      "train loss:1.035404692275724\n",
      "train loss:0.7998745065280295\n",
      "train loss:0.8294944243310843\n",
      "train loss:0.8279956518471181\n",
      "train loss:0.8552245425596786\n",
      "train loss:0.9411590189796076\n",
      "train loss:0.9844664180663019\n",
      "train loss:0.9735478341389882\n",
      "train loss:0.9269919220238289\n",
      "train loss:1.0178438836921928\n",
      "train loss:0.8560518668555891\n",
      "train loss:0.8570523570864301\n",
      "train loss:0.8712122276667634\n",
      "train loss:0.8299932962459999\n",
      "train loss:0.9532012379810706\n",
      "train loss:0.7929214687676222\n",
      "train loss:0.7661840022663\n",
      "train loss:0.8988446949841737\n",
      "train loss:0.9191632903811485\n",
      "train loss:0.8011929855026722\n",
      "train loss:1.0522815807457722\n",
      "train loss:0.9632888731079927\n",
      "train loss:0.8925445261044713\n",
      "train loss:0.8071494978446272\n",
      "train loss:0.8574450467406394\n",
      "train loss:0.9383940193394906\n",
      "train loss:0.9662125105528067\n",
      "train loss:0.9214544906468101\n",
      "train loss:0.7946660946720217\n",
      "train loss:0.9404431153614284\n",
      "train loss:0.9834189930837927\n",
      "train loss:0.9260165479489659\n",
      "train loss:0.877122483196847\n",
      "train loss:0.8809913150174234\n",
      "train loss:0.820385357571068\n",
      "train loss:0.8902148163121005\n",
      "train loss:0.8468657977639031\n",
      "train loss:1.0929104619906134\n",
      "train loss:1.0148493762243431\n",
      "train loss:0.9229548734655446\n",
      "train loss:0.970410187376185\n",
      "train loss:0.9696611246015832\n",
      "train loss:0.8824060060889818\n",
      "train loss:0.7845144607275827\n",
      "train loss:0.79028896840191\n",
      "train loss:0.8682363469376986\n",
      "train loss:1.0006214044056785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.8417143300661547\n",
      "train loss:0.8243109885770566\n",
      "train loss:0.9747986953029372\n",
      "train loss:0.9593496000197033\n",
      "train loss:1.0460298009033608\n",
      "train loss:0.9871084995620921\n",
      "train loss:0.823160778430935\n",
      "train loss:0.933862636206361\n",
      "train loss:1.0729473508958707\n",
      "train loss:0.9527685229280856\n",
      "train loss:0.7667979201215954\n",
      "train loss:0.907513478448562\n",
      "train loss:0.8688243934669337\n",
      "train loss:0.8503197881433987\n",
      "train loss:0.8760674232929024\n",
      "train loss:0.9705593199764212\n",
      "train loss:0.964419424889956\n",
      "train loss:0.7812822362230708\n",
      "train loss:0.820758802576152\n",
      "train loss:0.7906362412449517\n",
      "train loss:0.9186282250703931\n",
      "train loss:1.0428066062480323\n",
      "train loss:0.8236851564255886\n",
      "train loss:0.8269818906611905\n",
      "train loss:0.9310125200913197\n",
      "train loss:0.9301578225772884\n",
      "train loss:0.9777725680750504\n",
      "train loss:0.9141652295655744\n",
      "train loss:0.8900194463739801\n",
      "train loss:0.8972144640721613\n",
      "train loss:1.1240950781432195\n",
      "train loss:0.979187218067365\n",
      "train loss:1.1158854109309688\n",
      "train loss:0.8664324870374593\n",
      "train loss:0.8499275617269129\n",
      "train loss:0.9018114324722089\n",
      "train loss:0.8848451605470374\n",
      "train loss:0.7231794881228272\n",
      "train loss:0.843517071946614\n",
      "train loss:0.8031362263527347\n",
      "train loss:0.786732816420215\n",
      "train loss:0.8135844639443265\n",
      "train loss:0.9224751318946809\n",
      "train loss:0.8602360427526098\n",
      "train loss:1.1436029376265369\n",
      "train loss:1.0211700296409052\n",
      "train loss:0.689072398443887\n",
      "train loss:0.8473632629737915\n",
      "train loss:0.8630405846135814\n",
      "train loss:0.8369288766416535\n",
      "train loss:1.1002977013684603\n",
      "train loss:0.8526272582013843\n",
      "train loss:0.9524199224814178\n",
      "train loss:0.9663659670562921\n",
      "train loss:0.8251229116117936\n",
      "train loss:1.027496570842688\n",
      "train loss:0.7593093179179685\n",
      "train loss:1.0880018393392328\n",
      "train loss:0.900958596281527\n",
      "train loss:0.9863660323165392\n",
      "train loss:0.7866007727893104\n",
      "train loss:0.9763416750824443\n",
      "train loss:0.9069516956433588\n",
      "train loss:0.920164390348026\n",
      "train loss:1.016316252745407\n",
      "train loss:0.8375027866836491\n",
      "train loss:0.7969522942597416\n",
      "train loss:0.8734232372675279\n",
      "train loss:1.0026462968864034\n",
      "train loss:0.9189144281722033\n",
      "train loss:0.8850003294222618\n",
      "train loss:1.0177327170100583\n",
      "train loss:1.0228481760752135\n",
      "train loss:0.9292575160051042\n",
      "train loss:1.0462474355031868\n",
      "train loss:0.7196150918081966\n",
      "train loss:0.9333931366073355\n",
      "train loss:0.859882061307531\n",
      "train loss:0.939016195768545\n",
      "train loss:0.8995710727772392\n",
      "train loss:0.7839721785937117\n",
      "train loss:0.9996924104538495\n",
      "train loss:1.0352350108456247\n",
      "train loss:0.8053068733050908\n",
      "train loss:0.9049568134738754\n",
      "train loss:0.9839008677630595\n",
      "train loss:0.8186623200612035\n",
      "train loss:0.9021025325812414\n",
      "train loss:0.9900439938690296\n",
      "train loss:1.0049138908619448\n",
      "train loss:1.1040181841515098\n",
      "train loss:0.9343243308938068\n",
      "train loss:0.8430585585481587\n",
      "train loss:0.9188178859340613\n",
      "train loss:1.0127380008373559\n",
      "train loss:0.7972189436186631\n",
      "train loss:0.792770559801211\n",
      "train loss:0.9244747500583713\n",
      "train loss:0.8032814497371348\n",
      "train loss:0.9243007507961418\n",
      "train loss:0.8957014731796457\n",
      "train loss:0.9054365412062065\n",
      "train loss:0.8315211573963398\n",
      "train loss:0.8419205530957119\n",
      "train loss:0.8472551078672954\n",
      "train loss:1.0473775515317247\n",
      "train loss:0.8119701547812619\n",
      "train loss:0.7125108310296296\n",
      "train loss:1.0330810188348722\n",
      "train loss:0.9145878309761999\n",
      "train loss:1.068950080018725\n",
      "train loss:0.9825243160229512\n",
      "train loss:0.7327319583019003\n",
      "train loss:0.9313467326641629\n",
      "train loss:0.7184107474574482\n",
      "train loss:1.0559971390576133\n",
      "train loss:0.9268348151112213\n",
      "train loss:0.8883380232591007\n",
      "train loss:0.8587754582570135\n",
      "train loss:1.037472137840941\n",
      "train loss:0.979242977416765\n",
      "train loss:0.8862564477367906\n",
      "train loss:0.9486115028092152\n",
      "train loss:0.7354757080328511\n",
      "train loss:0.8770802005367112\n",
      "train loss:1.066802891186885\n",
      "train loss:0.7598492845725435\n",
      "train loss:0.9588475149024778\n",
      "train loss:0.913556511932843\n",
      "train loss:0.957335538859414\n",
      "train loss:0.870469437672776\n",
      "train loss:0.9400070137076685\n",
      "train loss:0.9671595367795027\n",
      "train loss:0.7509447607521169\n",
      "train loss:0.8765264772901795\n",
      "train loss:1.066917325583731\n",
      "train loss:0.9282332666196013\n",
      "train loss:0.845771603789575\n",
      "train loss:0.8169764816606151\n",
      "train loss:0.9908034590823627\n",
      "train loss:0.7000853654919044\n",
      "train loss:0.8690378223931958\n",
      "train loss:0.9241520929075167\n",
      "train loss:0.9680643123932109\n",
      "train loss:0.8698102009997313\n",
      "train loss:0.8880283825912011\n",
      "train loss:0.9313284216616909\n",
      "train loss:1.1274572353310761\n",
      "train loss:0.9978735085578258\n",
      "train loss:0.8033262836931253\n",
      "train loss:0.8157607602620744\n",
      "train loss:0.8770410715012169\n",
      "train loss:0.9457824323931499\n",
      "train loss:0.9770845184534721\n",
      "train loss:0.9126799915084358\n",
      "train loss:0.9457547003017025\n",
      "train loss:1.0225137890989209\n",
      "train loss:0.821776414691816\n",
      "train loss:0.9265866303254928\n",
      "train loss:0.8573651106073324\n",
      "train loss:0.8989574357177383\n",
      "train loss:0.9055911484003775\n",
      "train loss:0.9186779300526255\n",
      "train loss:1.0492691541619419\n",
      "train loss:0.9253658362169359\n",
      "train loss:0.9233590830847287\n",
      "train loss:0.8567643189073126\n",
      "train loss:1.028817812751558\n",
      "train loss:1.0079612680495402\n",
      "train loss:1.0959221197654234\n",
      "train loss:0.9835664934211983\n",
      "train loss:0.852725631888783\n",
      "train loss:0.8834817267371244\n",
      "train loss:0.7878065167152887\n",
      "train loss:0.9559755927855504\n",
      "train loss:0.8915992395552726\n",
      "train loss:1.060217567155937\n",
      "train loss:0.8941117425426311\n",
      "train loss:0.9411217893443764\n",
      "train loss:0.9297243716495455\n",
      "train loss:0.8954293699126322\n",
      "train loss:0.9198765528567118\n",
      "train loss:0.7153268267623984\n",
      "train loss:0.9126989629104989\n",
      "train loss:0.7608818403645146\n",
      "train loss:0.8951704720677752\n",
      "train loss:0.9578857991786021\n",
      "train loss:0.8564753720457362\n",
      "train loss:0.9696077827873278\n",
      "train loss:0.878168369867425\n",
      "train loss:0.7535793729008451\n",
      "train loss:0.7667298000715483\n",
      "train loss:0.8164778187520989\n",
      "train loss:0.8252238900175126\n",
      "train loss:0.8441878736959119\n",
      "train loss:1.1868924961798877\n",
      "train loss:0.7109214199351453\n",
      "train loss:0.8798976121172453\n",
      "train loss:0.9034884496417134\n",
      "train loss:0.8979636484095999\n",
      "train loss:0.8483609201102235\n",
      "train loss:0.9739196610182661\n",
      "train loss:0.9032466453034291\n",
      "train loss:0.9741008668559297\n",
      "train loss:0.731150977330564\n",
      "train loss:0.7142080828791155\n",
      "train loss:0.9501461006081213\n",
      "train loss:0.9252037189178691\n",
      "train loss:0.8379975348601644\n",
      "train loss:1.1260318205423834\n",
      "train loss:0.9812093413972208\n",
      "train loss:0.8129539397349531\n",
      "train loss:0.7649413148513553\n",
      "train loss:0.7549190802428147\n",
      "train loss:0.9847971526844277\n",
      "train loss:0.9637837825623226\n",
      "train loss:0.8563180253309105\n",
      "train loss:0.8532697200629998\n",
      "train loss:0.929231277873471\n",
      "train loss:0.9395582590869761\n",
      "train loss:1.122906003498899\n",
      "train loss:1.0889450497623754\n",
      "train loss:0.8138406030046818\n",
      "train loss:0.8432668457134557\n",
      "train loss:0.8652263418210572\n",
      "train loss:0.9434798746030773\n",
      "train loss:0.9197461389257238\n",
      "train loss:0.861692693766567\n",
      "train loss:0.9491506048413473\n",
      "train loss:0.7406290682657611\n",
      "train loss:0.8741115112244923\n",
      "train loss:0.8845020946324493\n",
      "train loss:0.8318239772288973\n",
      "train loss:0.9100142189630681\n",
      "train loss:0.854508714675319\n",
      "train loss:0.7949595090849501\n",
      "train loss:0.8582135397652234\n",
      "train loss:0.8531472807265951\n",
      "train loss:0.7691072400882902\n",
      "train loss:0.889918953794787\n",
      "train loss:0.8922497950482463\n",
      "train loss:0.983246290446738\n",
      "train loss:0.9370945050723551\n",
      "train loss:0.8633908943695058\n",
      "train loss:1.0084002413342958\n",
      "train loss:0.8187457737040167\n",
      "train loss:0.9416428453123512\n",
      "train loss:0.976489751610591\n",
      "train loss:0.9126543152523743\n",
      "train loss:0.7684362151119637\n",
      "train loss:1.0260831660378034\n",
      "train loss:1.0148017955159216\n",
      "train loss:0.9041786485932461\n",
      "train loss:0.7585267960085612\n",
      "train loss:0.9545363531524547\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9907\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "trainer = Trainer(network, x_train, model_pred, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルAの正答率： 0.9907\n"
     ]
    }
   ],
   "source": [
    "test_acc = network.accuracy(x_test, t_test)\n",
    "print(\"モデルAの正答率：\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# パラメータの保存\n",
    "network.save_params(\"keras_clone_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "plt.savefig('keras_clone_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
